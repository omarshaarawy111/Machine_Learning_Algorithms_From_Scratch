{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53f18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models to use them within bagging technique\n",
    "# Known that bagging work with only one base estimator cloning it within bootsreapped data with replacement\n",
    "from Libraries.LinearRegression import *\n",
    "from Libraries.LogisticRegression import *\n",
    "from Libraries.KNN import *\n",
    "from Libraries.SVM import *\n",
    "from Libraries.NaiveBayes import *\n",
    "from Libraries.DecisionTree import *\n",
    "from Libraries.RandomForest import *\n",
    "from Libraries.Voting import *\n",
    "from Libraries.AdaBoost import *\n",
    "from Libraries.GradientBoost import *\n",
    "import sys, importlib, shutil\n",
    "shutil.rmtree(\"Libraries/__pycache__\", ignore_errors=True)\n",
    "sys.modules.pop(\"Libraries.RandomForest\", None)\n",
    "importlib.invalidate_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95559678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need this copy for cloning base estimator\n",
    "import copy\n",
    "# For parallelism and working with multiple of cores \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Bagging class for Classification and Regression\n",
    "# We work with the base class then inheritence happen to build the classification or regression bagging\n",
    "class BaggingBase():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, base_estimator = None, n_estimators = 10, random_state = None, n_jobs = None):\n",
    "        # Here we gonna define the base estimator later in classification or regression\n",
    "        # Others is the same default paramerts of API\n",
    "        # n_jobs=None or 1 is the same and -1 means using all cores\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = []\n",
    "\n",
    "\n",
    "    # We have helper functions\n",
    "    # Bootstrapping samples function\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        # Here we fetch the number of samples from x \n",
    "        # X is combination of samples x features\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Every time we select random numbers of indice with enabling replacement option so we can use same indice in more models\n",
    "        # So we can get the sample 0, 1 or multiple of times \n",
    "        # Here we choose randomly from all samples and we need the size of bootstrapped data = size of orginal data that is why replace option should be applied\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "\n",
    "        # Here we return the bootstrapped data\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    # Clone function\n",
    "    def _clone(self):\n",
    "        # Here we use the library of copy to return clone of base_estimator\n",
    "        # The idea here to siolate object per model\n",
    "        # It is only for learning phase\n",
    "        return copy.deepcopy(self.base_estimator)\n",
    "    \n",
    "    # Modeling function\n",
    "    # We will have one fit function per every model then combine all fits in general fit function for all memebers\n",
    "    # Fit single model\n",
    "    def _fit_single_model(self, X, y):\n",
    "        # First we recieve the bootstrapped data to model it from given X and y\n",
    "        X_samples, y_samples = self._bootstrap_sample(X, y)\n",
    "        \n",
    "        # Get cloned copy of base_estimator\n",
    "        # Every time we gonna pass the object of base estimator\n",
    "        cloned_model = self._clone()\n",
    "\n",
    "        # We use the fit function for specific single model which was already built from scratch before\n",
    "        cloned_model.fit(X_samples, y_samples)\n",
    "\n",
    "        # Every time we return the fitted model\n",
    "        return cloned_model\n",
    "    \n",
    "    # Fit all models function\n",
    "    # Here we gather all fitted models \n",
    "    def fit(self, X, y):\n",
    "        # We set random state wuth the number user entered and seed it\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # Loop over estimator number collecting all fitted models in one place to be ready for test phase\n",
    "        # We call _fit_single_model every time\n",
    "        self.models = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._fit_single_model)(X, y) for estimator in range(self.n_estimators)\n",
    "        )    \n",
    "        \n",
    "        # We return nothing\n",
    "        return self\n",
    "    \n",
    "    # Predict probabilities\n",
    "    # This function is ready to use in soft oting later\n",
    "    def predict_proba(self, X):\n",
    "        probas = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(model.predict_proba)(X) for model in self.models\n",
    "        )\n",
    "        probas = np.array(probas)\n",
    "        return np.mean(probas, axis=0)\n",
    "\n",
    "    # The same structure of fit will be built for predict\n",
    "    # We will have one predict function per every model then combine all predicts in general predict function for all memebers\n",
    "    # Predict single model\n",
    "    def _predict_single_model(self, cloned_model, X):\n",
    "        # We use the predict function for specific single model which was already built from scratch before\n",
    "        # We use test data\n",
    "        return cloned_model.predict(X)\n",
    "\n",
    "    # Precit all models function\n",
    "    # Here we gather all precited models \n",
    "    def predict(self, X):\n",
    "        # Loop over model collecting all predictions in one place to be ready for aggregation later using cpu cores (parallelism)\n",
    "        # We call _predict_single_model every time\n",
    "        # to apply predict we need to concatenate with fit first this is gathered in self.models\n",
    "        # Here every fitted model work with all samples \n",
    "        # So each sample have multiple predictions for all models\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._predict_single_model)(model, X) for model in self.models\n",
    "        )    \n",
    "        \n",
    "        # Preditction as numpy array\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # After collecting predictions we are ready for aggregation\n",
    "        return self._aggregate(predictions)\n",
    "\n",
    "    # Aggregation function    \n",
    "    # Aggregation to decide the final output\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _aggregate(self, predictions):\n",
    "        raise NotImplementedError  \n",
    "\n",
    "# Bagging ensemble Classifier class\n",
    "class BaggingClassifier(BaggingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, random_state=None, n_jobs=1):\n",
    "        \n",
    "        # Default estimator for classification as API\n",
    "        if base_estimator is None:\n",
    "            base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "        super().__init__(base_estimator, n_estimators, random_state, n_jobs)\n",
    "\n",
    "    # Aggregation function    \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # Get number of samples as every sample wich will be columns as every column represents sample and indexes represent models  \n",
    "        n_samples = predictions.shape[1]\n",
    "\n",
    "        # Gather all predictions in one array and make sure its data type the same as the source predictions array\n",
    "        # Intialize\n",
    "        final_predictions = np.zeros(n_samples, dtype=predictions.dtype)\n",
    "\n",
    "        # Apply majority voting \n",
    "        # Loop over all samples to get the most voted label\n",
    "        for sample in range(n_samples):\n",
    "            # Featch models of samples values\n",
    "            # Every sample represents column\n",
    "            votes = predictions[:, sample]\n",
    "\n",
    "            # Here we get unique classes then counting them\n",
    "            # We return two related arrays of classes and labels \n",
    "            # One for unique values and other for unique counts\n",
    "            # So return the index of laregest count will be equivelant to the label of largest count\n",
    "            classes, counts = np.unique(votes, return_counts=True)\n",
    "            majority_voting = classes[np.argmax(counts)]\n",
    "            final_predictions[sample] = majority_voting\n",
    "\n",
    "        # Return the final predictions of the test data\n",
    "        return final_predictions\n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "# Bagging ensemble Tree Regressor class\n",
    "class BaggingRegressor(BaggingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, random_state=None, n_jobs=1):\n",
    "        \n",
    "        # Default estimator for classification as API\n",
    "        if base_estimator is None:\n",
    "            base_estimator = DecisionTreeRegressor()\n",
    "\n",
    "        super().__init__(base_estimator, n_estimators, random_state, n_jobs)\n",
    "\n",
    "    # Aggregation function \n",
    "    def _aggregate(self, predictions):\n",
    "        # We just return the mean of every sample wich will be columns as every column represents sample and indexes represent trees\n",
    "        return np.mean(predictions, axis=0)  \n",
    "  \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1f9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Defualt Bagging Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 99.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Defualt Bagging Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.77 9.94 9.94]\n",
      "R^2 score on training data: 27.52 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Ridge', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 85.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Lasso', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 59.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: KNN(task='Classification', k=3, distance_metric='Euclidean')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 90.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB8860>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 83.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB87C0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 82.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB8720>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 83.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB8680>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 49.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB85E0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 83.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB8540>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 83.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB84A0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 83.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB8400>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 35.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB8360>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB82C0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 83.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB80E0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 79.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000027FE7DB89A0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 51.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: GaussianNB(method='gaussian', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 85.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: MultinomialNB(method='multinomial', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 72.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: BernoulliNB(method='bernoulli', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, min_impurity_decrease=0.0, ccp_alpha=0.0, root=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 92.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: RandomForestClassifier(criterion='gini', n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=None, n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 85.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: VotingClassifier(estimators=[<Libraries.LogisticRegression.LogisticRegression object at 0x0000027FE7EDCB60>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000027FE7EDD4F0>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000027FE7EDC260>, <Libraries.KNN.KNN object at 0x0000027FE7EDE060>, <Libraries.SVM.SVC object at 0x0000027FE7EDF230>, <Libraries.SVM.SVC object at 0x0000027FE7EDE8D0>, <Libraries.SVM.SVC object at 0x0000027FE7EDDC10>, <Libraries.SVM.SVC object at 0x0000027FE7EDDAC0>, <Libraries.SVM.SVC object at 0x0000027FE7EDE630>, <Libraries.SVM.SVC object at 0x0000027FE7EDC290>, <Libraries.SVM.SVC object at 0x0000027FE7EDC920>, <Libraries.SVM.SVC object at 0x0000027FE7EDD070>, <Libraries.SVM.SVC object at 0x0000027FE7EDF4A0>, <Libraries.SVM.SVC object at 0x0000027FE7EDC140>, <Libraries.SVM.SVC object at 0x0000027FE7EDFB30>, <Libraries.SVM.SVC object at 0x0000027FE7EDCF20>, <Libraries.NaiveBayes.GaussianNB object at 0x0000027FE7EDCCB0>, <Libraries.NaiveBayes.MultinomialNB object at 0x0000027FE7EDCF50>, <Libraries.NaiveBayes.BernoulliNB object at 0x0000027FE7EDD760>, <Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000027FE7EDE1E0>, <Libraries.RandomForest.RandomForestClassifier object at 0x0000027FE7EDE480>, <Libraries.AdaBoost.AdaBoostClassifier object at 0x0000027FE7EDE660>, <Libraries.GradientBoost.GradientBoostClassifier object at 0x0000027FE7EDFEC0>], n_jobs=1, models=[], voting='hard', classes_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: VotingClassifier(estimators=[<Libraries.LogisticRegression.LogisticRegression object at 0x0000027FE7EDEF60>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000027FE7EDE4E0>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000027FE7EDF740>, <Libraries.KNN.KNN object at 0x0000027FE7EDC620>, <Libraries.SVM.SVC object at 0x0000027FE7EDC230>, <Libraries.SVM.SVC object at 0x0000027FE7EDC3E0>, <Libraries.SVM.SVC object at 0x0000027FE7EDD0D0>, <Libraries.SVM.SVC object at 0x0000027FE7EDD850>, <Libraries.SVM.SVC object at 0x0000027FE7EDE2A0>, <Libraries.SVM.SVC object at 0x0000027FE7EDE300>, <Libraries.SVM.SVC object at 0x0000027FE7EDE330>, <Libraries.SVM.SVC object at 0x0000027FE7EDE5D0>, <Libraries.SVM.SVC object at 0x0000027FE7EDE960>, <Libraries.SVM.SVC object at 0x0000027FE7EDEE10>, <Libraries.SVM.SVC object at 0x0000027FE7EDFCE0>, <Libraries.SVM.SVC object at 0x0000027FE7EDF3E0>, <Libraries.NaiveBayes.GaussianNB object at 0x0000027FE7EDFCB0>, <Libraries.NaiveBayes.MultinomialNB object at 0x0000027FE7EDD0A0>, <Libraries.NaiveBayes.BernoulliNB object at 0x0000027FE8334F50>, <Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000027FE8268740>, <Libraries.RandomForest.RandomForestClassifier object at 0x0000027FE826A840>, <Libraries.AdaBoost.AdaBoostClassifier object at 0x0000027FE828CB90>, <Libraries.GradientBoost.GradientBoostClassifier object at 0x0000027FE828D760>], n_jobs=1, models=[], voting='soft', classes_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: AdaBoostClassifier(base_estimator=<Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000027FE828CD40>, n_estimators=50, learning_rate=1.0, estimators_=[], estimator_weights_=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: GradientBoostClassifier(base_estimator=<Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000027FE828D2B0>, n_estimators=100, learning_rate=0.1, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, tol=1e-06, random_state=None, estimators_=[], train_loss_=[], val_loss_=[], best_iteration_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 91.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='OLS', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.92 13.6  16.52]\n",
      "R^2 score on training data: 107.05 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Normal', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.91 13.4  16.32]\n",
      "R^2 score on training data: 106.93 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.95 13.51 16.45]\n",
      "R^2 score on training data: 106.96 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Ridge', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.88 13.11 15.87]\n",
      "R^2 score on training data: 107.85 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Ridge-Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.94 13.55 16.4 ]\n",
      "R^2 score on training data: 107.08 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Lasso-Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.62  9.32 10.6 ]\n",
      "R^2 score on training data: 197.48 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: KNN(task='Regression', k=3, distance_metric='Euclidean')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.69 9.23 9.1 ]\n",
      "R^2 score on training data: 54.96 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000027FE77CF6A0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.29 14.71 17.33]\n",
      "R^2 score on training data: 110.93 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=0.5, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000027FE77CEFC0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.89  8.88 10.33]\n",
      "R^2 score on training data: 106.20 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=0.5, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000027FE7690EA0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.54 4.24 4.23]\n",
      "R^2 score on training data: 112.34 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000027FE7690E00>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [5.53 5.13 5.14]\n",
      "R^2 score on training data: 463.45 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: DecisionTreeRegressor(criterion='mse', max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, min_impurity_decrease=0.0, ccp_alpha=0.0, root=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.64 9.71 9.71]\n",
      "R^2 score on training data: 72.55 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: RandomForestRegressor(n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=None, n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.7  8.88 8.88]\n",
      "R^2 score on training data: 87.57 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: VotingRegressor(estimators=[<Libraries.LinearRegression.LinearRegression object at 0x0000027FE828D4F0>, <Libraries.LinearRegression.LinearRegression object at 0x0000027FE828C320>, <Libraries.LinearRegression.LinearRegression object at 0x0000027FE828C5C0>, <Libraries.LinearRegression.LinearRegression object at 0x0000027FE828DC10>, <Libraries.LinearRegression.LinearRegression object at 0x0000027FE828E360>, <Libraries.LinearRegression.LinearRegression object at 0x0000027FE828F3E0>, <Libraries.KNN.KNN object at 0x0000027FE828C080>, <Libraries.SVM.SVR object at 0x0000027FE828C260>, <Libraries.SVM.SVR object at 0x0000027FE828C680>, <Libraries.SVM.SVR object at 0x0000027FE828C8F0>, <Libraries.SVM.SVR object at 0x0000027FE828CB00>, <Libraries.DecisionTree.DecisionTreeRegressor object at 0x0000027FE828CC80>, <Libraries.RandomForest.RandomForestRegressor object at 0x0000027FE828CD70>, <Libraries.AdaBoost.AdaBoostRegressor object at 0x0000027FE828CE30>, <Libraries.GradientBoost.GradientBoostRegressor object at 0x0000027FE828D1C0>], n_jobs=1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.71 10.66 12.13]\n",
      "R^2 score on training data: 90.99 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: AdaBoostRegressor(base_estimator=<Libraries.DecisionTree.DecisionTreeRegressor object at 0x0000027FE828D790>, n_estimators=50, learning_rate=1.0, estimators_=[], estimator_weights_=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.61 8.13 8.13]\n",
      "R^2 score on training data: 108.87 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: GradientBoostRegressor(base_estimator=<Libraries.DecisionTree.DecisionTreeRegressor object at 0x0000027FE828D940>, n_estimators=100, learning_rate=0.1, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, tol=1e-06, random_state=None, estimators_=[], train_loss_=[], val_loss_=[], best_iteration_=None, init_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.37 10.03 10.03]\n",
      "R^2 score on training data: 24.45 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        bg_model = BaggingClassifier(n_jobs=-1)\n",
    "        bg_model.fit(X, y_labels)\n",
    "        y_pred = bg_model.predict(X_new)\n",
    "        score_train = bg_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Defualt Bagging Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        bg_model = BaggingRegressor(n_jobs=-1)\n",
    "        bg_model.fit(X, y_output)\n",
    "        y_pred = bg_model.predict(X_new)\n",
    "        score_train = bg_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Defualt Bagging Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    \n",
    "\n",
    "# Apply differnet estimators bagging with all possible tasks \n",
    "# Tasks\n",
    "classification_estimators = [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                             KNN(k=3, task='Classification'), \n",
    "                             SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                             SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                             GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                             DecisionTreeClassifier(max_depth=3),\n",
    "                             RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                             VotingClassifier(estimators= [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                                                           KNN(k=3, task='Classification'), \n",
    "                                                           SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                                                           GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                                                           DecisionTreeClassifier(max_depth=3),\n",
    "                                                           RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                                                           AdaBoostClassifier(),\n",
    "                                                           GradientBoostClassifier()\n",
    "                                                           ], n_jobs=1),\n",
    "                             VotingClassifier(estimators= [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                                                           KNN(k=3, task='Classification'), \n",
    "                                                           SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                                                           GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                                                           DecisionTreeClassifier(max_depth=3),\n",
    "                                                           RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                                                           AdaBoostClassifier(),\n",
    "                                                           GradientBoostClassifier()\n",
    "                                                           ], n_jobs=1, voting='soft'),\n",
    "                             AdaBoostClassifier(),\n",
    "                             GradientBoostClassifier()                                                        \n",
    "                             ]\n",
    "regression_estimators = [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                         LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                         KNN(k=3, task='Regression'), \n",
    "                         SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                         SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                         DecisionTreeRegressor(max_depth=3),\n",
    "                         RandomForestRegressor(max_depth=3, n_jobs=-1),\n",
    "                         VotingRegressor(estimators= [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                                                      LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                                                      KNN(k=3, task='Regression'), \n",
    "                                                      SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                                                      SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                                                      DecisionTreeRegressor(max_depth=3),\n",
    "                                                      RandomForestRegressor(max_depth=3, n_jobs=-1),\n",
    "                                                      AdaBoostRegressor(),\n",
    "                                                      GradientBoostRegressor()\n",
    "                                                      ], n_jobs=1),\n",
    "                         AdaBoostRegressor(),\n",
    "                         GradientBoostRegressor()                             \n",
    "                         ]\n",
    "\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        for estimator in classification_estimators:\n",
    "            # Get Model of Bagging Classifier object\n",
    "            bg_model = BaggingClassifier(base_estimator=estimator, n_jobs=1)\n",
    "            bg_model.fit(X, y_labels)\n",
    "            y_pred = bg_model.predict(X_new)\n",
    "            score_train = bg_model.score(X, y_labels) * 100\n",
    "            print(f'Task: {task}')\n",
    "            # Get model class name\n",
    "            model_name = estimator.__class__.__name__\n",
    "            # Get model attributes\n",
    "            attributes = estimator.__dict__\n",
    "            # Convert dictionary to list then to string\n",
    "            formatted_params = \", \".join([f\"{k}='{v}'\" if isinstance(v, str) else f\"{k}={v}\" for k, v in attributes.items()])\n",
    "            # Concatenate model class name with its attributes\n",
    "            estimator_name = f\"{model_name}({formatted_params})\"\n",
    "            print(f\"Model: {estimator_name}\")\n",
    "            print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "            print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "            print('-'*40)  \n",
    "\n",
    "    elif task == 'Regression':\n",
    "        for estimator in regression_estimators:\n",
    "            # Get Model of Bagging Regressor object\n",
    "            bg_model = BaggingRegressor(base_estimator=estimator, n_jobs=1)\n",
    "            bg_model.fit(X, y_output)\n",
    "            y_pred = bg_model.predict(X_new)\n",
    "            score_train = bg_model.score(X, y_output) * 100\n",
    "            print(f'Task: {task}')\n",
    "            # Get model class name\n",
    "            model_name = estimator.__class__.__name__\n",
    "            # Get model attributes\n",
    "            attributes = estimator.__dict__\n",
    "            # Convert dictionary to list then to string\n",
    "            formatted_params = \", \".join([f\"{k}='{v}'\" if isinstance(v, str) else f\"{k}={v}\" for k, v in attributes.items()])\n",
    "            # Concatenate model class name with its attributes\n",
    "            estimator_name = f\"{model_name}({formatted_params})\"\n",
    "            print(f\"Model: {estimator_name}\")\n",
    "            print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "            print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "            print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
