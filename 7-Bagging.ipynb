{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53f18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models to use them within bagging technique\n",
    "# Known that bagging work with only one base estimator cloning it within bootsreapped data with replacement\n",
    "from Libraries.LinearRegression import *\n",
    "from Libraries.LogisticRegression import *\n",
    "from Libraries.KNN import *\n",
    "from Libraries.SVM import *\n",
    "from Libraries.NaiveBayes import *\n",
    "from Libraries.DecisionTree import *\n",
    "from Libraries.RandomForest import *\n",
    "from Libraries.Voting import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95559678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need this copy for cloning base estimator\n",
    "import copy\n",
    "# For parallelism and working with multiple of cores \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Bagging class for Classification and Regression\n",
    "# We work with the base class then inheritence happen to build the classification or regression bagging\n",
    "class BaggingBase():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, base_estimator = None, n_estimators = 10, random_state = None, n_jobs = None):\n",
    "        # Here we gonna define the base estimator later in classification or regression\n",
    "        # Others is the same default paramerts of API\n",
    "        # n_jobs=None or 1 is the same and -1 means using all cores\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = []\n",
    "\n",
    "\n",
    "    # We have helper functions\n",
    "    # Bootstrapping samples function\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        # Here we fetch the number of samples from x \n",
    "        # X is combination of samples x features\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Every time we select random numbers of indice with enabling replacement option so we can use same indice in more models\n",
    "        # So we can get the sample 0, 1 or multiple of times \n",
    "        # Here we choose randomly from all samples and we need the size of bootstrapped data = size of orginal data that is why replace option should be applied\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "\n",
    "        # Here we return the bootstrapped data\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    # Clone function\n",
    "    def _clone(self):\n",
    "        # Here we use the library of copy to return clone of base_estimator\n",
    "        # The idea here to siolate object per model\n",
    "        # It is only for learning phase\n",
    "        return copy.deepcopy(self.base_estimator)\n",
    "    \n",
    "    # Modeling function\n",
    "    # We will have one fit function per every model then combine all fits in general fit function for all memebers\n",
    "    # Fit single model\n",
    "    def _fit_single_model(self, X, y):\n",
    "        # First we recieve the bootstrapped data to model it from given X and y\n",
    "        X_samples, y_samples = self._bootstrap_sample(X, y)\n",
    "        \n",
    "        # Get cloned copy of base_estimator\n",
    "        # Every time we gonna pass the object of base estimator\n",
    "        cloned_model = self._clone()\n",
    "\n",
    "        # We use the fit function for specific single model which was already built from scratch before\n",
    "        cloned_model.fit(X_samples, y_samples)\n",
    "\n",
    "        # Every time we return the fitted model\n",
    "        return cloned_model\n",
    "    \n",
    "    # Fit all models function\n",
    "    # Here we gather all fitted models \n",
    "    def fit(self, X, y):\n",
    "        # We set random state wuth the number user entered and seed it\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # Loop over estimator number collecting all fitted models in one place to be ready for test phase\n",
    "        # We call _fit_single_model every time\n",
    "        self.models = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._fit_single_model)(X, y) for estimator in range(self.n_estimators)\n",
    "        )    \n",
    "        \n",
    "        # We return nothing\n",
    "        return self\n",
    "    \n",
    "    # Predict probabilities\n",
    "    # This function is ready to use in soft oting later\n",
    "    def predict_proba(self, X):\n",
    "        probas = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(model.predict_proba)(X) for model in self.models\n",
    "        )\n",
    "        probas = np.array(probas)\n",
    "        return np.mean(probas, axis=0)\n",
    "\n",
    "    # The same structure of fit will be built for predict\n",
    "    # We will have one predict function per every model then combine all predicts in general predict function for all memebers\n",
    "    # Predict single model\n",
    "    def _predict_single_model(self, cloned_model, X):\n",
    "        # We use the predict function for specific single model which was already built from scratch before\n",
    "        # We use test data\n",
    "        return cloned_model.predict(X)\n",
    "\n",
    "    # Precit all models function\n",
    "    # Here we gather all precited models \n",
    "    def predict(self, X):\n",
    "        # Loop over model collecting all predictions in one place to be ready for aggregation later using cpu cores (parallelism)\n",
    "        # We call _predict_single_model every time\n",
    "        # to apply predict we need to concatenate with fit first this is gathered in self.models\n",
    "        # Here every fitted model work with all samples \n",
    "        # So each sample have multiple predictions for all models\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._predict_single_model)(model, X) for model in self.models\n",
    "        )    \n",
    "        \n",
    "        # Preditction as numpy array\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # After collecting predictions we are ready for aggregation\n",
    "        return self._aggregate(predictions)\n",
    "\n",
    "    # Aggregation function    \n",
    "    # Aggregation to decide the final output\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _aggregate(self, predictions):\n",
    "        raise NotImplementedError  \n",
    "\n",
    "# Bagging ensemble Classifier class\n",
    "class BaggingClassifier(BaggingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, random_state=None, n_jobs=1):\n",
    "        \n",
    "        # Default estimator for classification as API\n",
    "        if base_estimator is None:\n",
    "            base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "        super().__init__(base_estimator, n_estimators, random_state, n_jobs)\n",
    "\n",
    "    # Aggregation function    \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # Get number of samples as every sample wich will be columns as every column represents sample and indexes represent models  \n",
    "        n_samples = predictions.shape[1]\n",
    "\n",
    "        # Gather all predictions in one array and make sure its data type the same as the source predictions array\n",
    "        # Intialize\n",
    "        final_predictions = np.zeros(n_samples, dtype=predictions.dtype)\n",
    "\n",
    "        # Apply majority voting \n",
    "        # Loop over all samples to get the most voted label\n",
    "        for sample in range(n_samples):\n",
    "            # Featch models of samples values\n",
    "            # Every sample represents column\n",
    "            votes = predictions[:, sample]\n",
    "\n",
    "            # Here we get unique classes then counting them\n",
    "            # We return two related arrays of classes and labels \n",
    "            # One for unique values and other for unique counts\n",
    "            # So return the index of laregest count will be equivelant to the label of largest count\n",
    "            classes, counts = np.unique(votes, return_counts=True)\n",
    "            majority_voting = classes[np.argmax(counts)]\n",
    "            final_predictions[sample] = majority_voting\n",
    "\n",
    "        # Return the final predictions of the test data\n",
    "        return final_predictions\n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "# Bagging ensemble Tree Regressor class\n",
    "class BaggingRegressor(BaggingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, random_state=None, n_jobs=1):\n",
    "        \n",
    "        # Default estimator for classification as API\n",
    "        if base_estimator is None:\n",
    "            base_estimator = DecisionTreeRegressor()\n",
    "\n",
    "        super().__init__(base_estimator, n_estimators, random_state, n_jobs)\n",
    "\n",
    "    # Aggregation function \n",
    "    def _aggregate(self, predictions):\n",
    "        # We just return the mean of every sample wich will be columns as every column represents sample and indexes represent trees\n",
    "        return np.mean(predictions, axis=0)  \n",
    "  \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1f9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Defualt Bagging Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 99.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Defualt Bagging Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.97 9.92 9.92]\n",
      "R^2 score on training data: 28.69 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Ridge', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Lasso', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 60.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: KNN(task='Classification', k=3, distance_metric='Euclidean')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 92.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AEF20>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AEE80>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AEDE0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AED40>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 73.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AECA0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AEC00>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AE840>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 85.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AE3E0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 64.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AE660>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AE200>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AE0C0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 88.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x00000268FB2AF060>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 61.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: GaussianNB(method='gaussian', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 88.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: MultinomialNB(method='multinomial', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: BernoulliNB(method='bernoulli', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, min_impurity_decrease=0.0, ccp_alpha=0.0, root=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 92.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: RandomForestClassifier(criterion='gini', n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=None, n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 92.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: VotingClassifier(estimators=[<Libraries.LogisticRegression.LogisticRegression object at 0x00000268FB28E7B0>, <Libraries.LogisticRegression.LogisticRegression object at 0x00000268FB28E900>, <Libraries.LogisticRegression.LogisticRegression object at 0x00000268FB28ED50>, <Libraries.KNN.KNN object at 0x00000268FB28E960>, <Libraries.SVM.SVC object at 0x00000268FB28EC60>, <Libraries.SVM.SVC object at 0x00000268FB28FBF0>, <Libraries.SVM.SVC object at 0x00000268FB28F140>, <Libraries.SVM.SVC object at 0x00000268FB28FAD0>, <Libraries.SVM.SVC object at 0x00000268FB28FD70>, <Libraries.SVM.SVC object at 0x00000268FB28E210>, <Libraries.SVM.SVC object at 0x00000268FB28FB60>, <Libraries.SVM.SVC object at 0x00000268FB28EEA0>, <Libraries.SVM.SVC object at 0x00000268FB28FCB0>, <Libraries.SVM.SVC object at 0x00000268FB28F470>, <Libraries.SVM.SVC object at 0x00000268FB28D2E0>, <Libraries.SVM.SVC object at 0x00000268FB28C230>, <Libraries.NaiveBayes.GaussianNB object at 0x00000268FB28CA10>, <Libraries.NaiveBayes.MultinomialNB object at 0x00000268FB28D940>, <Libraries.NaiveBayes.BernoulliNB object at 0x00000268FB28DD90>, <Libraries.DecisionTree.DecisionTreeClassifier object at 0x00000268FB28E3F0>, <Libraries.RandomForest.RandomForestClassifier object at 0x00000268FB28E8D0>], n_jobs=-1, models=[], voting='hard')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='OLS', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.29 13.95 16.89]\n",
      "R^2 score on training data: 92.87 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Normal', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.16 13.38 16.37]\n",
      "R^2 score on training data: 92.80 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.22 13.37 16.28]\n",
      "R^2 score on training data: 92.80 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Ridge', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.21 13.3  16.01]\n",
      "R^2 score on training data: 94.06 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Ridge-Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.13 13.12 16.01]\n",
      "R^2 score on training data: 93.03 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Lasso-Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.87  9.56 11.02]\n",
      "R^2 score on training data: 173.37 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: KNN(task='Regression', k=3, distance_metric='Euclidean')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.49 8.92 9.21]\n",
      "R^2 score on training data: 56.92 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x00000268FB2AF9C0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.15 13.   15.8 ]\n",
      "R^2 score on training data: 93.57 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=0.5, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x00000268FB2AFA60>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [7.17 3.96 7.28]\n",
      "R^2 score on training data: 90.88 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=0.5, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x00000268FB2AFB00>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [7.18 5.68 5.69]\n",
      "R^2 score on training data: 99.12 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x00000268FB2AFBA0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.37 6.05 6.02]\n",
      "R^2 score on training data: 392.40 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: DecisionTreeRegressor(criterion='mse', max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, min_impurity_decrease=0.0, ccp_alpha=0.0, root=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.93 9.96 9.96]\n",
      "R^2 score on training data: 73.38 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: RandomForestRegressor(n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=None, n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.24 8.57 8.57]\n",
      "R^2 score on training data: 85.23 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: VotingRegressor(estimators=[<Libraries.LinearRegression.LinearRegression object at 0x00000268FB042A80>, <Libraries.LinearRegression.LinearRegression object at 0x00000268FB320230>, <Libraries.LinearRegression.LinearRegression object at 0x00000268FB3224B0>, <Libraries.LinearRegression.LinearRegression object at 0x00000268FB320050>, <Libraries.LinearRegression.LinearRegression object at 0x00000268FB323B90>, <Libraries.LinearRegression.LinearRegression object at 0x00000268FB3213D0>, <Libraries.KNN.KNN object at 0x00000268FB3210D0>, <Libraries.SVM.SVR object at 0x00000268FB320FE0>, <Libraries.SVM.SVR object at 0x00000268FB321A60>, <Libraries.SVM.SVR object at 0x00000268FB322A20>, <Libraries.SVM.SVR object at 0x00000268FB320A10>, <Libraries.DecisionTree.DecisionTreeRegressor object at 0x00000268FB3233E0>, <Libraries.RandomForest.RandomForestRegressor object at 0x00000268FB322720>], n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.79 10.32 12.26]\n",
      "R^2 score on training data: 92.96 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        bg_model = BaggingClassifier(n_jobs=-1)\n",
    "        bg_model.fit(X, y_labels)\n",
    "        y_pred = bg_model.predict(X_new)\n",
    "        score_train = bg_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Defualt Bagging Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        bg_model = BaggingRegressor(n_jobs=-1)\n",
    "        bg_model.fit(X, y_output)\n",
    "        y_pred = bg_model.predict(X_new)\n",
    "        score_train = bg_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Defualt Bagging Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    \n",
    "\n",
    "# Apply differnet estimators bagging with all possible tasks \n",
    "# Tasks\n",
    "classification_estimators = [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                             KNN(k=3, task='Classification'), \n",
    "                             SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                             SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                             GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                             DecisionTreeClassifier(max_depth=3),\n",
    "                             RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                             VotingClassifier(estimators= [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                                                           KNN(k=3, task='Classification'), \n",
    "                                                           SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                                                           GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                                                           DecisionTreeClassifier(max_depth=3),\n",
    "                                                           RandomForestClassifier(max_depth=3, n_jobs=-1),], n_jobs=-1)\n",
    "                             ]\n",
    "regression_estimators = [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                         LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                         KNN(k=3, task='Regression'), \n",
    "                         SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                         SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                         DecisionTreeRegressor(max_depth=3),\n",
    "                         RandomForestRegressor(max_depth=3, n_jobs=-1),\n",
    "                         VotingRegressor(estimators= [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                                                      LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                                                      KNN(k=3, task='Regression'), \n",
    "                                                      SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                                                      SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                                                      DecisionTreeRegressor(max_depth=3),\n",
    "                                                      RandomForestRegressor(max_depth=3, n_jobs=-1),], n_jobs=-1)\n",
    "                         ]\n",
    "\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        for estimator in classification_estimators:\n",
    "            # Get Model of Bagging Classifier object\n",
    "            bg_model = BaggingClassifier(base_estimator=estimator, n_jobs=-1)\n",
    "            bg_model.fit(X, y_labels)\n",
    "            y_pred = bg_model.predict(X_new)\n",
    "            score_train = bg_model.score(X, y_labels) * 100\n",
    "            print(f'Task: {task}')\n",
    "            # Get model class name\n",
    "            model_name = estimator.__class__.__name__\n",
    "            # Get model attributes\n",
    "            attributes = estimator.__dict__\n",
    "            # Convert dictionary to list then to string\n",
    "            formatted_params = \", \".join([f\"{k}='{v}'\" if isinstance(v, str) else f\"{k}={v}\" for k, v in attributes.items()])\n",
    "            # Concatenate model class name with its attributes\n",
    "            estimator_name = f\"{model_name}({formatted_params})\"\n",
    "            print(f\"Model: {estimator_name}\")\n",
    "            print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "            print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "            print('-'*40)  \n",
    "\n",
    "    elif task == 'Regression':\n",
    "        for estimator in regression_estimators:\n",
    "            # Get Model of Bagging Regressor object\n",
    "            bg_model = BaggingRegressor(base_estimator=estimator, n_jobs=-1)\n",
    "            bg_model.fit(X, y_output)\n",
    "            y_pred = bg_model.predict(X_new)\n",
    "            score_train = bg_model.score(X, y_output) * 100\n",
    "            print(f'Task: {task}')\n",
    "            # Get model class name\n",
    "            model_name = estimator.__class__.__name__\n",
    "            # Get model attributes\n",
    "            attributes = estimator.__dict__\n",
    "            # Convert dictionary to list then to string\n",
    "            formatted_params = \", \".join([f\"{k}='{v}'\" if isinstance(v, str) else f\"{k}={v}\" for k, v in attributes.items()])\n",
    "            # Concatenate model class name with its attributes\n",
    "            estimator_name = f\"{model_name}({formatted_params})\"\n",
    "            print(f\"Model: {estimator_name}\")\n",
    "            print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "            print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "            print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
