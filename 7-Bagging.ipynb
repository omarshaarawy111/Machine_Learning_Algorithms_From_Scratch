{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53f18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models to use them within bagging technique\n",
    "# Known that bagging work with only one base estimator cloning it within bootsreapped data with replacement\n",
    "for i in range(2):\n",
    "    try:\n",
    "        from Libraries.LinearRegression import *\n",
    "        from Libraries.LogisticRegression import *\n",
    "        from Libraries.KNN import *\n",
    "        from Libraries.SVM import *\n",
    "        from Libraries.NaiveBayes import *\n",
    "        from Libraries.DecisionTree import *\n",
    "        from Libraries.RandomForest import *\n",
    "        from Libraries.Voting import *\n",
    "        from Libraries.AdaBoost import *\n",
    "        from Libraries.GradientBoost import *\n",
    "        from Libraries.XGradientBoost import *\n",
    "        import sys, importlib, shutil\n",
    "        shutil.rmtree(\"Libraries/__pycache__\", ignore_errors=True)\n",
    "        sys.modules.pop(\"Libraries.RandomForest\", None)\n",
    "        importlib.invalidate_caches()\n",
    "    except:\n",
    "        continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95559678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need this copy for cloning base estimator\n",
    "import copy\n",
    "# For parallelism and working with multiple of cores \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Bagging class for Classification and Regression\n",
    "# We work with the base class then inheritence happen to build the classification or regression bagging\n",
    "class BaggingBase():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, base_estimator = None, n_estimators = 10, random_state = None, n_jobs = None):\n",
    "        # Here we gonna define the base estimator later in classification or regression\n",
    "        # Others is the same default paramerts of API\n",
    "        # n_jobs=None or 1 is the same and -1 means using all cores\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = []\n",
    "\n",
    "\n",
    "    # We have helper functions\n",
    "    # Bootstrapping samples function\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        # Here we fetch the number of samples from x \n",
    "        # X is combination of samples x features\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Every time we select random numbers of indice with enabling replacement option so we can use same indice in more models\n",
    "        # So we can get the sample 0, 1 or multiple of times \n",
    "        # Here we choose randomly from all samples and we need the size of bootstrapped data = size of orginal data that is why replace option should be applied\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "\n",
    "        # Here we return the bootstrapped data\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    # Clone function\n",
    "    def _clone(self):\n",
    "        # Here we use the library of copy to return clone of base_estimator\n",
    "        # The idea here to siolate object per model\n",
    "        # It is only for learning phase\n",
    "        return copy.deepcopy(self.base_estimator)\n",
    "    \n",
    "    # Modeling function\n",
    "    # We will have one fit function per every model then combine all fits in general fit function for all memebers\n",
    "    # Fit single model\n",
    "    def _fit_single_model(self, X, y):\n",
    "        # First we recieve the bootstrapped data to model it from given X and y\n",
    "        X_samples, y_samples = self._bootstrap_sample(X, y)\n",
    "        \n",
    "        # Get cloned copy of base_estimator\n",
    "        # Every time we gonna pass the object of base estimator\n",
    "        cloned_model = self._clone()\n",
    "\n",
    "        # We use the fit function for specific single model which was already built from scratch before\n",
    "        cloned_model.fit(X_samples, y_samples)\n",
    "\n",
    "        # Every time we return the fitted model\n",
    "        return cloned_model\n",
    "    \n",
    "    # Fit all models function\n",
    "    # Here we gather all fitted models \n",
    "    def fit(self, X, y):\n",
    "        # We set random state wuth the number user entered and seed it\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # Loop over estimator number collecting all fitted models in one place to be ready for test phase\n",
    "        # We call _fit_single_model every time\n",
    "        self.models = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._fit_single_model)(X, y) for estimator in range(self.n_estimators)\n",
    "        )    \n",
    "        \n",
    "        # We return nothing\n",
    "        return self\n",
    "    \n",
    "    # Predict probabilities\n",
    "    # This function is ready to use in soft oting later\n",
    "    def predict_proba(self, X):\n",
    "        probas = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(model.predict_proba)(X) for model in self.models\n",
    "        )\n",
    "        probas = np.array(probas)\n",
    "        return np.mean(probas, axis=0)\n",
    "\n",
    "    # The same structure of fit will be built for predict\n",
    "    # We will have one predict function per every model then combine all predicts in general predict function for all memebers\n",
    "    # Predict single model\n",
    "    def _predict_single_model(self, cloned_model, X):\n",
    "        # We use the predict function for specific single model which was already built from scratch before\n",
    "        # We use test data\n",
    "        return cloned_model.predict(X)\n",
    "\n",
    "    # Precit all models function\n",
    "    # Here we gather all precited models \n",
    "    def predict(self, X):\n",
    "        # Loop over model collecting all predictions in one place to be ready for aggregation later using cpu cores (parallelism)\n",
    "        # We call _predict_single_model every time\n",
    "        # to apply predict we need to concatenate with fit first this is gathered in self.models\n",
    "        # Here every fitted model work with all samples \n",
    "        # So each sample have multiple predictions for all models\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._predict_single_model)(model, X) for model in self.models\n",
    "        )    \n",
    "        \n",
    "        # Preditction as numpy array\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # After collecting predictions we are ready for aggregation\n",
    "        return self._aggregate(predictions)\n",
    "\n",
    "    # Aggregation function    \n",
    "    # Aggregation to decide the final output\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _aggregate(self, predictions):\n",
    "        raise NotImplementedError  \n",
    "\n",
    "# Bagging ensemble Classifier class\n",
    "class BaggingClassifier(BaggingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, random_state=None, n_jobs=1):\n",
    "        \n",
    "        # Default estimator for classification as API\n",
    "        if base_estimator is None:\n",
    "            base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "        super().__init__(base_estimator, n_estimators, random_state, n_jobs)\n",
    "\n",
    "    # Aggregation function    \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # Get number of samples as every sample wich will be columns as every column represents sample and indexes represent models  \n",
    "        n_samples = predictions.shape[1]\n",
    "\n",
    "        # Gather all predictions in one array and make sure its data type the same as the source predictions array\n",
    "        # Intialize\n",
    "        final_predictions = np.zeros(n_samples, dtype=predictions.dtype)\n",
    "\n",
    "        # Apply majority voting \n",
    "        # Loop over all samples to get the most voted label\n",
    "        for sample in range(n_samples):\n",
    "            # Featch models of samples values\n",
    "            # Every sample represents column\n",
    "            votes = predictions[:, sample]\n",
    "\n",
    "            # Here we get unique classes then counting them\n",
    "            # We return two related arrays of classes and labels \n",
    "            # One for unique values and other for unique counts\n",
    "            # So return the index of laregest count will be equivelant to the label of largest count\n",
    "            classes, counts = np.unique(votes, return_counts=True)\n",
    "            majority_voting = classes[np.argmax(counts)]\n",
    "            final_predictions[sample] = majority_voting\n",
    "\n",
    "        # Return the final predictions of the test data\n",
    "        return final_predictions\n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "# Bagging ensemble Tree Regressor class\n",
    "class BaggingRegressor(BaggingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, random_state=None, n_jobs=1):\n",
    "        \n",
    "        # Default estimator for classification as API\n",
    "        if base_estimator is None:\n",
    "            base_estimator = DecisionTreeRegressor()\n",
    "\n",
    "        super().__init__(base_estimator, n_estimators, random_state, n_jobs)\n",
    "\n",
    "    # Aggregation function \n",
    "    def _aggregate(self, predictions):\n",
    "        # We just return the mean of every sample wich will be columns as every column represents sample and indexes represent trees\n",
    "        return np.mean(predictions, axis=0)  \n",
    "  \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1f9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Defualt Bagging Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 99.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Defualt Bagging Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.25 10.74 10.74]\n",
      "R^2 score on training data: 14.66 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 88.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Ridge', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 88.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: LogisticRegression(method='Lasso', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 67.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: KNN(task='Classification', k=3, distance_metric='Euclidean')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 91.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DB1A0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 88.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DB100>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DB060>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DAFC0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 38.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DAF20>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DAE80>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DAA20>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovo', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DACA0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 24.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DAB60>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DA8E0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=0.5, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DAAC0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 88.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: SVC(c=1.0, degree=3, gamma=1, coef0=1, tol=0.001, max_iter=1000, decision_function_shape='ovr', models=[], classes=None, kernel=<function SVC.__init__.<locals>.<lambda> at 0x0000026ABF4DB2E0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 40.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: GaussianNB(method='gaussian', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: MultinomialNB(method='multinomial', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 72.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: BernoulliNB(method='bernoulli', alpha=1.0)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 80.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, min_impurity_decrease=0.0, ccp_alpha=0.0, root=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 95.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: RandomForestClassifier(criterion='gini', n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=None, n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 93.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: VotingClassifier(estimators=[<Libraries.LogisticRegression.LogisticRegression object at 0x0000026ABE9E5640>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000026ABE9E55E0>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000026ABF360260>, <Libraries.KNN.KNN object at 0x0000026ABF258560>, <Libraries.SVM.SVC object at 0x0000026ABF259BE0>, <Libraries.SVM.SVC object at 0x0000026ABF259DF0>, <Libraries.SVM.SVC object at 0x0000026ABF259670>, <Libraries.SVM.SVC object at 0x0000026ABF259CA0>, <Libraries.SVM.SVC object at 0x0000026ABF259E80>, <Libraries.SVM.SVC object at 0x0000026ABF25A780>, <Libraries.SVM.SVC object at 0x0000026ABF25A900>, <Libraries.SVM.SVC object at 0x0000026ABF25AC60>, <Libraries.SVM.SVC object at 0x0000026ABEFE6AE0>, <Libraries.SVM.SVC object at 0x0000026ABF5A4CE0>, <Libraries.SVM.SVC object at 0x0000026ABF5A64B0>, <Libraries.SVM.SVC object at 0x0000026ABF5A7950>, <Libraries.NaiveBayes.GaussianNB object at 0x0000026ABF5A4290>, <Libraries.NaiveBayes.MultinomialNB object at 0x0000026ABF5A5100>, <Libraries.NaiveBayes.BernoulliNB object at 0x0000026ABF5A5310>, <Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000026ABF5A4C50>, <Libraries.RandomForest.RandomForestClassifier object at 0x0000026ABF5A5BE0>, <Libraries.AdaBoost.AdaBoostClassifier object at 0x0000026ABF5A4050>, <Libraries.GradientBoost.GradientBoostClassifier object at 0x0000026ABF5A5B50>, <Libraries.XGradientBoost.XGBoostClassifier object at 0x0000026ABF5A7D40>], n_jobs=-1, models=[], voting='hard', classes_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: VotingClassifier(estimators=[<Libraries.LogisticRegression.LogisticRegression object at 0x0000026ABF5A4680>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000026ABF5A5580>, <Libraries.LogisticRegression.LogisticRegression object at 0x0000026ABF5A7170>, <Libraries.KNN.KNN object at 0x0000026ABF5A50A0>, <Libraries.SVM.SVC object at 0x0000026ABF5A7EC0>, <Libraries.SVM.SVC object at 0x0000026ABF5A5130>, <Libraries.SVM.SVC object at 0x0000026ABF5A72F0>, <Libraries.SVM.SVC object at 0x0000026ABF5A4F50>, <Libraries.SVM.SVC object at 0x0000026ABF5A7320>, <Libraries.SVM.SVC object at 0x0000026ABF5A78F0>, <Libraries.SVM.SVC object at 0x0000026ABF5A7D70>, <Libraries.SVM.SVC object at 0x0000026ABF5A48F0>, <Libraries.SVM.SVC object at 0x0000026ABF5A62D0>, <Libraries.SVM.SVC object at 0x0000026ABF5A4F20>, <Libraries.SVM.SVC object at 0x0000026ABF5A6C30>, <Libraries.SVM.SVC object at 0x0000026ABF5A4770>, <Libraries.NaiveBayes.GaussianNB object at 0x0000026ABF5A72C0>, <Libraries.NaiveBayes.MultinomialNB object at 0x0000026ABF5A6480>, <Libraries.NaiveBayes.BernoulliNB object at 0x0000026ABF5A43E0>, <Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000026ABF5A46E0>, <Libraries.RandomForest.RandomForestClassifier object at 0x0000026ABF5A7B30>, <Libraries.AdaBoost.AdaBoostClassifier object at 0x0000026ABF5A7710>, <Libraries.GradientBoost.GradientBoostClassifier object at 0x0000026ABF5A52E0>, <Libraries.XGradientBoost.XGBoostClassifier object at 0x0000026ABF5A5CA0>], n_jobs=-1, models=[], voting='soft', classes_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: AdaBoostClassifier(base_estimator=<Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000026ABF5A76B0>, n_estimators=50, learning_rate=1.0, estimators_=[], estimator_weights_=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 90.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: GradientBoostClassifier(base_estimator=<Libraries.DecisionTree.DecisionTreeClassifier object at 0x0000026ABF5A6D80>, n_estimators=25, learning_rate=0.1, random_state=None, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, tol=1e-06, estimators_=[], train_loss_=[], val_loss_=[], best_iteration_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 94.00 %\n",
      "----------------------------------------\n",
      "Task: Classification\n",
      "Model: XGBoostClassifier(base_estimator=<Libraries.XGradientBoost.XGBTreeRegressor object at 0x0000026ABF5A5F10>, n_estimators=25, learning_rate=0.1, random_state=None, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, tol=1e-06, estimators_=[], train_loss_=[], val_loss_=[], best_iteration_=None, init_=None, classes_=None, n_classes_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 90.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='OLS', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.71 13.1  16.7 ]\n",
      "R^2 score on training data: 73.84 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Normal', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.61 12.71 16.32]\n",
      "R^2 score on training data: 74.22 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.7  12.98 16.56]\n",
      "R^2 score on training data: 73.85 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Ridge', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.71 13.   16.48]\n",
      "R^2 score on training data: 74.41 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Ridge-Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.77 12.93 16.35]\n",
      "R^2 score on training data: 74.58 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: LinearRegression(method='Lasso-Gradient Descent', learning_rate=0.1, n_iterations=1000, alpha=1.0, theta=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.1  10.72 12.56]\n",
      "R^2 score on training data: 157.98 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: KNN(task='Regression', k=3, distance_metric='Euclidean')\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.51 9.95 9.63]\n",
      "R^2 score on training data: 51.99 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000026ABF5544A0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.75 13.64 17.34]\n",
      "R^2 score on training data: 75.33 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=0.5, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000026ABF554540>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.47  4.39 10.47]\n",
      "R^2 score on training data: 74.80 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=0.5, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000026ABF5545E0>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.7  5.22 5.26]\n",
      "R^2 score on training data: 78.17 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: SVR(c=1.0, epsilon=0.1, degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000, kernel=<function SVR.__init__.<locals>.<lambda> at 0x0000026ABF554680>)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.57 6.11 6.1 ]\n",
      "R^2 score on training data: 630.62 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: DecisionTreeRegressor(criterion='mse', max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, min_impurity_decrease=0.0, ccp_alpha=0.0, root=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.28 10.37 10.37]\n",
      "R^2 score on training data: 56.13 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: RandomForestRegressor(n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=None, n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [7.46 9.38 9.38]\n",
      "R^2 score on training data: 78.17 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: VotingRegressor(estimators=[<Libraries.LinearRegression.LinearRegression object at 0x0000026ABF5A4E00>, <Libraries.LinearRegression.LinearRegression object at 0x0000026ABF5A6AE0>, <Libraries.LinearRegression.LinearRegression object at 0x0000026ABF5A6D20>, <Libraries.LinearRegression.LinearRegression object at 0x0000026ABF5A5610>, <Libraries.LinearRegression.LinearRegression object at 0x0000026ABF5A6900>, <Libraries.LinearRegression.LinearRegression object at 0x0000026ABF5A75F0>, <Libraries.KNN.KNN object at 0x0000026ABF5A4470>, <Libraries.SVM.SVR object at 0x0000026ABF5A6BD0>, <Libraries.SVM.SVR object at 0x0000026ABF5A7800>, <Libraries.SVM.SVR object at 0x0000026ABF5A6000>, <Libraries.SVM.SVR object at 0x0000026ABF5A49E0>, <Libraries.DecisionTree.DecisionTreeRegressor object at 0x0000026ABF5A68A0>, <Libraries.RandomForest.RandomForestRegressor object at 0x0000026ABF5A7A10>, <Libraries.AdaBoost.AdaBoostRegressor object at 0x0000026ABF5A5220>, <Libraries.GradientBoost.GradientBoostRegressor object at 0x0000026ABF1FD7C0>, <Libraries.XGradientBoost.XGBoostRegressor object at 0x0000026ABF5A64E0>], n_jobs=-1, models=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.84  9.94 11.79]\n",
      "R^2 score on training data: 72.72 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: AdaBoostRegressor(base_estimator=<Libraries.DecisionTree.DecisionTreeRegressor object at 0x0000026ABF5A4B00>, n_estimators=50, learning_rate=1.0, estimators_=[], estimator_weights_=[])\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [7.89 8.79 8.79]\n",
      "R^2 score on training data: 121.77 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: GradientBoostRegressor(base_estimator=<Libraries.DecisionTree.DecisionTreeRegressor object at 0x0000026ABF5A43B0>, n_estimators=25, learning_rate=0.1, random_state=None, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, tol=1e-06, estimators_=[], train_loss_=[], val_loss_=[], best_iteration_=None, init_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 7.02 10.13 10.13]\n",
      "R^2 score on training data: 52.62 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: XGBoostRegressor(base_estimator=<Libraries.XGradientBoost.XGBTreeRegressor object at 0x0000026ABF5A6960>, n_estimators=25, learning_rate=0.1, random_state=None, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, tol=1e-06, estimators_=[], train_loss_=[], val_loss_=[], best_iteration_=None, init_=None)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [7.26 9.37 9.37]\n",
      "R^2 score on training data: 83.83 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        bg_model = BaggingClassifier(n_jobs=-1)\n",
    "        bg_model.fit(X, y_labels)\n",
    "        y_pred = bg_model.predict(X_new)\n",
    "        score_train = bg_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Defualt Bagging Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        bg_model = BaggingRegressor(n_jobs=-1)\n",
    "        bg_model.fit(X, y_output)\n",
    "        y_pred = bg_model.predict(X_new)\n",
    "        score_train = bg_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Defualt Bagging Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    \n",
    "\n",
    "# Apply differnet estimators bagging with all possible tasks \n",
    "# Tasks\n",
    "classification_estimators = [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                             KNN(k=3, task='Classification'), \n",
    "                             SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                             SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                             GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                             DecisionTreeClassifier(max_depth=3),\n",
    "                             RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                             VotingClassifier(estimators= [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                                                           KNN(k=3, task='Classification'), \n",
    "                                                           SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                                                           GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                                                           DecisionTreeClassifier(max_depth=3),\n",
    "                                                           RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                                                           AdaBoostClassifier(),\n",
    "                                                           GradientBoostClassifier(n_estimators=25, early_stopping=True),\n",
    "                                                           XGBoostClassifier(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0) \n",
    "                                                           ], n_jobs=-1),\n",
    "                             VotingClassifier(estimators= [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                                                           KNN(k=3, task='Classification'), \n",
    "                                                           SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                                                           SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                                                           SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                                                           GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                                                           DecisionTreeClassifier(max_depth=3),\n",
    "                                                           RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                                                           AdaBoostClassifier(),\n",
    "                                                           GradientBoostClassifier(n_estimators=25, early_stopping=True),\n",
    "                                                           XGBoostClassifier(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0) \n",
    "                                                           ], n_jobs=-1, voting='soft'),\n",
    "                             AdaBoostClassifier(),\n",
    "                             GradientBoostClassifier(n_estimators=25, early_stopping=True),\n",
    "                             XGBoostClassifier(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0)                         \n",
    "                             ]\n",
    "regression_estimators = [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                         LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                         KNN(k=3, task='Regression'), \n",
    "                         SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                         SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                         DecisionTreeRegressor(max_depth=3),\n",
    "                         RandomForestRegressor(max_depth=3, n_jobs=-1),\n",
    "                         VotingRegressor(estimators= [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                                                      LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                                                      KNN(k=3, task='Regression'), \n",
    "                                                      SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                                                      SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                                                      DecisionTreeRegressor(max_depth=3),\n",
    "                                                      RandomForestRegressor(max_depth=3, n_jobs=-1),\n",
    "                                                      AdaBoostRegressor(),\n",
    "                                                      GradientBoostRegressor(n_estimators=25, early_stopping=True),\n",
    "                                                      XGBoostRegressor(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0)\n",
    "                                                      ], n_jobs=-1),\n",
    "                         AdaBoostRegressor(),\n",
    "                         GradientBoostRegressor(n_estimators=25, early_stopping=True),\n",
    "                         XGBoostRegressor(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0)  \n",
    "                         ]\n",
    "\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        for estimator in classification_estimators:\n",
    "            # Get Model of Bagging Classifier object\n",
    "            bg_model = BaggingClassifier(base_estimator=estimator, n_jobs=-1)\n",
    "            bg_model.fit(X, y_labels)\n",
    "            y_pred = bg_model.predict(X_new)\n",
    "            score_train = bg_model.score(X, y_labels) * 100\n",
    "            print(f'Task: {task}')\n",
    "            # Get model class name\n",
    "            model_name = estimator.__class__.__name__\n",
    "            # Get model attributes\n",
    "            attributes = estimator.__dict__\n",
    "            # Convert dictionary to list then to string\n",
    "            formatted_params = \", \".join([f\"{k}='{v}'\" if isinstance(v, str) else f\"{k}={v}\" for k, v in attributes.items()])\n",
    "            # Concatenate model class name with its attributes\n",
    "            estimator_name = f\"{model_name}({formatted_params})\"\n",
    "            print(f\"Model: {estimator_name}\")\n",
    "            print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "            print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "            print('-'*40)  \n",
    "\n",
    "    elif task == 'Regression':\n",
    "        for estimator in regression_estimators:\n",
    "            # Get Model of Bagging Regressor object\n",
    "            bg_model = BaggingRegressor(base_estimator=estimator, n_jobs=1)\n",
    "            bg_model.fit(X, y_output)\n",
    "            y_pred = bg_model.predict(X_new)\n",
    "            score_train = bg_model.score(X, y_output) * 100\n",
    "            print(f'Task: {task}')\n",
    "            # Get model class name\n",
    "            model_name = estimator.__class__.__name__\n",
    "            # Get model attributes\n",
    "            attributes = estimator.__dict__\n",
    "            # Convert dictionary to list then to string\n",
    "            formatted_params = \", \".join([f\"{k}='{v}'\" if isinstance(v, str) else f\"{k}={v}\" for k, v in attributes.items()])\n",
    "            # Concatenate model class name with its attributes\n",
    "            estimator_name = f\"{model_name}({formatted_params})\"\n",
    "            print(f\"Model: {estimator_name}\")\n",
    "            print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "            print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "            print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
