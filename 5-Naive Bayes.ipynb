{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Naive Bayes base class\n",
    "class BaseNB():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, method='gaussian', alpha=1.0):\n",
    "        self.method = method\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Guassian likelihood function\n",
    "    # For the Bernoulli and Multinomial functions they are just counting\n",
    "    def guassian_likelihood_log_prob(self, X, mean, var):\n",
    "        # Here we log result to avoid muliplication and convert it to summation for each class\n",
    "        log_probs = np.zeros((self.n_classes, X.shape[0]))\n",
    "        for c in range(self.n_classes):\n",
    "            # Add stability to avoid zero division\n",
    "            log_probs[c, :] = -0.5 * np.sum(np.log(2 * np.pi * self.var[c] + 1e-9) + ((X - self.mean[c]) ** 2) / (self.var[c] + 1e-9), axis=1)\n",
    "        return log_probs\n",
    "    \n",
    "    # Fit \n",
    "    def fit(self, X, y):\n",
    "        # In learning phase we calculate prior probabilities and likelihood parameters and we neglect evidence cause it is constant for all classes so saving time\n",
    "        # For perior probabilities we calculate all formula cause its parts are already paramters\n",
    "        # We calculate part of likelihood based on method which is parameter terms which add later to the full formula of likelihood\n",
    "        # Intialize parameters\n",
    "        # We collect unique classes\n",
    "        self.classes = np.unique(y)\n",
    "        # We calcualte the number of classes first\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        # Get number of rows and columns of training data\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        # Intilaize prior probabilities\n",
    "        self.classes_prior = np.zeros(self.n_classes)\n",
    "\n",
    "        # Get likelihood parameters based on method\n",
    "        # Intialize paramerts then will fill them in the general array for each class (filtering)\n",
    "        # Intialize Guassian parameters\n",
    "        if self.method == 'gaussian':\n",
    "            self.mean = np.zeros((self.n_classes, self.n_features))\n",
    "            self.var = np.zeros((self.n_classes, self.n_features))\n",
    "\n",
    "        # intialize Bernoulli and Multinomial parameters\n",
    "        elif self.method == 'bernoulli' or self.method == 'multinomial':\n",
    "            self.feature_count = np.zeros((self.n_classes, self.n_features))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Method not found.\")\n",
    "\n",
    "        # Loop over each class to calculate prior probability and likelihood parameters to get pesterior probability later \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Indexing is very important cause prior probabilities and likelihood parameters are arrays so we sort in order\n",
    "            # Filtering samples belong to current class then reduce sample space\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            # Calculate prior probability which will be count of samples belong to class / total samples belong to all classes\n",
    "            self.classes_prior[idx] = float(X_c.shape[0] / self.n_samples)\n",
    "\n",
    "            # Calculate likelihood parameters based on method for each class for all features\n",
    "            # Guassian likelihood Parameters\n",
    "            if self.method == 'gaussian':\n",
    "                # axis = 0 for mean and variance calculation along rows (features)\n",
    "                self.mean[idx] = X_c.mean(axis=0)\n",
    "                # Add stability to avoid zero division\n",
    "                self.var[idx] = X_c.var(axis=0) + 1e-9  \n",
    "\n",
    "            # Multinomial likelihood Parameters\n",
    "            elif self.method == 'multinomial':\n",
    "                # Add alpha for laplace smoothing to avoid deviding zero or being zero cause of one of more probabilities being zero\n",
    "                # Multinomianl is current feature count / smaples count for all features in class \n",
    "                # Here we devide by the sum of occurrences smaples in class\n",
    "                self.feature_count[idx] = (X_c.sum(axis=0) + self.alpha) / (X_c.sum() + self.alpha * self.n_features)\n",
    "\n",
    "            # Bernoulli likelihood Parameters\n",
    "            elif self.method == 'bernoulli':    \n",
    "                # Bernoulli is current feature count / samples number for currnet features in class\n",
    "                # We devide by the count of occurences of the feature in class\n",
    "                self.feature_count[idx] = (X_c.sum(axis=0) + self.alpha) / (X_c.shape[0] + 2 * self.alpha)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predict    \n",
    "    def predict(self, X):\n",
    "        # Calculate log posterior probability for each class which will be log prior probability + log likelihood\n",
    "        # Intialize log posterior and perior array\n",
    "        log_posteriors = np.zeros((self.n_classes, X.shape[0]))\n",
    "        \n",
    "        # Loop over each class to calculate log posterior probability\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Get log prior probability\n",
    "            # We don't need log_periors and log_likelihood to be arrays permentant matrix just temporary variable carry array every time\n",
    "            # Add stability to avoid zero addition\n",
    "            log_perior = np.log(self.classes_prior[idx] + 1e-9)\n",
    "\n",
    "            # Get log likelihood based on method and apply the full formula of likelihood\n",
    "            # Gaussian likelihood\n",
    "            if self.method == 'gaussian':   \n",
    "                log_likelihood_all = self.guassian_likelihood_log_prob(X, self.mean[idx], self.var[idx])\n",
    "                log_likelihood = log_likelihood_all[idx, :]\n",
    "\n",
    "            # Multinomial likelihood\n",
    "            elif self.method == 'multinomial':\n",
    "                # Here is the multinomial version of feature count\n",
    "                # Log probability calculation and we multiplication with new data so when word appears more times then its probability will be ehigher times and instead of power we use multiplication cause of log\n",
    "                # Use clip(min=1e-15) to prevent log(0) which leads to -inf or NaN\n",
    "                # We only clip the lower bound because the formula only involves log(probs)\n",
    "                # Even if probs reach 1.0, log(1.0) is 0, which is mathematically stable\n",
    "                # We choose 1e-15 as it is one of the safest smallest numbers in computer which is closest to zero without being zero and it is also small enough to not affect the probabilities significantly\n",
    "                probs = np.clip(self.feature_count[idx], 1e-15, 1.0)\n",
    "                log_likelihood = X @ np.log(probs.T)\n",
    "\n",
    "            # Bernoulli likelihood  \n",
    "            elif self.method == 'bernoulli':\n",
    "                # Here is the bernoulli version of feature count\n",
    "                # Log probability calculation and we multiplication with new data so when word appears or disappear then it affects its probability so we use addition of p and 1-p parts to cover all cases instead of mulitplication cause of log\n",
    "                # Use clip(1e-15, 1 - 1e-15) to prevent log(0) in both parts of the formula:\n",
    "                # 1. np.log(probs) -> requires probs > 0\n",
    "                # 2. np.log(1 - probs) -> requires probs < 1\n",
    "                # 3- We clip to less then one so practically no 100 % probabaility in real world also prevent more than one which leads to negative or NaN\n",
    "                # We choose 1e-15 as it is one of the safest smallest numbers in computer which is closest to zero without being zero and it is also small enough to not affect the probabilities significantly\n",
    "                probs = np.clip(self.feature_count[idx], 1e-15, 1 - 1e-15)\n",
    "                log_likelihood = X @ np.log(probs) + (1 - X) @ np.log(1 - probs)\n",
    "\n",
    "            # Final calculation of log posterior probability  \n",
    "            log_posteriors[idx, :] = log_perior + log_likelihood  \n",
    "\n",
    "        # Return the class with the highest log posterior probability\n",
    "        return self.classes[np.argmax(log_posteriors, axis=0)]\n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y) \n",
    "\n",
    "# Guassian Naive Bayes class\n",
    "class GaussianNB(BaseNB):\n",
    "    def __init__(self):\n",
    "        super().__init__(method='gaussian')\n",
    "\n",
    "# Multinomial Naive Bayes class\n",
    "class MultinomialNB(BaseNB):    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        # Here we need to add alpha parameter for child so we can pass it to the parent\n",
    "        super().__init__(method='multinomial', alpha=alpha)\n",
    "\n",
    "# Bernoulli Naive Bayes class\n",
    "class BernoulliNB(BaseNB):      \n",
    "    def __init__(self, alpha=1.0):\n",
    "        # Here we need to add alpha parameter for child so we can pass it to the parent\n",
    "        super().__init__(method='bernoulli', alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dab15eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: GaussianNB\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Method: MultinomialNB\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 78.00 %\n",
      "----------------------------------------\n",
      "Method: BernoulliNB\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 76.00 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# List of Naive bayes classifiers\n",
    "nb_classifiers = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(alpha=1.0),   \n",
    "    \"BernoulliNB\": BernoulliNB(alpha=1.0)\n",
    "                }    \n",
    "\n",
    "for method in nb_classifiers:\n",
    "    nb_model = nb_classifiers[method]\n",
    "    nb_model.fit(X, y_labels)\n",
    "    y_pred = np.round(nb_model.predict(X_new),2)\n",
    "    score_train = nb_model.score(X, y_labels) * 100\n",
    "    print(f'Method: {method}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print('-' * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
