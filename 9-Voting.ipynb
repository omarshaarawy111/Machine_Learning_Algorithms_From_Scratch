{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models to use them within voting technique\n",
    "# Known that voting work with mulitple of estimators with the same data set\n",
    "# So no bootstrapping \n",
    "# We have the same idea of aprallelism and voting with extra feature of soft voting so we work with probablities with pre aggregate and hard voting is after aggregate which is the same idea of bagging\n",
    "from Libraries.LinearRegression import *\n",
    "from Libraries.LogisticRegression import *\n",
    "from Libraries.KNN import *\n",
    "from Libraries.SVM import *\n",
    "from Libraries.NaiveBayes import *\n",
    "from Libraries.DecisionTree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ccaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need this copy for cloning base estimator\n",
    "import copy\n",
    "# For parallelism and working with multiple of cores \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class VotingBase():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, estimators = None, n_jobs = None):\n",
    "        # Here we gonna define the estimators used later in classification or regression\n",
    "        # Others is the same default paramerts of API\n",
    "        # n_jobs=None or 1 is the same\n",
    "        self.n_estimators = estimators\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = []\n",
    "\n",
    "    # Clone function\n",
    "    def _clone(self):\n",
    "        # Here we use the library of copy to return clone of base_estimator\n",
    "        # The idea here to siolate object per model\n",
    "        # It is only for learning phase\n",
    "        return copy.deepcopy(self.n_estimator)\n",
    "\n",
    "    # Modeling function\n",
    "    # We will have one fit function per every model then combine all fits in general fit function for all memebers\n",
    "    # Fit single model\n",
    "    def _fit_single_model(self,current_estimator X, y):      \n",
    "        # Get cloned copy of base_estimator object\n",
    "        # Every time we gonna pass the object of base estimator\n",
    "        # here we have to pass current estimator as it is not the same every time\n",
    "        cloned_model = self._clone(current_estimator)\n",
    "\n",
    "        # We use the fit function for specific single model which was already built from scratch before\n",
    "        # Here no bottstrapped data so we work with X and y directly\n",
    "        cloned_model.fit(X, y)\n",
    "\n",
    "        # Every time we return the fitted model\n",
    "        return cloned_model\n",
    "    \n",
    "    # Fit all models function\n",
    "    # Here we gather all fitted models \n",
    "    def fit(self, X, y):\n",
    "        # No random state exist\n",
    "        # Loop over estimator number collecting all fitted models in one place to be ready for test phase\n",
    "        # We call _fit_single_model every time\n",
    "        self.models = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._fit_single_model)(curtrent_estimator, X, y) for curtrent_estimator in self.n_estimators\n",
    "        )    \n",
    "        \n",
    "        # We return nothing\n",
    "        return self\n",
    "    \n",
    "    # The same structure of fit will be built for predict\n",
    "    # We will have one predict function per every model then combine all predicts in general predict function for all memebers\n",
    "    # Predict single model\n",
    "    def _predict_single_model(self, current_estimator, X):\n",
    "        # We use the predict function for specific single model which was already built from scratch before\n",
    "        # We use test data\n",
    "        return current_estimator.predict(X)\n",
    "    \n",
    "    # Precit all models function\n",
    "    # Here we gather all precited models \n",
    "    def predict(self, X):\n",
    "        # Loop over model collecting all predictions in one place to be ready for aggregation later using cpu cores (parallelism)\n",
    "        # We call _predict_single_model every time\n",
    "        # to apply predict we need to concatenate with fit first this is gathered in self.models\n",
    "        # Here every fitted model work with all samples \n",
    "        # So each sample have multiple predictions for all models\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._predict_single_model)(model, X) for model in self.models\n",
    "        )    \n",
    "        \n",
    "        # Preditction as numpy array\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # After collecting predictions we are ready for aggregation\n",
    "        return self._aggregate(predictions)\n",
    "    \n",
    "    # Aggregation function    \n",
    "    # Aggregation to decide the final output\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _aggregate(self, predictions):\n",
    "        raise NotImplementedError  \n",
    "    \n",
    "# Voting ensemble Classifier class\n",
    "class VotingClassifier(VotingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, estimators = None, n_jobs=1, voting = 'hard'):\n",
    "        \n",
    "        super().__init__(estimators, n_jobs)\n",
    "\n",
    "        # Here handling error of voting know that default is hard voting\n",
    "        if voting not in (\"hard\", \"soft\"):\n",
    "            raise ValueError(\"Voting must be 'hard' or 'soft'.\")\n",
    "        \n",
    "        self.voting = voting\n",
    "\n",
    "    # Aggregation function  \n",
    "    # This represents hard voting which is the same as bagging  \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # Get number of samples as every sample wich will be columns as every column represents sample and indexes represent models  \n",
    "        n_samples = predictions.shape[1]\n",
    "\n",
    "        # Gather all predictions in one array and make sure its data type the same as the source predictions array\n",
    "        # Intialize\n",
    "        final_predictions = np.zeros(n_samples, dtype=predictions.dtype)\n",
    "\n",
    "        # Apply majority voting \n",
    "        # Loop over all samples to get the most voted label\n",
    "        for sample in range(n_samples):\n",
    "            # Featch models of samples values\n",
    "            # Every sample represents column\n",
    "            votes = predictions[:, sample]\n",
    "\n",
    "            # Here we get unique classes then counting them\n",
    "            # We return two related arrays of classes and labels \n",
    "            # One for unique values and other for unique counts\n",
    "            # So return the index of laregest count will be equivelant to the label of largest count\n",
    "            classes, counts = np.unique(votes, return_counts=True)\n",
    "            majority_voting = classes[np.argmax(counts)]\n",
    "            final_predictions[sample] = majority_voting\n",
    "\n",
    "        # Return the final predictions of the test data\n",
    "        return final_predictions\n",
    "    \n",
    "    # Predict function\n",
    "    def predict(self, X):\n",
    "        # Here we need to add new logic of soft voting and for hard vote we just use the predict function of the base class\n",
    "        if self.voting == \"soft\":\n",
    "            probas = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(model.predict_proba)(X)\n",
    "                for model in self.models\n",
    "            )\n",
    "            probas = np.array(probas)\n",
    "            return self._aggregate(probas)\n",
    "        \n",
    "        # Hard voting logic is the same depends on base class\n",
    "        return super().predict(X)()\n",
    "\n",
    "\n",
    "# Voting ensemble Tree Regressor class\n",
    "class VotingRegressor(VotingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, estimators = None, n_jobs=1):\n",
    "        \n",
    "        # Here we handle if not estimator we raise and error\n",
    "        if estimators is None:\n",
    "            raise ValueError('No estimators have been passed.')\n",
    "\n",
    "        super().__init__(estimators, n_jobs)\n",
    "\n",
    "    # Aggregation function \n",
    "    def _aggregate(self, predictions):\n",
    "        # We just return the mean of every sample wich will be columns as every column represents sample and indexes represent models\n",
    "        return np.mean(predictions, axis=0)  \n",
    "  \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
