{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models to use them within voting technique\n",
    "# Known that voting work with mulitple of estimators with the same data set\n",
    "# So no bootstrapping \n",
    "# We have the same idea of parallelism and voting with extra feature of soft voting so we work with probablities with pre aggregate and hard voting is after aggregate which is the same idea of bagging\n",
    "from Libraries.LinearRegression import *\n",
    "from Libraries.LogisticRegression import *\n",
    "from Libraries.KNN import *\n",
    "from Libraries.SVM import *\n",
    "from Libraries.NaiveBayes import *\n",
    "from Libraries.DecisionTree import *\n",
    "from Libraries.RandomForest import *\n",
    "from Libraries.Bagging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2ccaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For parallelism and working with multiple of cores \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class VotingBase():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, estimators = None, n_jobs = None):\n",
    "        # Here we gonna define the estimators used later in classification or regression\n",
    "        # Others is the same default paramerts of API\n",
    "        # n_jobs=None or 1 is the same and -1 means using all cores\n",
    "        self.estimators = estimators\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = []\n",
    "\n",
    "   \n",
    "    # Modeling function\n",
    "    # We will have one fit function per every model then combine all fits in general fit function for all memebers\n",
    "    # Fit single model\n",
    "    def _fit_single_model(self,current_estimator, X, y):      \n",
    "        # Get estimator from the list of estimators and fit it directly \n",
    "        # No need to create helper function like _make_estimator or _clone\n",
    "        # Every time we gonna pass the object of current estimator\n",
    "        # We use the fit function for specific single model which was already built from scratch before\n",
    "        # Here no bottstrapped data so we work with X and y directly\n",
    "        current_estimator.fit(X, y)\n",
    "\n",
    "        # Every time we return the fitted model\n",
    "        return current_estimator\n",
    "    \n",
    "    # Fit all models function\n",
    "    # Here we gather all fitted models \n",
    "    def fit(self, X, y):\n",
    "        # No random state exist\n",
    "        # Loop over estimator number collecting all fitted models in one place to be ready for test phase\n",
    "        # We call it n_estimators but it is actually a list of estimators not a number\n",
    "        # We call _fit_single_model every time\n",
    "        self.models = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._fit_single_model)(curtrent_estimator, X, y) for curtrent_estimator in self.estimators\n",
    "        )    \n",
    "        \n",
    "        # We return nothing\n",
    "        return self\n",
    "    \n",
    "    # The same structure of fit will be built for predict\n",
    "    # We will have one predict function per every model then combine all predicts in general predict function for all memebers\n",
    "    # Predict single model\n",
    "    def _predict_single_model(self, current_estimator, X):\n",
    "        # We use the predict function for specific single model which was already built from scratch before\n",
    "        # We use test data\n",
    "        return current_estimator.predict(X)\n",
    "    \n",
    "    # Precit all models function\n",
    "    # Here we gather all precited models \n",
    "    def predict(self, X):\n",
    "        # Loop over model collecting all predictions in one place to be ready for aggregation later using cpu cores (parallelism)\n",
    "        # We call _predict_single_model every time\n",
    "        # to apply predict we need to concatenate with fit first this is gathered in self.models\n",
    "        # Here every fitted model work with all samples \n",
    "        # So each sample have multiple predictions for all models\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._predict_single_model)(model, X) for model in self.models\n",
    "        )    \n",
    "        \n",
    "        # Preditction as numpy array\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # After collecting predictions we are ready for aggregation\n",
    "        return self._aggregate(predictions)\n",
    "    \n",
    "    # Aggregation function    \n",
    "    # Aggregation to decide the final output\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _aggregate(self, predictions):\n",
    "        raise NotImplementedError  \n",
    "    \n",
    "# Voting ensemble Classifier class\n",
    "class VotingClassifier(VotingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, estimators = None, n_jobs=1, voting = 'hard'):\n",
    "        \n",
    "         # Here we handle if not estimator we raise and error\n",
    "        if estimators is None:\n",
    "            raise ValueError('No estimators have been passed.')\n",
    "        \n",
    "        super().__init__(estimators, n_jobs)\n",
    "\n",
    "        # Here handling error of voting know that default is hard voting\n",
    "        if voting not in (\"hard\", \"soft\"):\n",
    "            raise ValueError(\"Voting must be 'hard' or 'soft'.\")\n",
    "        \n",
    "        self.voting = voting\n",
    "\n",
    "    # Aggregation function  \n",
    "    # This represents hard voting which is the same as bagging \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # Get number of samples as every sample wich will be columns as every column represents sample and indexes represent models  \n",
    "        n_samples = predictions.shape[1]\n",
    "\n",
    "        # Gather all predictions in one array and make sure its data type the same as the source predictions array\n",
    "        # Intialize\n",
    "        final_predictions = np.zeros(n_samples, dtype=predictions.dtype)\n",
    "\n",
    "        # Apply majority voting \n",
    "        # Loop over all samples to get the most voted label\n",
    "        for sample in range(n_samples):\n",
    "            # Featch models of samples values\n",
    "            # Every sample represents column\n",
    "            votes = predictions[:, sample]\n",
    "\n",
    "            # Here we get unique classes then counting them\n",
    "            # We return two related arrays of classes and labels \n",
    "            # One for unique values and other for unique counts\n",
    "            # So return the index of laregest count will be equivelant to the label of largest count\n",
    "            classes, counts = np.unique(votes, return_counts=True)\n",
    "            majority_voting = classes[np.argmax(counts)]\n",
    "            final_predictions[sample] = majority_voting\n",
    "\n",
    "        # Return the final predictions of the test data\n",
    "        return final_predictions\n",
    "    \n",
    "    # Predict function\n",
    "    def predict(self, X):\n",
    "        if self.voting == \"soft\":\n",
    "            # Get probabilities from all models\n",
    "            probas = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(model.predict_proba)(X) for model in self.models\n",
    "            )\n",
    "\n",
    "            # shape is 3D: (n_models, n_samples, n_classes)\n",
    "            probas = np.array(probas) \n",
    "\n",
    "            # Average probabilities across models\n",
    "            # shape is 2D: (n_samples, n_classes)\n",
    "            avg_proba = np.mean(probas, axis=0)  \n",
    "\n",
    "            # Step 3: Choose class with highest probability\n",
    "            # Shape is 1D: (n_samples,) and value is the class label with highest average probability\n",
    "            final_labels = np.argmax(avg_proba, axis=1)  \n",
    "\n",
    "            return final_labels\n",
    "        \n",
    "        # Hard voting logic is the same depends on base class\n",
    "        return super().predict(X)\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "# Voting ensemble Tree Regressor class\n",
    "class VotingRegressor(VotingBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, estimators = None, n_jobs=1):\n",
    "        \n",
    "        # Here we handle if not estimator we raise and error\n",
    "        if estimators is None:\n",
    "            raise ValueError('No estimators have been passed.')\n",
    "\n",
    "        super().__init__(estimators, n_jobs)\n",
    "\n",
    "    # Aggregation function \n",
    "    def _aggregate(self, predictions):\n",
    "        # We just return the mean of every sample wich will be columns as every column represents sample and indexes represent models\n",
    "        return np.mean(predictions, axis=0)  \n",
    "  \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa10e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Voting Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 90.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Voting Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 6.85 10.47 12.42]\n",
      "R^2 score on training data: 80.22 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply differnet estimators bagging with all possible tasks \n",
    "# Tasks\n",
    "classification_estimators = [LogisticRegression(method='Gradient Descent'), LogisticRegression(method='Ridge'), LogisticRegression(method='Lasso'), \n",
    "                             KNN(k=3, task='Classification'), \n",
    "                             SVC(kernel='Linear'), SVC(kernel='Polynomial', gamma=0.5), \n",
    "                             SVC(kernel='RBF', gamma=0.5), SVC(kernel='Sigmoid'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovo'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovo'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'), SVC(kernel='Sigmoid',decision_function_shape='ovo'),\n",
    "                             SVC(kernel='Linear',decision_function_shape='ovr'), SVC(kernel='Polynomial', gamma=0.5,decision_function_shape='ovr'), \n",
    "                             SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'), SVC(kernel='Sigmoid',decision_function_shape='ovr'),\n",
    "                             GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0),\n",
    "                             DecisionTreeClassifier(max_depth=3),\n",
    "                             RandomForestClassifier(max_depth=3, n_jobs=-1),\n",
    "                             BaggingClassifier(n_jobs=-1)\n",
    "                             ]\n",
    "regression_estimators = [LinearRegression(method='OLS'), LinearRegression(method='Normal'), LinearRegression(method='Gradient Descent'), \n",
    "                         LinearRegression(method='Ridge'), LinearRegression(method='Ridge-Gradient Descent'), LinearRegression(method='Lasso-Gradient Descent'),\n",
    "                         KNN(k=3, task='Regression'), \n",
    "                         SVR(kernel='Linear'), SVR(kernel='Polynomial', gamma=0.5), \n",
    "                         SVR(kernel='RBF', gamma=0.5), SVR(kernel='Sigmoid'),\n",
    "                         DecisionTreeRegressor(max_depth=3),\n",
    "                         RandomForestRegressor(max_depth=3, n_jobs=-1),\n",
    "                         BaggingRegressor(n_jobs=-1)\n",
    "                         ]\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Model of Voting Classifier object\n",
    "        vt_model = VotingClassifier(estimators=classification_estimators, n_jobs=-1)\n",
    "        vt_model.fit(X, y_labels)\n",
    "        y_pred = vt_model.predict(X_new)\n",
    "        score_train = vt_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Voting Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "        print('-'*40)  \n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Model of Voting Regressor object\n",
    "        vt_model = VotingRegressor(estimators=regression_estimators, n_jobs=-1)\n",
    "        vt_model.fit(X, y_output)\n",
    "        y_pred = vt_model.predict(X_new)\n",
    "        score_train = vt_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Voting Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}s')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "        print('-'*40)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
