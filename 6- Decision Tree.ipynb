{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81be87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Decision Tree class for Classification and Regression\n",
    "# We start with the core class which is related to node \n",
    "class Node():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, value = None):\n",
    "        # We need to intailize everything of node properties (features, threshold, left, right, value)\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value \n",
    "# We work with the base class then inheritence happen to build the classification or regression tree\n",
    "class BaseDecisionTree():\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, criterion, max_depth = None, min_samples_split = 2, min_samples_leaf = 1, max_features = None, min_impurity_decrease = 0.0, ccp_alpha = 0.0):\n",
    "        # Here we intialize tree properties there is mandatory and optional propeties based on use case and also your prunning decision   \n",
    "        # min_samples_split = 2, min_samples_leaf = 1 these are mandatory cause they are logically the min amount to split or work rather than it isn't logical\n",
    "        # min_impurity_decrease = 0.0, ccp_alpha = 0.0 becasue the default is no pruning till you decide\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.root = None\n",
    "\n",
    "    # Helper functions and will be called in build_tree later\n",
    "    # Impurity functions\n",
    "    # Categorical impurities\n",
    "    # Gini\n",
    "    def _gini(self, y):\n",
    "        # We collect probabilities then calculate\n",
    "        probs = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "    \n",
    "    # Numerical impurity\n",
    "    # Entropy\n",
    "    def _entropy(self, y):\n",
    "        # We collect probabilities then calculate\n",
    "        probs = np.bincount(y) / len(y)\n",
    "        # Add stability to avoid log(0) as the result is \\(\\log _{2}(0)\\) is undefined.Â \n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
    "    \n",
    "    def _mse(self, y):\n",
    "        # Get the MSE = varinace\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "    \n",
    "    # Accumlative function to make all criterion in one place\n",
    "    def _impurity(self, y):\n",
    "        if self.criterion == \"gini\":\n",
    "            return self._gini(y)\n",
    "        \n",
    "        elif self.criterion == \"entropy\":\n",
    "            return self._entropy(y)\n",
    "        \n",
    "        elif self.criterion == \"mse\":\n",
    "            return self._mse(y)\n",
    "\n",
    "    # Best split function\n",
    "    def _best_split(self, X, y, features=None):\n",
    "        # Here we need to have the best split by having samples and features\n",
    "        # This function to get the best feature with all related properties and later in _build_tree we will assign samples actually\n",
    "        # My target is to reach the best info gain or gini gain \n",
    "        # We have internal splits for every feature to choose the best gain \n",
    "        # Then compare it with others features to decide which is the best among all internal and features split to choose this feature\n",
    "        # Intialize the best gain first\n",
    "        best_gain = -1\n",
    "\n",
    "        # My target is to retrieve best feature with best threshold\n",
    "        split_feature, split_threshold = None, None\n",
    "\n",
    "        # Calcualte the parent impurity so later we calcualte info gain or gini gain \n",
    "        parent_impurity = self._impurity(y)\n",
    "\n",
    "        # Number of features\n",
    "        n_features = X.shape[1]\n",
    "        if features is None:\n",
    "            features = np.arange(n_features)\n",
    "\n",
    "        # Gains score list\n",
    "        gains_score = []\n",
    "        for feature in features:\n",
    "            # First we decalre the thresholds for internal splits\n",
    "            # Thresholds come from features itself so coming from x\n",
    "            # As we know acedemically that continuos values (nuu=merical features) we take the midpoint between each pairs\n",
    "            # And for discrete values (Categorical features) we take the unique labels\n",
    "            # Preprocessing for data happen and eventually we got numbers wether it is categorical features or numerical one\n",
    "            # So in both cases sickit learn API take the midpoint as midpoint covers all cases and get less number of thresholds\n",
    "            # We need the all samples of X within this feature\n",
    "            # We got sorted non repeatable array (Core)\n",
    "            values = np.unique(X[:, feature])\n",
    "\n",
    "            # Midpoints fetched and stored in array \n",
    "            # Midpoint = (sum of each pair) / 2 \n",
    "            # Start at 0,1 index then 1,2 and so on\n",
    "            thresholds = (values[:-1] + values[1:]) / 2\n",
    "\n",
    "            # Loop over thresholds to see the highest one \n",
    "            for threshold in thresholds:\n",
    "                # Samples role\n",
    "                # We have left and right children\n",
    "                # Left samples gonna be less or equal threshold\n",
    "                # Right samples gonna be other wise\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                # If one child has no samples then immediately stop the current threshold\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Target role to calculate gian info or gini info\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                # total number of unique target\n",
    "                n_y = len(y)    \n",
    "\n",
    "                # Average weighted impurity\n",
    "                weighted_average_impurity = (\n",
    "                    len(left_y) / n_y * self._impurity(left_y)\n",
    "                    + len(right_y) / n_y * self._impurity(right_y)\n",
    "                )\n",
    "\n",
    "                # Gain info = Gini info\n",
    "                gain = parent_impurity - weighted_average_impurity\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    # By deafult first time we pass the condition as min value of gain is 0 which is bigger than -1\n",
    "                    best_gain = gain\n",
    "                    split_feature = feature\n",
    "                    split_threshold = threshold\n",
    "\n",
    "            # Add the best gain of the current feature to the score list then later we choose the highest score from it\n",
    "            gains_score.append([best_gain, split_feature, split_threshold])\n",
    "            \n",
    "        # Get the highest score\n",
    "        max_gain = max(gains_score, key = lambda x:x[0])\n",
    "\n",
    "        # Return the all properties of the best features\n",
    "        # Return best_gain we will use it as stopping criteria for min impurity decrease\n",
    "        # Return split_feature to work with and build tree\n",
    "        # Return split_threshold to assign features in tree\n",
    "        return max_gain[0], max_gain[1], max_gain[2]\n",
    "\n",
    "    # Check the purity function\n",
    "    def _is_pure(self, y):\n",
    "        # We could know by having only one label \n",
    "        # This fucntion is exclusive for the classification only and not for regression as we never reach the exact value\n",
    "        return len(np.unique(y)) == 1\n",
    "\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly for leaf_value for prediction later\n",
    "    def _leaf_value(self, y):\n",
    "        raise NotImplementedError    \n",
    "\n",
    "    # For building our tree \n",
    "    # This is the most important function with the help of other helper functions\n",
    "    def _build_tree(self, X, y, depth):  \n",
    "        # Ofcourse we need to retrieve smaples and fatures count\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Stop case\n",
    "        # Stopping criteria (pre prunning)\n",
    "        # The case of leaf node is automatic stopping criteria\n",
    "        # The cases of inaccurate number of samples in the node or exceed the max depth is hyper paramter stopping cirteria\n",
    "        if self._is_pure(y) or n_samples < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return Node(value=self._leaf_value(y))\n",
    "\n",
    "        # This part is related to random forest as here we don't want to work with all features to smaller correlation between tress, higher variance and cut the dominance of one feature\n",
    "        # Note that it is just hyper parametr\n",
    "        # Indexing feature for future use\n",
    "        features_idx = np.arange(n_features)\n",
    "        # Make sure we adjust that parametr\n",
    "        if self.max_features is not None:\n",
    "            # Make it random number from predefined range \n",
    "            features_idx = np.random.choice(\n",
    "                n_features, self.max_features, replace=False\n",
    "            )\n",
    "\n",
    "        # Step 1 ---> find the best feature with all properties\n",
    "        # Return the all properties of the best features from best_split function\n",
    "        # Return best_gain we will use it as stopping criteria for min impurity decrease\n",
    "        # Return split_feature to work with and build tree\n",
    "        # Return split_threshold to assign features in tree\n",
    "        best_gain, split_feature, split_threshold = self._best_split(X, y, features_idx)\n",
    "\n",
    "        # Stop case\n",
    "        # Stopping criteria (pre prunning)\n",
    "        # The case of non split feature is automatic stopping criteria\n",
    "        # The cases of min impurity decrease is hyper paramter stopping cirteria\n",
    "        if split_feature is None or best_gain < self.min_impurity_decrease:\n",
    "            return Node(value=self._leaf_value(y))\n",
    "\n",
    "        # Step 2 ---> assign samples in tree\n",
    "        # Samples role\n",
    "        # We have left and right children\n",
    "        # Left samples gonna be less or equal threshold\n",
    "        # Right samples gonna be other wise\n",
    "        left_mask = X[:, split_feature] <= split_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Stop case\n",
    "        # Stopping criteria (pre prunning)\n",
    "        # The cases of inaccurate number of samples in the leaf (after split)\n",
    "        # We can't gathering all stopping conditions at one place as each step has its own stopping criteria\n",
    "        if left_mask.sum() < self.min_samples_leaf or right_mask.sum() < self.min_samples_leaf :\n",
    "            return Node(value=self._leaf_value(y))\n",
    "\n",
    "        # Recusive case\n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        # We return Node  bacause we need all information about tree so we can move through it in prediction knowing conditions\n",
    "        return Node(split_feature, split_threshold, left, right)   \n",
    "\n",
    "    # Post prunning function\n",
    "    def _prune(self, node):\n",
    "        # We pass the tree for the function to post prune\n",
    "        if node.left is None or node.right is None:\n",
    "            return self._impurity(np.array([node.value])), 1\n",
    "\n",
    "        left_err, left_leaves = self._prune(node.left)\n",
    "        right_err, right_leaves = self._prune(node.right)\n",
    "\n",
    "        subtree_error = left_err + right_err\n",
    "        leaf_error = self._impurity(np.array([node.value]))\n",
    "\n",
    "        if leaf_error + self.ccp_alpha <= subtree_error:\n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            return leaf_error, 1\n",
    "\n",
    "        return subtree_error, left_leaves + right_leaves\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        # The fit is building our tree and save it in root as symbol of our journey starting with pre prunning hyper paramerts\n",
    "        self.root = self._build_tree(X, y, 0)\n",
    "        \n",
    "        # The decision of post pruning based on hyper  cc_aplpha greater than zero\n",
    "        if self.ccp_alpha > 0:\n",
    "            self._prune(self.root)\n",
    "\n",
    "    # Predict \n",
    "    def predict(self, X):\n",
    "        # Each observation has the journey till we reach the suitable leaf node based on condtions\n",
    "        def traverse(x, node):\n",
    "            if node.value is not None:\n",
    "                # Stop case\n",
    "                # We reach leaf node\n",
    "                return node.value\n",
    "            \n",
    "            # Recursive case\n",
    "            # I keep moving till i reach the value of node which is the leaf node (end of journey)\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return traverse(x, node.left)\n",
    "            return traverse(x, node.right)\n",
    "        \n",
    "        # We just loop on new observation one by one\n",
    "        return np.array([traverse(x, self.root) for x in X])\n",
    "\n",
    "    # Simple visual tree print function\n",
    "    def print_tree(self, node=None, indent=\"\"):\n",
    "        # It is simple simulation of tree\n",
    "        if node is None:\n",
    "            # Get tree\n",
    "            node = self.root\n",
    "            \n",
    "        if node.value is not None:\n",
    "            # Stop case\n",
    "            # For leaf node ---> it has value of course\n",
    "            print(indent + \"Leaf:\", node.value.round(2))\n",
    "        else:\n",
    "            # Recursive case\n",
    "            # Here the 2nd importance of getting threshold for drawing\n",
    "            print(indent + f\"X[{node.feature}] <= {node.threshold.round(2)}\")\n",
    "            self.print_tree(node.left, indent + \"  \")\n",
    "            self.print_tree(node.right, indent + \"  \")\n",
    "\n",
    "# Decsion Tree Classifier class\n",
    "class DecisionTreeClassifier(BaseDecisionTree):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass unknown numbers of paramters to parent class and also at the same time get unknown numbers of paramters\n",
    "    def __init__(self, criterion=\"gini\", **kwargs):\n",
    "        super().__init__(criterion=criterion, **kwargs)\n",
    "\n",
    "    # Here the value leaf is the most majority class\n",
    "    def _leaf_value(self, y):\n",
    "        # We retrieve index which is the most label occur\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "\n",
    "# Decsion Tree Regressor class\n",
    "class DecisionTreeRegressor(BaseDecisionTree):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass unknown numbers of paramters to parent class and also at the same time get unknown numbers of paramters\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(criterion=\"mse\", **kwargs)\n",
    "\n",
    "    # Here the value leaf is the mean of y values\n",
    "    def _leaf_value(self, y):\n",
    "        return np.mean(y)\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a3ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Predictions for [[1, 2], [3, 4], [4, 3], [10, 12], [30, 5], [44, 21]]: [0 1 1 1 1 1]\n",
      "Accuracy score on training data: 95.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Predictions for [[1, 2], [3, 4], [4, 3], [10, 12], [30, 5], [44, 21]]: [6.76 9.97 9.97 9.97 9.97 9.97]\n",
      "R^2 score on training data: 88.92 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply Dt with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Decision Tree Classifier object\n",
    "        dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "        dt_model.fit(X, y_labels)\n",
    "        y_pred = dt_model.predict(X_new)\n",
    "        score_train = dt_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Decision Tree Regressor object\n",
    "        dt_model = DecisionTreeRegressor(max_depth=3)\n",
    "        dt_model.fit(X, y_output)\n",
    "        y_pred = dt_model.predict(X_new)\n",
    "        score_train = dt_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3441ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visual Tree:\n",
      "X[0] <= 0.84\n",
      "  X[0] <= 0.83\n",
      "    Leaf: 0\n",
      "    X[0] <= 0.83\n",
      "      Leaf: 1\n",
      "      Leaf: 0\n",
      "  X[0] <= 1.16\n",
      "    X[1] <= 0.75\n",
      "      Leaf: 1\n",
      "      Leaf: 0\n",
      "    X[0] <= 1.47\n",
      "      Leaf: 1\n",
      "      Leaf: 1\n"
     ]
    }
   ],
   "source": [
    "# Classification tree visual\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "dt_model.fit(X, y_labels)\n",
    "y_pred = dt_model.predict(X_new)\n",
    "print(\"\\nVisual Tree:\")\n",
    "dt_model.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c1b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visual Tree:\n",
      "X[0] <= 1.13\n",
      "  X[0] <= 0.59\n",
      "    X[0] <= 0.22\n",
      "      Leaf: 4.2\n",
      "      Leaf: 5.25\n",
      "    X[0] <= 1.1\n",
      "      Leaf: 6.76\n",
      "      Leaf: 4.05\n",
      "  X[0] <= 1.79\n",
      "    X[0] <= 1.57\n",
      "      Leaf: 7.69\n",
      "      Leaf: 8.78\n",
      "    X[0] <= 1.86\n",
      "      Leaf: 10.82\n",
      "      Leaf: 9.97\n"
     ]
    }
   ],
   "source": [
    "# Regression tree visual\n",
    "dt_model = DecisionTreeRegressor(max_depth=3)\n",
    "dt_model.fit(X, y_output)\n",
    "y_pred = dt_model.predict(X_new)\n",
    "print(\"\\nVisual Tree:\")\n",
    "dt_model.print_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
