{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c75c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known that gradient boost is boosting with decsion trees as base estimator \n",
    "# We have multiple of weak learners to have a strong learner at the end working sequentially and each weak learner is trained on the previous one errors\n",
    "from Libraries.DecisionTree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cb1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need this copy for cloning decision tree estimator\n",
    "import copy\n",
    "\n",
    "# We work with the base class then inheritence happen to build the classification or regression tree\n",
    "class GradientBoostBase:\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, base_estimator, n_estimators=100, learning_rate=0.1, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, tol=1e-6, random_state=None):\n",
    "        # Later we will decide base estimator is classification or regression tree \n",
    "        # Stopping condition will be the number of weak learners or enable early stopping (error has no more improvement after k iterations)\n",
    "        self.base_estimator = base_estimator\n",
    "        # Others is the same default paramerts of API\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.early_stopping = early_stopping\n",
    "        # Percentage of data to be vlaidation set\n",
    "        self.validation_fraction = validation_fraction\n",
    "        # The number of times (patience) till no change in erorr happen then early stop comes\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        # The min tolerance which means the min improvement and it is very close to zero\n",
    "        self.tol = tol\n",
    "        # Storage of leaners\n",
    "        self.estimators_ = []\n",
    "        # For early stopping\n",
    "        # Train loss and validation loss array\n",
    "        self.train_loss_ = []\n",
    "        self.val_loss_ = []\n",
    "        # The best number of learners before we early stop\n",
    "        self.best_iteration_ = None\n",
    "\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def fit(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Helper function\n",
    "    def _clone_estimator(self):\n",
    "        # Create clone of learner to fit the data \n",
    "        return copy.deepcopy(self.base_estimator)\n",
    "\n",
    "    # Train val split function\n",
    "    # This train val split for early stopping \n",
    "    def _train_val_split(self, X, y):\n",
    "        # Here we will depend on indices not data\n",
    "        # get number of samples\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Seed based on random_state value to ensure it is the same when we run the code again\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        # Get the number of samples as np array with evenly spaced values within a specified interval\n",
    "        # It is just indexing (creating indices)\n",
    "        idx = np.arange(m)\n",
    "\n",
    "        # Shuffle array \n",
    "        # We shuffle indices itself rather than X beacause it is more memory-efficient\n",
    "        rng.shuffle(idx)\n",
    "        \n",
    "        # Portion of validation set to be used in Loss functions\n",
    "        n_val = int(np.floor(self.validation_fraction * m))\n",
    "\n",
    "        # if the portions is less than one make it at least one\n",
    "        n_val = max(1, n_val)\n",
    "\n",
    "        # Filter orginal train dataset by the validation indexes\n",
    "        # So retrieve the first 0:n_val as validation and the rest as train n_val:end\n",
    "        val_idx = idx[:n_val]\n",
    "        train_idx = idx[n_val:]\n",
    "\n",
    "        # If after split the train size is 0 then no Early stopping happen\n",
    "        if train_idx.size == 0:\n",
    "            # If too small dataset, fallback: no validation\n",
    "            return X, y, None, None\n",
    "\n",
    "        # Here we return indices of training and valdiation sets    \n",
    "        return X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    \n",
    "# Gradient Boost Classifier class\n",
    "class GradientBoostClassifier(GradientBoostBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters       \n",
    "    def __init__(self, estimator=None, n_estimators=100, learning_rate=0.1, \n",
    "                 early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, tol=1e-6, random_state=None):\n",
    "\n",
    "        # Gradient boost depends on Decision Tree Regressor as we pass numbers (residuals) and we get numbers (average)\n",
    "        # Handle the error of estimator\n",
    "        # If estimator is None or not equal decision tree classifier object or max_depth is not 1\n",
    "        # is_instance is used to check object type \n",
    "        if estimator is None or not isinstance(estimator, DecisionTreeClassifier):\n",
    "            # Here is the correct learner\n",
    "            base_estim_to_use = DecisionTreeClassifier(max_depth=3)\n",
    "        else:\n",
    "            base_estim_to_use = estimator\n",
    "\n",
    "        super().__init__(base_estim_to_use, n_estimators, learning_rate,\n",
    "                         early_stopping, validation_fraction, n_iter_no_change, tol, random_state)\n",
    "\n",
    "    # Helper functions\n",
    "    # Functions that help to convert scoreF to P\n",
    "    # Case of binary classification\n",
    "    # Sigmoid funtion to convert scoreF to P and in predictions to change final scoreF to P then to label\n",
    "    def _sigmoid(self, z):\n",
    "         # We add clip to make sure that z won't reach infinity or negative infinity for stability\n",
    "         z = np.clip(z, -35, 35)\n",
    "         return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    # Case of multi classification\n",
    "    # Softmax funtion to convert scoreF to P\n",
    "    def _softmax(self, F):\n",
    "        # We convert all logits (log odds) (scoreF) to probabilites\n",
    "        # We subtracted from max for stability trick\n",
    "        # So we can shrink the high value number when exponentialing\n",
    "        F = F - np.max(F, axis=1, keepdims=True)\n",
    "\n",
    "        # Then convert it to exp\n",
    "        expF = np.exp(F)\n",
    "\n",
    "        # Add stability of 1e-9 to make sure we won't devide by zero\n",
    "        # This is normalization to make sure sum of all probabilities after conversion from F is = 1\n",
    "        return expF / (np.sum(expF, axis=1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    # Case of early stopping\n",
    "    # We apply logloss for classification\n",
    "    # So logloss for binary and logloss for multi\n",
    "    # Logloss function\n",
    "    def _log_loss_binary(self, y01, p1):\n",
    "        # We add clip to make sure that p1 has limits using stability value\n",
    "        # y01 refers to y = 0  and y = 1 and p1 to refer to probability of y = 1\n",
    "        # When y = 1 we have probability and we cancel another term and versa\n",
    "        # We don't need the probility of y = 0 as it is the 1 - p of y = 1\n",
    "        p1 = np.clip(p1, 1e-9, 1.0 - 1e-9)\n",
    "        \n",
    "        # The logistic formula is -1/n sum(y * log (p) + (1 - y) * log (1 - p)) to make sure we cover the abscene and the presence of the y\n",
    "        # Logloss function which is the mean of log loss for all points that is why we devide by n\n",
    "        return -np.mean(y01 * np.log(p1) + (1 - y01) * np.log(1 - p1))\n",
    "\n",
    "    # Softmax cross entropy loss\n",
    "    def _log_loss_multiclass(self, Y_onehot, P):\n",
    "        # We add clip to make sure that p has limits using stability value\n",
    "        P = np.clip(P, 1e-9, 1.0)\n",
    "\n",
    "        # Cross entorpy log loss function\n",
    "        # e(scoreF) / sum(e(scoreF))\n",
    "        return -np.mean(np.sum(Y_onehot * np.log(P), axis=1))\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        # Use numpy array (asarray0) to point to the data itself for memory effieciency\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Encode classes to numbers (label encoder)\n",
    "        # So for both binary or multi it is fine\n",
    "        # We convert real labels to numbers\n",
    "        self.classes_, y_enc = np.unique(y, return_inverse=True)\n",
    "        # Get the number of classess\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        # Early stopping split if it enabled\n",
    "        if self.early_stopping:\n",
    "            # Once we apply the early stopping then we pass x and classess to be splitted (slicing)\n",
    "            # The function is for indices level and it returns data\n",
    "            X_train, y_train, X_val, y_val = self._train_val_split(X, y_enc)\n",
    "        else:\n",
    "            # No validation set\n",
    "            X_train, y_train, X_val, y_val = X, y_enc, None, None\n",
    "\n",
    "        # For early stopping    \n",
    "        self.estimators_ = []\n",
    "        self.train_loss_ = []\n",
    "        self.val_loss_ = []\n",
    "        self.best_iteration_ = None\n",
    "        \n",
    "        # Fit binary classes and apply early stop\n",
    "        if self.n_classes_ == 2:\n",
    "            # Step 1\n",
    "            # Intialize for both train and validation\n",
    "            # y_train is {0,1}\n",
    "            # The probability that class y = 1 happen\n",
    "            p0 = np.mean(y_train)\n",
    "\n",
    "            # For stability we add limits\n",
    "            p0 = np.clip(p0, 1e-12, 1.0 - 1e-12)\n",
    "\n",
    "            # F0 = log(p0/(1-p0))\n",
    "            # Log odds which will be F0\n",
    "            # The most naive prediction\n",
    "            self.init_ = np.log(p0 / (1.0 - p0))\n",
    "\n",
    "            # Current scores on train (F and P) for current learner\n",
    "            # Every time we have scoreF\n",
    "            # Create array with the same shape of train rows to store init_ and the dtype is float\n",
    "            # I make like column in table equivelant to each sample that is why we have the same shape\n",
    "            F_train = np.full(X_train.shape[0], self.init_, dtype=float)\n",
    "\n",
    "            # Convert F --> P \n",
    "            # I make like column in table equivelant to each sample that is why we have the same shape\n",
    "            p_train = self._sigmoid(F_train)\n",
    "\n",
    "            # For validation tracking\n",
    "            if X_val is not None:\n",
    "                # Intialize validation set\n",
    "                # Create array with the same shape of train rows to store init_ for predictions of validation\n",
    "                # We need of course F and p in our loss\n",
    "                # The purpose is loss scale\n",
    "                F_val = np.full(X_val.shape[0], self.init_, dtype=float)\n",
    "                p_val = self._sigmoid(F_val)\n",
    "            \n",
    "            # For loss purposes we have the best validation ever and no_improve counter which will be later\n",
    "            best_val = np.inf\n",
    "            no_improve = 0\n",
    "            \n",
    "            # Loop over leaners\n",
    "            for m in range(self.n_estimators):\n",
    "                # Step 2\n",
    "                # Calculate residuals (negative gradients): r = y - p\n",
    "                # Here we assign .astype to make sure flaot - float because p will be float [0:1]\n",
    "                residuals = y_train.astype(float) - p_train\n",
    "\n",
    "                # Step 3\n",
    "                # Create clone of learner to fit the data \n",
    "                # It will be always decision tree regressor as we pass residuals as numbers not labales even in classification\n",
    "                estimator = self._clone_estimator()\n",
    "                estimator.fit(X_train, residuals)\n",
    "\n",
    "                # Step 4\n",
    "                # Update scores F \n",
    "                # Fnew = Fold + learning_rate * leaf_value (average) which will be the ouput of the decision tree regressor (predict)\n",
    "                F_train += self.learning_rate * estimator.predict(X_train)\n",
    "                # Convert F to P to start new learner\n",
    "                p_train = self._sigmoid(F_train)\n",
    "\n",
    "                # Store estimator\n",
    "                self.estimators_.append(estimator)\n",
    "\n",
    "                # Step 5 \n",
    "                # evaluation step\n",
    "                # Not like academic there is no evaluation before learner 1\n",
    "                # Track losses\n",
    "                tr_loss = self._log_loss_binary(y_train, p_train)\n",
    "\n",
    "                # Store losses\n",
    "                self.train_loss_.append(tr_loss)\n",
    "\n",
    "                if X_val is not None:\n",
    "                    # Here we need to precit\n",
    "                    # It is concident that train fomrula looks like test \n",
    "                    # Here we don't build we just collect\n",
    "                    # We don't duplicate training work\n",
    "                    # But no fitting just predict\n",
    "                    F_val += self.learning_rate * estimator.predict(X_val)\n",
    "                    p_val = self._sigmoid(F_val)\n",
    "                    va_loss = self._log_loss_binary(y_val, p_val)\n",
    "                    self.val_loss_.append(va_loss)\n",
    "\n",
    "                    # Early stopping check\n",
    "                    if va_loss + self.tol < best_val:\n",
    "                        # First time the best_val will be va_loss because it is equal infinity and for sure the condition gonna happen\n",
    "                        best_val = va_loss\n",
    "                        \n",
    "                        # Best iteration is m + 1 cause m starts at zero\n",
    "                        self.best_iteration_ = m + 1 \n",
    "                        \n",
    "                        # no_improve is the counter of n_iter_no_change (patience)\n",
    "                        # Track the iteration with no change\n",
    "                        # So logic to make it zero becasue it is the best and we record again\n",
    "                        no_improve = 0\n",
    "\n",
    "                    else:\n",
    "                        # We give chances\n",
    "                        no_improve += 1\n",
    "\n",
    "                        # When pateince run out\n",
    "                        if no_improve >= self.n_iter_no_change:\n",
    "                            # Roll back to best iteration\n",
    "                            if self.best_iteration_ is not None:\n",
    "                                # We stop and we will update the whole lists\n",
    "                                # Index at all other lists there\n",
    "                                self.estimators_ = self.estimators_[:self.best_iteration_]\n",
    "                                self.val_loss_ = self.val_loss_[:self.best_iteration_]\n",
    "                                self.train_loss_ = self.train_loss_[:self.best_iteration_]\n",
    "                            break\n",
    "\n",
    "            return self\n",
    "\n",
    "        # For multi classification\n",
    "        # Softmax method\n",
    "        K = self.n_classes_\n",
    "\n",
    "        # One-hot for training labels\n",
    "        # Here we seperate work per class\n",
    "        # We shape new y_train as matrix of zeros (m, k)\n",
    "        # Seperate data of k classes\n",
    "        Y_train = np.zeros((X_train.shape[0], K), dtype=float)\n",
    "\n",
    "        # Fill all cells with dummy var = 1.0\n",
    "        Y_train[np.arange(X_train.shape[0]), y_train] = 1.0\n",
    "\n",
    "        # Step 1 \n",
    "        # Initialize scores with log priors: F0_k = log(pi_k)\n",
    "        # Here we get Pi\n",
    "        priors = np.mean(Y_train, axis=0)\n",
    "        \n",
    "        # For stability we add limits\n",
    "        priors = np.clip(priors, 1e-12, 1.0)\n",
    "\n",
    "        # Here is the scoreF which will be log pi\n",
    "        self.init_ = np.log(priors)\n",
    "\n",
    "        # Current scores on train: (n, K)\n",
    "        # We make matrix for F0 for every iteration it will be number of samples with one column\n",
    "        # I make like column in table equivelant to each sample that is why we have the same shape\n",
    "        F_train = np.tile(self.init_, (X_train.shape[0], 1))\n",
    "\n",
    "        # Step 2\n",
    "        # Calcualte softmax to convert scoreF to P after first learner\n",
    "        P_train = self._softmax(F_train)\n",
    "\n",
    "        # For validation tracking\n",
    "        if X_val is not None:\n",
    "            # Intialize validation set\n",
    "            # We need of course F and p in our loss\n",
    "            # Create array with the same shape of train rows to store init_ for predictions of validation\n",
    "            # The purpose is loss scale\n",
    "            Y_val = np.zeros((X_val.shape[0], K), dtype=float)\n",
    "            # Fill all cells with dummy var  = 1\n",
    "            Y_val[np.arange(X_val.shape[0]), y_val] = 1.0\n",
    "            F_val = np.tile(self.init_, (X_val.shape[0], 1))\n",
    "            P_val = self._softmax(F_val)\n",
    "\n",
    "        # For loss purposes we have the best validation ever and no_improve counter which will be later\n",
    "        best_val = np.inf\n",
    "        no_improve = 0\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Step 3\n",
    "            # Residuals per class: R = Y - P\n",
    "            R = Y_train - P_train\n",
    "\n",
    "            # Step 4\n",
    "            # We create trees = number of classes per iteration\n",
    "            trees_this_iter = []\n",
    "            for k in range(K):\n",
    "                # Create clone of learner to fit the data \n",
    "                # It will be always decision tree regressor as we pass residuals as numbers not labales even in classification\n",
    "                estimator = self._clone_estimator()\n",
    "                estimator.fit(X_train, R[:, k])\n",
    "                trees_this_iter.append(estimator)\n",
    "\n",
    "            # Step 5\n",
    "            # Update scores\n",
    "            for k in range(K):\n",
    "                F_train[:, k] += self.learning_rate * trees_this_iter[k].predict(X_train)\n",
    "\n",
    "            # Step 6 \n",
    "            # Convert F to P to start new learner            \n",
    "            P_train = self._softmax(F_train)\n",
    "\n",
    "            # Store this iteration's trees\n",
    "            self.estimators_.append(trees_this_iter)\n",
    "\n",
    "            # Track losses\n",
    "            tr_loss = self._log_loss_multiclass(Y_train, P_train)\n",
    "            self.train_loss_.append(tr_loss)\n",
    "\n",
    "            if X_val is not None:\n",
    "                # Here we need to precit\n",
    "                # It is concident that train fomrula looks like test \n",
    "                # Here we don't build we just collect\n",
    "                # We don't duplicate training work\n",
    "                # But no fitting just predict\n",
    "                # Make sure we work on k trees per iteration so we predict Fks\n",
    "                for k in range(K):\n",
    "                    F_val[:, k] += self.learning_rate * trees_this_iter[k].predict(X_val)\n",
    "\n",
    "                # Convert F to P    \n",
    "                P_val = self._softmax(F_val)\n",
    "                # Evaluation\n",
    "                va_loss = self._log_loss_multiclass(Y_val, P_val)\n",
    "                self.val_loss_.append(va_loss)\n",
    "\n",
    "                # Early stopping\n",
    "                # The same as binary\n",
    "                if va_loss + self.tol < best_val:\n",
    "                    best_val = va_loss\n",
    "                    self.best_iteration_ = m + 1\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= self.n_iter_no_change:\n",
    "                        if self.best_iteration_ is not None:\n",
    "                            self.estimators_ = self.estimators_[:self.best_iteration_]\n",
    "                            self.val_loss_ = self.val_loss_[:self.best_iteration_]\n",
    "                            self.train_loss_ = self.train_loss_[:self.best_iteration_]\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predict probabilities\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Binary\n",
    "        if self.n_classes_ == 2:\n",
    "            # Get all socres intialization\n",
    "            F = np.full(X.shape[0], self.init_, dtype=float)\n",
    "\n",
    "            # Loop over learners\n",
    "            for estimator in self.estimators_:\n",
    "                # Get scoreF\n",
    "                F += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "            # Get P\n",
    "            p1 = self._sigmoid(F)\n",
    "\n",
    "            # In code binary classification has also one hot encoder so we have two columns \n",
    "            # For y = 0 and y = 1 and we combine them to get 2 columns so we return P0 and P1\n",
    "            return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "        # Multi class\n",
    "        K = self.n_classes_\n",
    "\n",
    "        # Get all socres intialization\n",
    "        F = np.tile(self.init_, (X.shape[0], 1))\n",
    "\n",
    "        # Loop over learners\n",
    "        for trees_this_iter in self.estimators_:\n",
    "            # Loop over trees\n",
    "            for k in range(K):\n",
    "                F[:, k] += self.learning_rate * trees_this_iter[k].predict(X)\n",
    "\n",
    "        # Get P        \n",
    "        return self._softmax(F)\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # We get probabilities first\n",
    "        probas = self.predict_proba(X)\n",
    "\n",
    "        # We get the max probability per sample\n",
    "        pred_idx = np.argmax(probas, axis=1)\n",
    "\n",
    "        # Return the label\n",
    "        return self.classes_[pred_idx]\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "class GradientBoostRegressor(GradientBoostBase):\n",
    "    \n",
    "    # Initialization\n",
    "    def __init__(self, estimator=None, n_estimators=100, learning_rate=0.1,\n",
    "                 early_stopping=False, validation_fraction=0.1, n_iter_no_change=5,\n",
    "                 tol=1e-6, random_state=None):\n",
    "        \n",
    "        # Gradient boost regression depends on Decision Tree Regressor as base learner\n",
    "        # Because we pass residuals as numbers and we get numbers (leaf averages) as predictions\n",
    "        # Handle the error of estimator\n",
    "        # If estimator is None or not equal decision tree regressor object then use default\n",
    "        if estimator is None or not isinstance(estimator, DecisionTreeRegressor):\n",
    "            base_estim_to_use = DecisionTreeRegressor(max_depth=3)\n",
    "        else:\n",
    "            base_estim_to_use = estimator\n",
    "        \n",
    "        super().__init__(base_estim_to_use, n_estimators, learning_rate,\n",
    "                         early_stopping, validation_fraction, n_iter_no_change,\n",
    "                         tol, random_state)\n",
    "        \n",
    "        # Init_ will be the naive prediction which is mean(y) in MSE regression\n",
    "        self.init_ = None\n",
    "\n",
    "    \n",
    "    # Case of early stopping\n",
    "    # We apply MSE for regression as loss function\n",
    "    # MSE function\n",
    "    def _mse(self, y, y_pred):\n",
    "        # Mean Squared Error = mean((y - y_pred)^2)\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        # Use numpy array (asarray) to point to the data itself for memory efficiency\n",
    "        X = np.asarray(X)\n",
    "        # Ensure y is float because regression and residuals are continuous numbers\n",
    "        y = np.asarray(y).astype(float)\n",
    "\n",
    "        # Early stopping split (optional)\n",
    "        # If enabled then we split data into train and validation sets\n",
    "        if self.early_stopping:\n",
    "            X_train, y_train, X_val, y_val = self._train_val_split(X, y)\n",
    "        else:\n",
    "            # No validation set\n",
    "            X_train, y_train, X_val, y_val = X, y, None, None\n",
    "\n",
    "        # Step 1\n",
    "        # Initialization for regression with squared error:\n",
    "        # F0 = mean(y_train) which is the most naive prediction (constant baseline)\n",
    "        self.init_ = np.mean(y_train)\n",
    "\n",
    "        # Current prediction on train\n",
    "        # Create array with the same shape of train rows to store init_ for predictions\n",
    "        # I make like column in table equivalent to each sample that is why we have the same shape\n",
    "        y_pred_train = np.full(X_train.shape[0], self.init_, dtype=float)\n",
    "\n",
    "        # For validation tracking\n",
    "        if X_val is not None:\n",
    "            # Initialize validation predictions with the same init_\n",
    "            # The purpose is loss scale (baseline) and cumulative prediction updates\n",
    "            y_pred_val = np.full(X_val.shape[0], self.init_, dtype=float)\n",
    "\n",
    "        # Reset storages for re-fit\n",
    "        self.estimators_ = []\n",
    "        self.train_loss_ = []\n",
    "        self.val_loss_ = []\n",
    "        self.best_iteration_ = None\n",
    "\n",
    "        # For loss purposes we have the best validation ever and no_improve counter which will be later\n",
    "        best_val = np.inf\n",
    "        no_improve = 0\n",
    "\n",
    "        # Loop over learners\n",
    "        for m in range(self.n_estimators):\n",
    "            # Step 2\n",
    "            # Calculate residuals (negative gradients) for MSE:\n",
    "            # residuals = y - prediction\n",
    "            residuals = y_train - y_pred_train\n",
    "\n",
    "            # Step 3\n",
    "            # Create clone of learner to fit the data\n",
    "            # It will be decision tree regressor as we pass residuals as numbers\n",
    "            estimator = self._clone_estimator()\n",
    "            estimator.fit(X_train, residuals)\n",
    "\n",
    "            # Step 4\n",
    "            # Update predictions:\n",
    "            # y_pred_new = y_pred_old + learning_rate * tree_prediction\n",
    "            # Where tree_prediction is leaf_value (average) for the leaf that each sample falls into\n",
    "            y_pred_train += self.learning_rate * estimator.predict(X_train)\n",
    "\n",
    "            # Store estimator\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "            # Step 5\n",
    "            # Evaluation step\n",
    "            # Not like academic there is no evaluation before learner 1\n",
    "            # Track training loss (MSE)\n",
    "            tr_loss = self._mse(y_train, y_pred_train)\n",
    "            self.train_loss_.append(tr_loss)\n",
    "\n",
    "            if X_val is not None:\n",
    "                # Here we need to predict\n",
    "                # It is coincident that train formula looks like test\n",
    "                # Here we don't build we just collect\n",
    "                # We don't duplicate training work\n",
    "                # But no fitting just predict\n",
    "                y_pred_val += self.learning_rate * estimator.predict(X_val)\n",
    "\n",
    "                # Evaluation on validation (MSE)\n",
    "                va_loss = self._mse(y_val, y_pred_val)\n",
    "                self.val_loss_.append(va_loss)\n",
    "\n",
    "                # Early stopping check\n",
    "                # The same as classifier:\n",
    "                # If validation improves by at least tol then reset patience\n",
    "                if va_loss + self.tol < best_val:\n",
    "                    best_val = va_loss\n",
    "                    # Best iteration is m + 1 cause m starts at zero\n",
    "                    self.best_iteration_ = m + 1\n",
    "                    # Reset counter because this is the best so far\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    # No improvement\n",
    "                    no_improve += 1\n",
    "\n",
    "                    # When patience runs out\n",
    "                    if no_improve >= self.n_iter_no_change:\n",
    "                        # Roll back to best iteration\n",
    "                        if self.best_iteration_ is not None:\n",
    "                            self.estimators_ = self.estimators_[:self.best_iteration_]\n",
    "                            self.val_loss_ = self.val_loss_[:self.best_iteration_]\n",
    "                            self.train_loss_ = self.train_loss_[:self.best_iteration_]\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # Convert X to numpy\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Start from baseline init_\n",
    "        # Create array with the same shape of X rows to store init_ for predictions\n",
    "        y_pred = np.full(X.shape[0], self.init_, dtype=float)\n",
    "\n",
    "        # Loop over learners and accumulate their contributions\n",
    "        for estimator in self.estimators_:\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)  # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa68616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Gradient Boost Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 85.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Gradient Boost Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [7.41 8.81 8.81]\n",
      "R^2 score on training data: 10.78 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        gb_model = GradientBoostClassifier()\n",
    "        gb_model.fit(X, y_labels)\n",
    "        y_pred = gb_model.predict(X_new)\n",
    "        score_train = gb_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Gradient Boost Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        gb_model = GradientBoostRegressor()\n",
    "        gb_model.fit(X, y_output)\n",
    "        y_pred = gb_model.predict(X_new)\n",
    "        score_train = gb_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Gradient Boost Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
