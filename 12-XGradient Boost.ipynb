{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c392c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known that exteme gradient boost is boosting with greedy tress as base estimator \n",
    "# So we need to get decision tress\n",
    "# We have multiple of weak learners to have a strong learner at the end working sequentially and each weak learner is trained on the previous one errors\n",
    "from Libraries.DecisionTree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b96512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Here is modified version of Decsion Tree (Greedy Tree) and Gradient Boost (Extreme Gradient Boost)\n",
    "\n",
    "\n",
    "# XGBoost Tree Regressor class (Greedy Tree)\n",
    "# All we need just regressor as we work with optimal leaf weights not samples\n",
    "# This is the greedy regression tree used inside Extreme Gradient Boosting\n",
    "class XGBTreeRegressor(BaseDecisionTree):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1, max_features=None, reg_lambda=1.0, reg_alpha=0.0, gamma=0.0, min_child_weight=1.0):\n",
    "        # Here we intialize tree properties there is mandatory and optional propeties based on use case\n",
    "        # Note that criterion here is not gini/entropy/mse, it is XGBoost gain based on extreme gradients and hessians\n",
    "        # Here we don't work with min_impurity_decreace nor ccp_alpha so we set them to zero\n",
    "        super().__init__(criterion=\"xgb\",\n",
    "                         max_depth=max_depth,\n",
    "                         min_samples_split=min_samples_split,\n",
    "                         min_samples_leaf=min_samples_leaf,\n",
    "                         max_features=max_features,\n",
    "                         min_impurity_decrease=0.0,\n",
    "                         ccp_alpha=0.0)\n",
    "\n",
    "        # Regularization on leaf weights (L2)\n",
    "        self.reg_lambda = reg_lambda\n",
    "        # Regularization on leaf weights (L1)\n",
    "        # This is used less than L2\n",
    "        self.reg_alpha = reg_alpha\n",
    "        # Penalization term on each split \n",
    "        # This is regularization but for tree structure\n",
    "        self.gamma = gamma\n",
    "        # Minimum hessian sum per leaf (to avoid unreliable splits)\n",
    "        self.min_child_weight = min_child_weight\n",
    "\n",
    "        # Root node\n",
    "        self.root = None\n",
    "\n",
    "    # Helper functions\n",
    "    # L1 regularization for reg_alpha\n",
    "    def _soft_threshold(self, G):\n",
    "        # We adjust the gain based on these conditions\n",
    "        # We adjust as the existence of alpha make us shrink the gain wether it is for root, left or right\n",
    "        # If reg_alpha is zero then no shrink happens\n",
    "        a = self.reg_alpha\n",
    "        if a <= 0:\n",
    "            return G\n",
    "\n",
    "        # Shrink happen\n",
    "        if G > a:\n",
    "            return G - a\n",
    "        if G < -a:\n",
    "            return G + a\n",
    "        return 0.0\n",
    "\n",
    "    # Similarity score function\n",
    "    # This is used to calculate the gain of split\n",
    "    def _similarity(self, G, H):\n",
    "        # This score is term of calcualting the gain\n",
    "        # We add reg_lambda to avoid overfitting \n",
    "        # Add stability to avoid zero devision\n",
    "        G_adj = self._soft_threshold(G)\n",
    "        return (G_adj ** 2) / (H + self.reg_lambda + 1e-9)\n",
    "\n",
    "    # Gain function\n",
    "    # This is the criterion to choose the best feature and the best threshold\n",
    "    def _gain(self, G, H, GL, HL, GR, HR):\n",
    "        # gamma penalizes each split to avoid overfitting in tree structure\n",
    "        return 0.5 * (self._similarity(GL, HL) + self._similarity(GR, HR) - self._similarity(G, H)) - self.gamma\n",
    "\n",
    "    # Here the value leaf is the optimal weight of this leaf\n",
    "    # This is not average like gradient boost when calling decision tree\n",
    "    def _leaf_value(self, g, h):\n",
    "        # Summation of extreme gradients and hessians inside the leaf\n",
    "        G = np.sum(g)\n",
    "        H = np.sum(h)\n",
    "\n",
    "        # Apply L1 shrink if it is enabled\n",
    "        G_adj = self._soft_threshold(G)\n",
    "\n",
    "        # Optimal leaf weight as w = -G/(H + lambda)\n",
    "        # Add stability to avoid zero devision\n",
    "        w = - G_adj / (H + self.reg_lambda + 1e-9)\n",
    "        return w, None\n",
    "\n",
    "    # Best split function \n",
    "    # Here it is the same skelton as decsion tree but with different methodology\n",
    "    def _best_split(self, X, g, h, features=None):\n",
    "        # Here we need to have the best split by having samples and features\n",
    "        # This function to get the best feature with all related properties and later in _build_tree we will assign samples actually\n",
    "        # My target is to reach the best gain based on extreme gradients and \n",
    "        # here the same concept of applying internal and global splits\n",
    "\n",
    "        # Intialize the best gain first\n",
    "        # - infinity is the safest option always\n",
    "        best_gain = -np.inf\n",
    "\n",
    "        # My target is to retrieve best feature with best threshold\n",
    "        split_feature, split_threshold = None, None\n",
    "\n",
    "        # Number of features\n",
    "        # Here if we apply portion of features of not\n",
    "        n_features = X.shape[1]\n",
    "        if features is None:\n",
    "            features = np.arange(n_features)\n",
    "\n",
    "        # Parent stats\n",
    "        G = np.sum(g)\n",
    "        H = np.sum(h)\n",
    "\n",
    "        # Gains score list\n",
    "        gains_score = []\n",
    "\n",
    "        # Loop over features\n",
    "        for feature in features:\n",
    "            # First we decalre the thresholds for internal splits\n",
    "            # Thresholds come from features itself so coming from x\n",
    "            # As we know acadeemically that continuos values (num=numerical features) we take average but here we take the midpoint between each pairs\n",
    "            # And for discrete values (Categorical features) we take the unique labels\n",
    "            # Preprocessing for data happen and eventually we got numbers wether it is categorical features or numerical one\n",
    "            # So in both cases sickit learn API take the midpoint as midpoint covers all cases and get less number of thresholds\n",
    "            # We need the all samples of X within this feature\n",
    "            # We got sorted non repeatable array (Core)\n",
    "            values = np.unique(X[:, feature])\n",
    "\n",
    "            # If the feature has only one unique value then no split possible\n",
    "            if values.shape[0] < 2:\n",
    "                continue\n",
    "\n",
    "            # Midpoints fetched and stored in array\n",
    "            # Midpoint = (sum of each pair) / 2 \n",
    "            # Start at 0,1 index then 1,2 and so on\n",
    "            thresholds = (values[:-1] + values[1:]) / 2\n",
    "\n",
    "            # Loop over thresholds to see the highest one\n",
    "            for threshold in thresholds:\n",
    "                # Samples role\n",
    "                # We have left and right children\n",
    "                # Left samples gonna be less or equal threshold\n",
    "                # Right samples gonna be other wise\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                # If one child has no samples then immediately stop the current threshold\n",
    "                # Sum will be applied to samples means when no sapmples exsist\n",
    "                # sum() == 0\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Child stats\n",
    "                GL = np.sum(g[left_mask])\n",
    "                HL = np.sum(h[left_mask])\n",
    "                # Of course the rest will be in the right\n",
    "                GR = G - GL\n",
    "                HR = H - HL\n",
    "\n",
    "                # Stop case\n",
    "                # Stopping criteria (pre prunning)\n",
    "                # min_child_weight is minimum hessian sum in each leaf\n",
    "                if HL < self.min_child_weight or HR < self.min_child_weight:\n",
    "                    continue\n",
    "\n",
    "                # Gain score\n",
    "                gain = self._gain(G, H, GL, HL, GR, HR)\n",
    "\n",
    "                # Here instead\n",
    "                if gain > best_gain:\n",
    "                    # By deafult first time we pass the condition as min value of gain is 0 which is bigger than -1\n",
    "                    best_gain = gain\n",
    "                    split_feature = feature\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        # Add the best gain of the current feature to the score list then later we choose the highest score from it\n",
    "            gains_score.append([best_gain, split_feature, split_threshold])\n",
    "            \n",
    "        # Get the highest score\n",
    "        max_gain = max(gains_score, key = lambda x:x[0])\n",
    "\n",
    "        # Return the all properties of the best features\n",
    "        # Return best_gain we will use it as stopping criteria for min impurity decrease\n",
    "        # Return split_feature to work with and build tree\n",
    "        # Return split_threshold to assign features in tree\n",
    "        return max_gain[0], max_gain[1], max_gain[2]\n",
    "\n",
    "    # For building our tree\n",
    "    # This is the most important function with the help of other helper functions\n",
    "    def _build_tree(self, X, g, h, depth):\n",
    "        # Ofcourse we need to retrieve smaples and fatures count\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Stop case\n",
    "        # Stopping criteria (pre prunning)\n",
    "        # Preprunning within node\n",
    "        # The cases of inaccurate number of samples in the node or exceed the max depth is hyper paramter stopping cirteria\n",
    "        if n_samples < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            leaf_value, leaf_dist = self._leaf_value(g, h)\n",
    "            return Node(value=leaf_value, value_dist=leaf_dist)\n",
    "\n",
    "        # This part is related to option of choosing portion of features as here we don't want to work with all features\n",
    "        # Note that it is just hyper parametr (optional)\n",
    "        features_idx = np.arange(n_features)\n",
    "        if self.max_features is not None:\n",
    "            # Make it random number from predefined range \n",
    "            # Replace  = False so we won't duplicate feature\n",
    "            features_idx = np.random.choice(\n",
    "                n_features, self.max_features, replace=False\n",
    "                )\n",
    "\n",
    "        # Step 1 ---> find the best feature with all properties\n",
    "        best_gain, split_feature, split_threshold = self._best_split(X, g, h, features_idx)\n",
    "\n",
    "        # Stop case\n",
    "        # Stopping criteria (pre prunning)\n",
    "        # Preprunning within leaves\n",
    "        # If no split feature found OR gain is not positive then leaf node\n",
    "        # Here the prepruning of gamma happen as we envolve gamma in our calcualtion then if it is bigger than or equal the gain \n",
    "        # So logically best gain gonna beneth zero\n",
    "        if split_feature is None or best_gain <= 0:\n",
    "            leaf_value, leaf_dist = self._leaf_value(g, h)\n",
    "            return Node(value=leaf_value, value_dist=leaf_dist)\n",
    "\n",
    "        # Step 2 ---> assign samples in tree\n",
    "        # Now we work with the best split (best feature + best threshold) after test and assign samples\n",
    "        left_mask = X[:, split_feature] <= split_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Stop case\n",
    "        # Stopping criteria (pre prunning)\n",
    "        # The cases of inaccurate number of samples in the leaf (after split)\n",
    "        if left_mask.sum() < self.min_samples_leaf or right_mask.sum() < self.min_samples_leaf:\n",
    "            leaf_value, leaf_dist = self._leaf_value(g, h)\n",
    "            return Node(value=leaf_value, value_dist=leaf_dist)\n",
    "\n",
    "        # Recusive case\n",
    "        left = self._build_tree(X[left_mask], g[left_mask], h[left_mask], depth + 1)\n",
    "        right = self._build_tree(X[right_mask], g[right_mask], h[right_mask], depth + 1)\n",
    "\n",
    "        # We return Node bacause we need all information about tree so we can move through it in prediction knowing conditions\n",
    "        return Node(split_feature, split_threshold, left, right)\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, g, h):\n",
    "        # Use numpy array (asarray) to point to the data itself for memory efficiency\n",
    "        X = np.asarray(X)\n",
    "        g = np.asarray(g).astype(float)\n",
    "        h = np.asarray(h).astype(float)\n",
    "\n",
    "        # The fit is building our tree and save it in root as symbol of our journey starting with pre prunning hyper paramerts\n",
    "        self.root = self._build_tree(X, g, h, 0)\n",
    "\n",
    "    # Predict\n",
    "    # It returns leaf weights (numbers)\n",
    "    def predict(self, X):\n",
    "        return super().predict(X)\n",
    "\n",
    "# We work with the base class then inheritence happen to build the classifier or regressor\n",
    "class XGBoostBase:\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, base_estimator, n_estimators=100, learning_rate=0.1,\n",
    "                 early_stopping=False, validation_fraction=0.1,\n",
    "                 n_iter_no_change=5, tol=1e-6, random_state=None):\n",
    "        # Later we will decide base estimator is greedy regression tree\n",
    "        # Stopping condition will be the number of weak learners or enable early stopping (error has no more improvement after k iterations)\n",
    "        self.base_estimator = base_estimator\n",
    "        # Others is the same default paramerts of API\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.early_stopping = early_stopping\n",
    "        # Percentage of data to be vlaidation set\n",
    "        self.validation_fraction = validation_fraction\n",
    "        # The number of times (patience) till no change in erorr happen then early stop comes\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        # The min tolerance which means the min improvement and it is very close to zero\n",
    "        self.tol = tol\n",
    "\n",
    "        # Storage of leaners\n",
    "        self.estimators_ = []\n",
    "        # For early stopping\n",
    "        # Train loss and validation loss array\n",
    "        self.train_loss_ = []\n",
    "        self.val_loss_ = []\n",
    "        # The best number of learners before we early stop\n",
    "        self.best_iteration_ = None\n",
    "\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Helper function\n",
    "    def _clone_estimator(self):\n",
    "        # Create clone of learner to fit the data\n",
    "        return copy.deepcopy(self.base_estimator)\n",
    "\n",
    "    # Train val split function\n",
    "    # This train val split for early stopping\n",
    "    def _train_val_split(self, X, y):\n",
    "        # Here we will depend on indices not data\n",
    "        # get number of samples\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Seed based on random_state value to ensure it is the same when we run the code again\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        # Get the number of samples as np array with evenly spaced values within a specified interval\n",
    "        # It is just indexing (creating indices)\n",
    "        idx = np.arange(m)\n",
    "\n",
    "        # Shuffle array\n",
    "        # We shuffle indices itself rather than X beacause it is more memory-efficient\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        # Portion of validation set to be used in Loss functions\n",
    "        n_val = int(np.floor(self.validation_fraction * m))\n",
    "\n",
    "        # if the portions is less than one make it at least one\n",
    "        n_val = max(1, n_val)\n",
    "\n",
    "        # Filter orginal train dataset by the validation indexes\n",
    "        # So retrieve the first 0:n_val as validation and the rest as train n_val:end\n",
    "        val_idx = idx[:n_val]\n",
    "        train_idx = idx[n_val:]\n",
    "\n",
    "        # If after split the train size is 0 then no Early stopping happen\n",
    "        if train_idx.size == 0:\n",
    "            # If too small dataset, fallback: no validation\n",
    "            return X, y, None, None\n",
    "\n",
    "        # Here we return indices of training and valdiation sets\n",
    "        return X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "\n",
    "\n",
    "# XGBoost Classifier class\n",
    "class XGBoostClassifier(XGBoostBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self,estimator=None, n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "                 min_samples_split=2, min_samples_leaf=1, max_features=None, \n",
    "                 reg_lambda=1.0, reg_alpha=0.0, gamma=0.0, min_child_weight=1.0,\n",
    "                 early_stopping=False, validation_fraction=0.1, n_iter_no_change=5,\n",
    "                 tol=1e-6, random_state=None):\n",
    "\n",
    "        # Extreme gradient boost classification depends on XGBTreeRegressor as base learner\n",
    "        # Because we pass extreme gradients and hessians as numbers and we get numbers (leaf weights) as predictions\n",
    "        # Handle the error of estimator\n",
    "        # If estimator is None or not equal XGBTreeRegressor object then use default\n",
    "        if estimator is None or not isinstance(estimator, XGBTreeRegressor):\n",
    "            base_estim_to_use = XGBTreeRegressor(max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            reg_lambda=reg_lambda,\n",
    "            reg_alpha=reg_alpha,\n",
    "            gamma=gamma,\n",
    "            min_child_weight=min_child_weight)\n",
    "\n",
    "        else:\n",
    "            base_estim_to_use = estimator\n",
    "\n",
    "        super().__init__(base_estim_to_use, n_estimators, learning_rate,\n",
    "                         early_stopping, validation_fraction, n_iter_no_change,\n",
    "                         tol, random_state)\n",
    "\n",
    "        # Init_ will be the naive prediction in classification\n",
    "        self.init_ = None\n",
    "        # Store original labels\n",
    "        self.classes_ = None\n",
    "        # Store number of classes\n",
    "        self.n_classes_ = None\n",
    "\n",
    "    # Helper functions\n",
    "    # Functions that help to convert scoreZ to P\n",
    "    # Case of binary classification\n",
    "    # Sigmoid funtion to convert scoreZ to P and in predictions to change final scoreZ to P then to label\n",
    "    def _sigmoid(self, z):\n",
    "        # We add clip to make sure that z won't reach infinity or negative infinity for stability\n",
    "        z = np.clip(z, -35, 35)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    # Case of multi classification\n",
    "    # Softmax funtion to convert scoreZ to P\n",
    "    def _softmax(self, Z):\n",
    "        # We convert all logits (log odds) (scoreZ) to probabilites\n",
    "        # We subtracted from max for stability trick\n",
    "        Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "\n",
    "        # Then convert it to exp\n",
    "        expZ = np.exp(Z)\n",
    "\n",
    "        # This is normalization to make sure sum of all probabilities after conversion from Z is = 1\n",
    "        return expZ / (np.sum(expZ, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "    # Case of early stopping\n",
    "    # We apply logloss for classification\n",
    "    # So logloss for binary and logloss for multi\n",
    "    # Logloss function\n",
    "    def _log_loss_binary(self, y01, p1):\n",
    "        # We add clip to make sure that p1 has limits using stability value\n",
    "        p1 = np.clip(p1, 1e-9, 1.0 - 1e-9)\n",
    "        return -np.mean(y01 * np.log(p1) + (1 - y01) * np.log(1 - p1))\n",
    "\n",
    "    # Softmax cross entropy loss\n",
    "    def _log_loss_multiclass(self, Y_onehot, P):\n",
    "        # We add clip to make sure that p has limits using stability value\n",
    "        P = np.clip(P, 1e-9, 1.0)\n",
    "        return -np.mean(np.sum(Y_onehot * np.log(P), axis=1))\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        # Use numpy array (asarray) to point to the data itself for memory effieciency\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Encode classes to numbers (label encoder)\n",
    "        self.classes_, y_enc = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        # Early stopping split if it enabled\n",
    "        if self.early_stopping:\n",
    "            X_train, y_train, X_val, y_val = self._train_val_split(X, y_enc)\n",
    "        else:\n",
    "            # No validation set\n",
    "            X_train, y_train, X_val, y_val = X, y_enc, None, None\n",
    "\n",
    "        # For early stopping\n",
    "        self.estimators_ = []\n",
    "        self.train_loss_ = []\n",
    "        self.val_loss_ = []\n",
    "        self.best_iteration_ = None\n",
    "\n",
    "        # Fit binary classes and apply early stop\n",
    "        if self.n_classes_ == 2:\n",
    "            # Step 1\n",
    "            # Intialize for both train and validation\n",
    "            # y_train is {0,1}\n",
    "            # The probability that class y = 1 happen\n",
    "            p0 = np.mean(y_train)\n",
    "            p0 = np.clip(p0, 1e-12, 1.0 - 1e-12)\n",
    "\n",
    "            # Z0 = log(p0/(1-p0))\n",
    "            # Log odds which will be Z0\n",
    "            # The most naive prediction\n",
    "            self.init_ = np.log(p0 / (1.0 - p0))\n",
    "\n",
    "            # Current scores on train (Z and P) for current learner\n",
    "            Z_train = np.full(X_train.shape[0], self.init_, dtype=float)\n",
    "            p_train = self._sigmoid(Z_train)\n",
    "\n",
    "            # For validation tracking\n",
    "            if X_val is not None:\n",
    "                # Intialize validation set\n",
    "                # Create array with the same shape of train rows to store init_ for predictions of validation\n",
    "                # We need of course F and p in our loss\n",
    "                # The purpose is loss scale\n",
    "                Z_val = np.full(X_val.shape[0], self.init_, dtype=float)\n",
    "                p_val = self._sigmoid(Z_val)\n",
    "\n",
    "            best_val = np.inf\n",
    "            no_improve = 0\n",
    "\n",
    "            # Loop over learners\n",
    "            for m in range(self.n_estimators):\n",
    "                # Step 2\n",
    "                # Calculate extreme gradients and hessians for logistic loss:\n",
    "                # g = p - y\n",
    "                # h = p(1 - p)\n",
    "                g = p_train - y_train.astype(float)\n",
    "                h = p_train * (1.0 - p_train)\n",
    "\n",
    "                # Step 3\n",
    "                # Create clone of learner to fit the data\n",
    "                # It will be XGBTreeRegressor as we pass extreme gradients and hessians as numbers\n",
    "                estimator = self._clone_estimator()\n",
    "                estimator.fit(X_train, g, h)\n",
    "\n",
    "                # Step 4\n",
    "                # Update scores Z\n",
    "                # Znew = Zold + learning_rate * leaf_weight\n",
    "                Z_train += self.learning_rate * estimator.predict(X_train)\n",
    "                p_train = self._sigmoid(Z_train)\n",
    "\n",
    "                # Store estimator\n",
    "                self.estimators_.append(estimator)\n",
    "\n",
    "                # Step 5\n",
    "                # Evaluation step\n",
    "                # Track losses\n",
    "                tr_loss = self._log_loss_binary(y_train, p_train)\n",
    "                self.train_loss_.append(tr_loss)\n",
    "\n",
    "                if X_val is not None:\n",
    "                    # Here we need to precit\n",
    "                    # It is concident that train fomrula looks like test \n",
    "                    # Here we don't build we just collect\n",
    "                    # We don't duplicate training work\n",
    "                    # But no fitting just predict\n",
    "                    Z_val += self.learning_rate * estimator.predict(X_val)\n",
    "                    p_val = self._sigmoid(Z_val)\n",
    "                    va_loss = self._log_loss_binary(y_val, p_val)\n",
    "                    self.val_loss_.append(va_loss)\n",
    "\n",
    "                    # Early stopping check\n",
    "                    if va_loss + self.tol < best_val:\n",
    "                        # First time the best_val will be va_loss because it is equal infinity and for sure the condition gonna happen\n",
    "                        best_val = va_loss\n",
    "                        \n",
    "                        # Best iteration is m + 1 cause m starts at zero\n",
    "                        self.best_iteration_ = m + 1 \n",
    "                        \n",
    "                        # no_improve is the counter of n_iter_no_change (patience)\n",
    "                        # Track the iteration with no change\n",
    "                        # So logic to make it zero becasue it is the best and we record again\n",
    "                        no_improve = 0\n",
    "\n",
    "                    else:\n",
    "                        # We give chances\n",
    "                        no_improve += 1\n",
    "\n",
    "                        # When pateince run out\n",
    "                        if no_improve >= self.n_iter_no_change:\n",
    "                            # Roll back to best iteration\n",
    "                            if self.best_iteration_ is not None:\n",
    "                                # We stop and we will update the whole lists\n",
    "                                # Index at all other lists there\n",
    "                                self.estimators_ = self.estimators_[:self.best_iteration_]\n",
    "                                self.val_loss_ = self.val_loss_[:self.best_iteration_]\n",
    "                                self.train_loss_ = self.train_loss_[:self.best_iteration_]\n",
    "                            break\n",
    "\n",
    "            return self\n",
    "\n",
    "        # For multi classification\n",
    "        # Softmax method\n",
    "        K = self.n_classes_\n",
    "\n",
    "        # One-hot for training labels\n",
    "        Y_train = np.zeros((X_train.shape[0], K), dtype=float)\n",
    "        Y_train[np.arange(X_train.shape[0]), y_train] = 1.0\n",
    "\n",
    "        # Step 1\n",
    "        # Initialize scores with log priors: Z0_k = log(pi_k)\n",
    "        priors = np.mean(Y_train, axis=0)\n",
    "        priors = np.clip(priors, 1e-12, 1.0)\n",
    "        self.init_ = np.log(priors)\n",
    "\n",
    "        # Current scores on train: (n, K)\n",
    "        Z_train = np.tile(self.init_, (X_train.shape[0], 1))\n",
    "        P_train = self._softmax(Z_train)\n",
    "\n",
    "        # For validation tracking\n",
    "        # Intialize validation set\n",
    "        # We need of course F and p in our loss\n",
    "        # Create array with the same shape of train rows to store init_ for predictions of validation\n",
    "        # The purpose is loss scale \n",
    "        if X_val is not None:\n",
    "            # Intialize validation set\n",
    "            # We need of course F and p in our loss\n",
    "            # Create array with the same shape of train rows to store init_ for predictions of validation\n",
    "            # The purpose is loss scale\n",
    "            Y_val = np.zeros((X_val.shape[0], K), dtype=float)\n",
    "            Y_val[np.arange(X_val.shape[0]), y_val] = 1.0\n",
    "            # Fill all cells with dummy var  = 1\n",
    "            Z_val = np.tile(self.init_, (X_val.shape[0], 1))\n",
    "            P_val = self._softmax(Z_val)\n",
    "\n",
    "        best_val = np.inf\n",
    "        no_improve = 0\n",
    "\n",
    "        # Loop over learners\n",
    "        for m in range(self.n_estimators):\n",
    "            # Step 2\n",
    "            # Calculate extreme gradients and hessians per class:\n",
    "            # g_k = p_k - y_k\n",
    "            # h_k = p_k(1 - p_k)\n",
    "            G_mat = P_train - Y_train\n",
    "            H_mat = P_train * (1.0 - P_train)\n",
    "\n",
    "            # Step 3\n",
    "            # We create trees = number of classes per iteration\n",
    "            trees_this_iter = []\n",
    "            for k in range(K):\n",
    "                estimator = self._clone_estimator()\n",
    "                estimator.fit(X_train, G_mat[:, k], H_mat[:, k])\n",
    "                trees_this_iter.append(estimator)\n",
    "\n",
    "            # Step 4\n",
    "            # Update scores Z\n",
    "            for k in range(K):\n",
    "                Z_train[:, k] += self.learning_rate * trees_this_iter[k].predict(X_train)\n",
    "\n",
    "            # Step 5\n",
    "            # Convert Z to P to start new learner\n",
    "            P_train = self._softmax(Z_train)\n",
    "\n",
    "            # Store this iteration's trees\n",
    "            self.estimators_.append(trees_this_iter)\n",
    "\n",
    "            # Step 6\n",
    "            # Evaluation step\n",
    "            tr_loss = self._log_loss_multiclass(Y_train, P_train)\n",
    "            self.train_loss_.append(tr_loss)\n",
    "\n",
    "            if X_val is not None:\n",
    "                # Here we need to precit\n",
    "                # It is concident that train fomrula looks like test \n",
    "                # Here we don't build we just collect\n",
    "                # We don't duplicate training work\n",
    "                # But no fitting just predict\n",
    "                # Make sure we work on k trees per iteration so we predict Zks\n",
    "                for k in range(K):\n",
    "                    Z_val[:, k] += self.learning_rate * trees_this_iter[k].predict(X_val)\n",
    "\n",
    "                # Convert Z to P       \n",
    "                P_val = self._softmax(Z_val)\n",
    "                # Evaluation\n",
    "                va_loss = self._log_loss_multiclass(Y_val, P_val)\n",
    "                self.val_loss_.append(va_loss)\n",
    "             \n",
    "                # Early stopping\n",
    "                # The same as binary\n",
    "                if va_loss + self.tol < best_val:\n",
    "                    best_val = va_loss\n",
    "                    self.best_iteration_ = m + 1\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= self.n_iter_no_change:\n",
    "                        if self.best_iteration_ is not None:\n",
    "                            self.estimators_ = self.estimators_[:self.best_iteration_]\n",
    "                            self.val_loss_ = self.val_loss_[:self.best_iteration_]\n",
    "                            self.train_loss_ = self.train_loss_[:self.best_iteration_]\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predict probabilities\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Binary\n",
    "        if self.n_classes_ == 2:\n",
    "            # Get all scores intialization\n",
    "            Z = np.full(X.shape[0], self.init_, dtype=float)\n",
    "\n",
    "            # Loop over learners\n",
    "            for estimator in self.estimators_:\n",
    "                Z += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "            # Get P\n",
    "            p1 = self._sigmoid(Z)\n",
    "\n",
    "            # In code binary classification has also one hot encoder so we have two columns \n",
    "            # For y = 0 and y = 1 and we combine them to get 2 columns so we return P0 and P1\n",
    "            return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "        # Multi class\n",
    "        K = self.n_classes_\n",
    "        Z = np.tile(self.init_, (X.shape[0], 1))\n",
    "\n",
    "        # Loop over learners\n",
    "        for trees_this_iter in self.estimators_:\n",
    "            # Loop over trees\n",
    "            for k in range(K):\n",
    "                Z[:, k] += self.learning_rate * trees_this_iter[k].predict(X)\n",
    "\n",
    "        # Get P\n",
    "        return self._softmax(Z)\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # We get probabilities first\n",
    "        probas = self.predict_proba(X)\n",
    "\n",
    "        # We get the max probability per sample\n",
    "        pred_idx = np.argmax(probas, axis=1)\n",
    "\n",
    "        # Return the label\n",
    "        return self.classes_[pred_idx]\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        return np.mean(self.predict(X) == y)\n",
    "    \n",
    "# XGBoost Regressor class\n",
    "class XGBoostRegressor(XGBoostBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters\n",
    "    def __init__(self,estimator=None, n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "                 min_samples_split=2, min_samples_leaf=1, max_features=None, \n",
    "                 reg_lambda=1.0, reg_alpha=0.0, gamma=0.0, min_child_weight=1.0,\n",
    "                 early_stopping=False, validation_fraction=0.1, n_iter_no_change=5,\n",
    "                 tol=1e-6, random_state=None):\n",
    "\n",
    "        # Extreme gradient boost classification depends on XGBTreeRegressor as base learner\n",
    "        # Because we pass extreme gradients and hessians as numbers and we get numbers (leaf weights) as predictions\n",
    "        # Handle the error of estimator\n",
    "        # If estimator is None or not equal XGBTreeRegressor object then use default\n",
    "        if estimator is None or not isinstance(estimator, XGBTreeRegressor):\n",
    "            base_estim_to_use = XGBTreeRegressor(max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            reg_lambda=reg_lambda,\n",
    "            reg_alpha=reg_alpha,\n",
    "            gamma=gamma,\n",
    "            min_child_weight=min_child_weight)\n",
    "\n",
    "        super().__init__(base_estim_to_use, n_estimators, learning_rate,\n",
    "                         early_stopping, validation_fraction, n_iter_no_change,\n",
    "                         tol, random_state)\n",
    "\n",
    "        # Init_ will be the naive prediction which is mean(y) in squared error regression\n",
    "        self.init_ = None\n",
    "\n",
    "    # Case of early stopping\n",
    "    # We apply MSE for regression as loss function\n",
    "    # MSE function\n",
    "    def _mse(self, y, y_pred):\n",
    "        # Mean Squared Error = mean((y - y_pred)^2)\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        # Use numpy array (asarray) to point to the data itself for memory efficiency\n",
    "        X = np.asarray(X)\n",
    "        # Ensure y is float because regression and predictions are continuous numbers\n",
    "        y = np.asarray(y).astype(float)\n",
    "\n",
    "        # Early stopping split (optional)\n",
    "        # If enabled then we split data into train and validation sets\n",
    "        if self.early_stopping:\n",
    "            X_train, y_train, X_val, y_val = self._train_val_split(X, y)\n",
    "        else:\n",
    "            # No validation set\n",
    "            X_train, y_train, X_val, y_val = X, y, None, None\n",
    "\n",
    "        # Step 1\n",
    "        # Initialization for regression with squared error:\n",
    "        # F0 = mean(y_train) which is the most naive prediction (constant baseline)\n",
    "        self.init_ = np.mean(y_train)\n",
    "\n",
    "        # Current prediction on train\n",
    "        # Create array with the same shape of train rows to store init_ for predictions\n",
    "        # I make like column in table equivalent to each sample that is why we have the same shape\n",
    "        y_pred_train = np.full(X_train.shape[0], self.init_, dtype=float)\n",
    "\n",
    "        # For validation tracking\n",
    "        if X_val is not None:\n",
    "            # Initialize validation predictions with the same init_\n",
    "            # The purpose is loss scale (baseline) and cumulative prediction updates\n",
    "            y_pred_val = np.full(X_val.shape[0], self.init_, dtype=float)\n",
    "\n",
    "        # Reset storages for re-fit\n",
    "        self.estimators_ = []\n",
    "        self.train_loss_ = []\n",
    "        self.val_loss_ = []\n",
    "        self.best_iteration_ = None\n",
    "\n",
    "        # For loss purposes we have the best validation ever and no_improve counter which will be later\n",
    "        best_val = np.inf\n",
    "        no_improve = 0\n",
    "\n",
    "        # Loop over learners\n",
    "        for m in range(self.n_estimators):\n",
    "            # Step 2\n",
    "            # Calculate extreme gradients and hessians for squared error:\n",
    "            # g = y_pred - y\n",
    "            # h = 1\n",
    "            g = y_pred_train - y_train\n",
    "            h = np.ones_like(g, dtype=float)\n",
    "\n",
    "            # Step 3\n",
    "            # Create clone of learner to fit the data\n",
    "            # It will be XGBTreeRegressor as we pass extreme gradients and hessians as numbers\n",
    "            estimator = self._clone_estimator()\n",
    "            estimator.fit(X_train, g, h)\n",
    "\n",
    "            # Step 4\n",
    "            # Update predictions:\n",
    "            # y_pred_new = y_pred_old + learning_rate * tree_prediction\n",
    "            # Where tree_prediction is leaf weight for the leaf that each sample falls into\n",
    "            y_pred_train += self.learning_rate * estimator.predict(X_train)\n",
    "\n",
    "            # Store estimator\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "            # Step 5\n",
    "            # Evaluation step\n",
    "            # Not like academic there is no evaluation before learner 1\n",
    "            # Track training loss (MSE)\n",
    "            tr_loss = self._mse(y_train, y_pred_train)\n",
    "            self.train_loss_.append(tr_loss)\n",
    "\n",
    "            if X_val is not None:\n",
    "                # Here we need to predict\n",
    "                # It is coincident that train formula looks like test\n",
    "                # Here we don't build we just collect\n",
    "                # We don't duplicate training work\n",
    "                # But no fitting just predict\n",
    "                y_pred_val += self.learning_rate * estimator.predict(X_val)\n",
    "\n",
    "                # Evaluation on validation (MSE)\n",
    "                va_loss = self._mse(y_val, y_pred_val)\n",
    "                self.val_loss_.append(va_loss)\n",
    "\n",
    "                # Early stopping check\n",
    "                # The same as classifier:\n",
    "                # If validation improves by at least tol then reset patience\n",
    "                if va_loss + self.tol < best_val:\n",
    "                    best_val = va_loss\n",
    "                    # Best iteration is m + 1 cause m starts at zero\n",
    "                    self.best_iteration_ = m + 1\n",
    "                    # Reset counter because this is the best so far\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    # No improvement\n",
    "                    no_improve += 1\n",
    "\n",
    "                    # When patience runs out\n",
    "                    if no_improve >= self.n_iter_no_change:\n",
    "                        # Roll back to best iteration\n",
    "                        if self.best_iteration_ is not None:\n",
    "                            self.estimators_ = self.estimators_[:self.best_iteration_]\n",
    "                            self.val_loss_ = self.val_loss_[:self.best_iteration_]\n",
    "                            self.train_loss_ = self.train_loss_[:self.best_iteration_]\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # Convert X to numpy\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Start from baseline init_\n",
    "        # Create array with the same shape of X rows to store init_ for predictions\n",
    "        y_pred = np.full(X.shape[0], self.init_, dtype=float)\n",
    "\n",
    "        # Loop over learners and accumulate their contributions\n",
    "        for estimator in self.estimators_:\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd542cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: XGradient Boost Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 92.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: XGradient Boost Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.98 8.95 8.95]\n",
      "R^2 score on training data: 123.51 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        gb_model = XGBoostClassifier(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0)\n",
    "        gb_model.fit(X, y_labels)\n",
    "        y_pred = gb_model.predict(X_new)\n",
    "        score_train = gb_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: XGradient Boost Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        gb_model = XGBoostRegressor(n_estimators=25, early_stopping=True, reg_lambda=2.0, reg_alpha=2.0, gamma=2.0)\n",
    "        gb_model.fit(X, y_output)\n",
    "        y_pred = gb_model.predict(X_new)\n",
    "        score_train = gb_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: XGradient Boost Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
