{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known that ada boost is boosting with decsion trees as base estimator \n",
    "# We have multiple of weak learners to have a strong learner at the end working sequentially and each weak learner is trained on the previous one errors\n",
    "from Libraries.DecisionTree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1c85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need this copy for cloning decision tree estimator\n",
    "import copy\n",
    "\n",
    "# We work with the base class then inheritence happen to build the classification or regression tree\n",
    "class AdaBoostBase():\n",
    "    \n",
    "    # Initialization\n",
    "    def __init__(self, base_estimator, n_estimators=50, learning_rate=1.0):\n",
    "        # Later we will decide base estimator is classification or regression tree \n",
    "        # Stopping condtions will be no update in weights if error is 0 or 1 \n",
    "        self.base_estimator = base_estimator\n",
    "        # Others is the same default paramerts of API\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        # Storage of leaners and their weights\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        # Get number of samples\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # As usaul we fit means we collect paramerts to use it later\n",
    "        # We need at the end estimators and their weights to use it in prediction --> (except MMSE) no work with hypothesis \n",
    "        # Intialize weights for all samples which will be 1 / number of samples and gonna updated each iterarion\n",
    "        # Matrix of weights for each sample \n",
    "        # Here we will use np.full to give the whole number of samples n_samples the same weight \n",
    "        sample_weights = np.full(n_samples, 1 / n_samples)\n",
    "\n",
    "        # Loop over all learners\n",
    "        for learner in range(self.n_estimators):\n",
    "            \n",
    "           # Create clone of learner to fit the data and update the weights\n",
    "           estimator = copy.deepcopy(self.base_estimator)\n",
    "\n",
    "           # Fit the cloned estimator with sample weights\n",
    "           # Here decsion trees will work with passing sample weights not counter \n",
    "           estimator.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "           # Update sample weights based on the error of the current learner and Calculate error (Alpha) using _boost function which will be different based on classification or regression \n",
    "           sample_weights, alpha = self._boost(X, y, sample_weights, estimator)\n",
    "\n",
    "           # Stopping condition (no more updates on weights) => we return none in the case of error is 0 or 1 in the _boost function and we break the loop here\n",
    "           if sample_weights is None:\n",
    "               break\n",
    "\n",
    "           # Append new model to estimators_ with the equivelant weight in estimatr_weights_ to be used in prediction --> (except MMSE) no work with hypothesis \n",
    "           self.estimators_.append(estimator)\n",
    "           self.estimator_weights_.append(alpha) \n",
    "\n",
    "        return self\n",
    "\n",
    "    # Boost function\n",
    "    # This function will be responsible for calculating the error of the current learner and updating the sample weights based on that error   \n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _boost(self, X, y, sample_weights, estimator):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# Ada Boost Classifier class\n",
    "class AdaBoostClassifier(AdaBoostBase):\n",
    "    \n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters       \n",
    "    def __init__(self, estimator=None, n_estimators=50, learning_rate=1.0):\n",
    "\n",
    "        # Handle the error of estimator\n",
    "        # If estimator is None or not equal decision tree classifier object or max_depth is not 1\n",
    "        # is_instance is used to check object type \n",
    "        if estimator is None or not isinstance(estimator, DecisionTreeClassifier) or (hasattr(estimator, 'max_depth') and estimator.max_depth != 1):\n",
    "            # Here is the correct learner\n",
    "            base_estim_to_use = DecisionTreeClassifier(max_depth=1)\n",
    "        else:\n",
    "            base_estim_to_use = estimator\n",
    "\n",
    "        super().__init__(base_estim_to_use, n_estimators, learning_rate)  \n",
    "\n",
    "    # Boost function\n",
    "    # It is for classification\n",
    "    # It is the most important part in ada boost and it is the one which update the weights and the error for each learner\n",
    "    def _boost(self, X, y, sample_weights, estimator):\n",
    "        # Here we will return sample weight and performance of stump (PS) (Alpha) (Amount of say)\n",
    "        # For binary classification we need to convert labels to -1 and 1 for the mathematical calculation of error and weight update\n",
    "        # We train our learner\n",
    "        y_pred = estimator.predict(X)\n",
    "\n",
    "        # Identify incorrect predictions\n",
    "        # When training y not equal to actual y\n",
    "        # We do this so that we can calculate the error (TE) of the learner and update the weights accordingly\n",
    "        # Make sure type is int at the end\n",
    "        # Matrix of 1 for incorrect predictions and 0 for correct predictions\n",
    "        incorrect = (y_pred != y).astype(int)\n",
    "\n",
    "        # Get the number of unique classes (k) which will be used in the amount of say calculation\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Calculate the error of the learner (TE) which is the sum of weights of incorrect predictions divided by the sum of all weights\n",
    "        # Take care that incorrect total is sum of all samples which were incorrect\n",
    "        total_error = np.dot(sample_weights, incorrect) / np.sum(sample_weights)\n",
    "\n",
    "        # Stopping condition (error is 0 or  1 which means the learner is perfect or completely wrong and no more updates on weights)\n",
    "        # Amount of say will be the highest when error is 0 and the lowest when error is 1\n",
    "        if total_error <= 0: return None, 1.0 \n",
    "        if total_error >= 1.0 - (1.0 / n_classes): return None, None\n",
    "\n",
    "        # Calculate the amount of say (Alpha) which is the logarithm of the ratio of correct predictions to incorrect predictions multiplied by the learning rate\n",
    "        # Add satbility term to avoid division by zero and log of zero (1e-9)\n",
    "        alpha = self.learning_rate * (np.log((1.0 - total_error) / (total_error + 1e-9)) + np.log(n_classes - 1.0))\n",
    "\n",
    "        # Update weight and for software implementation it is different than the mathematical one as we just make e power amout of say * number of incorrect\n",
    "        # So we higher the incorrect weights and lower the correct ones as alpha will be the same but the trick that incorrect = 0 or positive number\n",
    "        sample_weights *= np.exp(alpha * incorrect)\n",
    "\n",
    "        # Nomalize weights to sum to 1\n",
    "        sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "        return sample_weights, alpha\n",
    "    \n",
    "    # Predict probabilities\n",
    "    # This function is ready to use in soft oting later\n",
    "    def predict_proba(self, X):\n",
    "        # Loop over learners and calculate the weighted average of their predictions to get the final probability estimates for each class then get the max of them to get the final prediction\n",
    "        # SAMME method\n",
    "        # Unlike text books in both cases wether it is binary or multi class we will use the same method of weighted average of probabilities and then get the max of them to get the final prediction\n",
    "        # We get the number of classes from first estimator as all estimators will have the same number of classes\n",
    "        n_classes = self.estimators_[0].n_classes_\n",
    "\n",
    "        # Make probabilities matrix initialized with zeros for each class and each sample\n",
    "        # Samples represent as indexes and classes represent as columns\n",
    "        probas = np.zeros((X.shape[0], n_classes))\n",
    "        # We gather all aplha values to get the total alpha for normalization of probabilities\n",
    "        total_alpha = np.sum(self.estimator_weights_)\n",
    "\n",
    "        # Here all alphas are total not for specificlabel so we muliply probabilities of each classes / estimator to make each class have the percentage of this alpha and we add old to new     \n",
    "        for estimator, alpha in zip(self.estimators_, self.estimator_weights_):\n",
    "            # We get the total number of probabilities for each sample per all estimators\n",
    "            probas += (alpha / total_alpha) * estimator.predict_proba(X)\n",
    "        return probas\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # We just return the max of the probabilities of all labels to get the final prediction\n",
    "        # We make axis=1 to ge the max of rows to get the max probability for each sample across all classes\n",
    "        return np.argmax(self.predict_proba(X), axis=1)  \n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "# Ada Boost Regressor class\n",
    "class AdaBoostRegressor(AdaBoostBase):  \n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters \n",
    "    def __init__(self, estimator=None, n_estimators=50, learning_rate=1.0):\n",
    "\n",
    "        # Handle the error of estimator\n",
    "        # If estimator is None or not equal decision tree regressor object or max_depth is not 1\n",
    "        # is_instance is used to check object type \n",
    "        if estimator is None or not isinstance(estimator, DecisionTreeRegressor) or (hasattr(estimator, 'max_depth') and estimator.max_depth != 1):\n",
    "            # Here is the correct learner\n",
    "            base_estim_to_use = DecisionTreeRegressor(max_depth=1)\n",
    "        else:\n",
    "            base_estim_to_use = estimator\n",
    "\n",
    "        super().__init__(base_estim_to_use, n_estimators, learning_rate) \n",
    "\n",
    "    # Boost function\n",
    "    # It is for regression\n",
    "    # It is the most important part in ada boost and it is the one which update the weights and the error for each learner\n",
    "    def _boost(self, X, y, sample_weights, estimator):\n",
    "        # Here we will return sample weight and performance of stump (PS) (Alpha) (Amount of say)\n",
    "        # The steps of regression boostig is different than classification \n",
    "        # We train our learner\n",
    "        y_pred = estimator.predict(X)   \n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = np.abs(y - y_pred)\n",
    "\n",
    "        # Calculate max error\n",
    "        total_error = np.max(residuals)\n",
    "\n",
    "        # Stopping condition (error is 0)\n",
    "        # Amount of say will be the highest when error is 0 \n",
    "        if total_error <= 0: return None, 1.0 \n",
    "\n",
    "        # Calculate the relative error\n",
    "        # Add satbility term to avoid division by zero \n",
    "        e_rel = residuals / (total_error + 1e-9)\n",
    "        \n",
    "        # Calculate weighted error\n",
    "        weighted_error = np.sum(sample_weights * e_rel)\n",
    "\n",
    "        # Stopping criteria (error is highes) --> weighted error is greater than or equal to 0.5 which means the learner is completely wrong and no more updates on weights\n",
    "        if weighted_error >= 0.5: return None, None\n",
    "\n",
    "        # Calculate Beta\n",
    "        beta = weighted_error / (1.0 - weighted_error)\n",
    "\n",
    "        # Calculate the amount of say (Alpha)\n",
    "        # Add satbility term to avoid division by zero \n",
    "        alpha = self.learning_rate * np.log(1.0 / (beta + 1e-9))\n",
    "\n",
    "        # Update weights\n",
    "        # So based on e_rel when you are close the sample weight will be lower and when you are far the sample weight will be higher \n",
    "        # When beta close to zero that means lower e_rel and when beta close to 1 that means higher e_rel\n",
    "        sample_weights *= np.power(beta, 1 - e_rel)\n",
    "\n",
    "        # Nomalize weights to sum to 1\n",
    "        sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "        return sample_weights, alpha\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "            # We just return the average of predictions instead of getting max probabiliy in classification\n",
    "            # Collecting predictions from estimators in array\n",
    "            predictions = np.array([estimator.predict(X) for estimator in self.estimators_]).T\n",
    "\n",
    "            # Applying the weight (alpha) to each tree's prediction\n",
    "            # Return the weighted average of predictions for each sample\n",
    "            return np.average(predictions, axis=1, weights=self.estimator_weights_)\n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1222453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Ada Boost Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Ada Boost Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.71 9.17 9.17]\n",
      "R^2 score on training data: 143.89 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        ada_model = AdaBoostClassifier()\n",
    "        ada_model.fit(X, y_labels)\n",
    "        y_pred = ada_model.predict(X_new)\n",
    "        score_train = ada_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Ada Boost Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        ada_model = AdaBoostRegressor()\n",
    "        ada_model.fit(X, y_output)\n",
    "        y_pred = ada_model.predict(X_new)\n",
    "        score_train = ada_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Ada Boost Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
