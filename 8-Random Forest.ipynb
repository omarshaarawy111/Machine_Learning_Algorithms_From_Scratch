{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac9a1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known that random forest is bagging + decision trees but with some edits \n",
    "# So academiacally we just inherit bagging + decision trees and add edits\n",
    "# Known that random forest work with only one base estimator (Descision Tree) cloning it within bootsreapped data with replacement\n",
    "# But for API Design it is seperated from bagging and no inherit happended and we just duplicate bagging functions from scratch\n",
    "from Libraries.DecisionTree import *\n",
    "from Libraries.Bagging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a43651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For parallelism and working with multiple of cores \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Random Forest class for Classification and Regression\n",
    "# We work with the base class then inheritence happen to build the classification or regression random forest\n",
    "class RandomForestBase():\n",
    "\n",
    "    # Intialization \n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True,  random_state=None, n_jobs=None):\n",
    "        # Here no pruning happen and only prepruning paramaters is for capacity regulazation only\n",
    "        # Here we intialize random forest trees properties there is mandatory and optional propeties based on use case and also your capacity regularization decision   \n",
    "        # min_samples_split = 2, min_samples_leaf = 1 these are mandatory cause they are logically the min amount to split or work rather than it isn't logical\n",
    "        # min_impurity_decrease = 0.0, ccp_alpha = 0.0 becasue the default is no pruning till you decide\n",
    "        # There are common factors that will be passed from random fores to called trees like max_depth, min_samples_split, min_samples_leaf, max_features which was none there but here will be intialized, random_state and n_jobs\n",
    "        # n_jobs=None or 1 is the same and -1 means using all cores\n",
    "        # We can specifiy max features to use per decsion tree with three ways (log2, sqrt, number)\n",
    "        # Here base estimator is desicion tree and no need to specifiy it\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = []\n",
    "        \n",
    "    # We have helper functions\n",
    "    # Bootstrapping samples function\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        # Here we fetch the number of samples from x \n",
    "        # X is combination of samples x features\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Every time we select random numbers of indice with enabling replacement option so we can use same indice in more models\n",
    "        # So we can get the sample 0, 1 or multiple of times \n",
    "        # Here we choose randomly from all samples and we need the size of bootstrapped data = size of orginal data that is why replace option should be applied\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "\n",
    "        # Here we return the bootstrapped data\n",
    "        return X[indices], y[indices]\n",
    "    \n",
    "    # Function of selecting max features technique for each tree\n",
    "    def _get_max_features(self, n_features):\n",
    "        # Here we pass the cirteria of selecting max features to use in each tree and we return the number of features to use in each tree based on the criteria\n",
    "        # For SQRT the number\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        \n",
    "        # For the Log2 number\n",
    "        elif self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        \n",
    "        # For the number only\n",
    "        elif self.max_features is None:\n",
    "            return n_features\n",
    "        \n",
    "        # This is the case of another option\n",
    "        return self.max_features\n",
    "    \n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of make new estimator of tree or predict\n",
    "    # Make estimator function to make new tree with the criteria of max features to use in each tree\n",
    "    def _make_estimator(self, max_features):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Modeling function\n",
    "    # We will have one fit function per every tree then combine all fits in general fit function for all trees\n",
    "    # Fit single tree\n",
    "    def _fit_single_model(self, X, y):\n",
    "        if self.bootstrap:\n",
    "            # If bootstrap is true \n",
    "            # First we recieve the bootstrapped data to model it from given X and y\n",
    "            X_samples, y_samples = self._bootstrap_sample(X, y)\n",
    "        else:\n",
    "            # If bootstrap is false we just work with the original data\n",
    "            X_samples, y_samples = X, y\n",
    "\n",
    "        # Get number of features to pass it to make tree\n",
    "        n_features = X_samples.shape[1]\n",
    "\n",
    "        # Create tree estimator\n",
    "        # Every time we gonna pass the max_features using get_max_features function\n",
    "        tree = self._make_estimator(self._get_max_features(n_features))\n",
    "\n",
    "        # We use the fit function for specific single tree which was already built from scratch before\n",
    "        tree.fit(X_samples, y_samples)\n",
    "\n",
    "        # Every time we return the fitted model\n",
    "        return tree\n",
    "    \n",
    "    # Fit all models function\n",
    "    # Here we gather all fitted models \n",
    "    def fit(self, X, y):\n",
    "        # We set random state wuth the number user entered and seed it\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # Loop over trees number collecting all fitted models in one place to be ready for test phase\n",
    "        # We call _fit_single_model every time\n",
    "        self.models = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._fit_single_model)(X, y) for tree in range(self.n_estimators)\n",
    "        )    \n",
    "        \n",
    "        # We return nothing\n",
    "        return self\n",
    "\n",
    "\n",
    "    # Precit function\n",
    "    # Here no _predict_single_model as bagging we just use one fucntion only\n",
    "    # Here we gather all precited models \n",
    "    def predict(self, X):\n",
    "        # Loop over model collecting all predictions in one place to be ready for aggregation later using cpu cores (parallelism)\n",
    "        # to apply predict we need to concatenate with fit first this is gathered in self.models\n",
    "        # Here every fitted tree work with all samples \n",
    "        # So each sample have multiple predictions for all trees\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(model.predict)(X) for model in self.models\n",
    "        )    \n",
    "        \n",
    "        # Preditction as numpy array\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # After collecting predictions we are ready for aggregation\n",
    "        return self._aggregate(predictions)\n",
    "\n",
    "    # Aggregation function    \n",
    "    # Aggregation to decide the final output\n",
    "    # Here we create just abstract so after inheritence each of classification and regression has its own methodoly of aggregation for final prediction\n",
    "    def _aggregate(self, predictions):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Random Forest Classifier class\n",
    "class RandomForestClassifier(RandomForestBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        random_state=None,\n",
    "        n_jobs=None\n",
    "    ):\n",
    "       \n",
    "        # That is the only parameter i take from user and the rect will be passed to the parent \n",
    "        self.criterion = criterion\n",
    "        super().__init__(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "\n",
    "    # Helper functions\n",
    "    # Make estimator function and the idea is very simple we just call tree classification class taking it is result with determined number of max_features and other essential paramerts\n",
    "    def _make_estimator(self, max_features):\n",
    "        return DecisionTreeClassifier(\n",
    "            criterion=self.criterion,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            max_features=max_features\n",
    "        )\n",
    "    \n",
    "    # Aggregation function    \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # Get number of samples as every sample wich will be columns as every column represents sample and indexes represent trees\n",
    "        n_samples = predictions.shape[1]\n",
    "\n",
    "        # Gather all predictions in one array and make sure its data type the same as the source predictions array\n",
    "        # Intialize\n",
    "        final_predictions = np.zeros(n_samples, dtype=predictions.dtype)\n",
    "\n",
    "        # Apply majority voting \n",
    "        # Loop over all samples to get the most voted label\n",
    "        for sample in range(n_samples):\n",
    "            # Featch trees of samples values\n",
    "            # Every sample represents column\n",
    "            votes = predictions[:, sample]\n",
    "\n",
    "            # Here we get unique classes then counting them\n",
    "            # We return two related arrays of classes and labels \n",
    "            # One for unique values and other for unique counts\n",
    "            # So return the index of laregest count will be equivelant to the label of largest count\n",
    "            classes, counts = np.unique(votes, return_counts=True)\n",
    "            majority_voting = classes[np.argmax(counts)]\n",
    "            final_predictions[sample] = majority_voting\n",
    "\n",
    "        # Return the final predictions of the test data\n",
    "        return final_predictions\n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        # Return accuracy score \n",
    "        return np.mean(self.predict(X) == y)\n",
    "    \n",
    "# Random Forest Regressor class\n",
    "class RandomForestRegressor(RandomForestBase):\n",
    "\n",
    "    # Intialization\n",
    "    # We pass known numbers of paramters to parent class and also at the same time get known numbers of paramters    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        random_state=None,\n",
    "        n_jobs=None\n",
    "    ):\n",
    "       \n",
    "       # No need to wrtie criterion = 'mse' as default because in our implementation of decision tree regression we just work with one criterion which is mse and no need to specify it as user input\n",
    "        super().__init__(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "\n",
    "    # Helper functions\n",
    "    # Make estimator function and the idea is very simple we just call tree regression class taking it is result with determined number of max_features and other essential paramerts\n",
    "    def _make_estimator(self, max_features):\n",
    "        return DecisionTreeRegressor(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            max_features=max_features\n",
    "        )\n",
    "    \n",
    "    # Aggregation function    \n",
    "    def _aggregate(self, predictions):\n",
    "\n",
    "        # We just return the mean of every sample wich will be columns as every column represents sample and indexes represent models\n",
    "        return np.mean(predictions, axis=0)  \n",
    "    \n",
    "    # Score\n",
    "    def score(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5391be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classification\n",
      "Model: Random Forest Classifier\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 1]\n",
      "Accuracy score on training data: 91.00 %\n",
      "----------------------------------------\n",
      "Task: Regression\n",
      "Model: Random Forest Regressor\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [6.45 8.02 8.02]\n",
      "R^2 score on training data: 98.42 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "# Use rand then multiple by 2 to make sure the samples values are between 0 and 2 this would make sure we simulate feature scaling\n",
    "# Here we create 2 features\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "# Combine features to create X matrix\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Y value will split to intercept + value + noise from 1st feature [row, columns] to simulate real data doing regression\n",
    "y_output = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "\n",
    "# Y value will be Continuous target (intercept + value + noise) from 1st feature [row, columns] then convert to binary classes doing classification\n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "\n",
    "# Test data with 2 rows\n",
    "X_new = np.array([[1,2], [3,4], [4,3]])\n",
    "\n",
    "# Apply default bagging with all possible tasks \n",
    "# Tasks\n",
    "tasks = ['Classification', 'Regression']\n",
    "for task in tasks:\n",
    "    if task == 'Classification':\n",
    "        # Get Default Bagging Classifier object\n",
    "        rf_model = RandomForestClassifier(max_depth=3, n_jobs=-1)\n",
    "        rf_model.fit(X, y_labels)\n",
    "        y_pred = rf_model.predict(X_new)\n",
    "        score_train = rf_model.score(X, y_labels) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Random Forest Classifier')\n",
    "        print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "        print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "\n",
    "    elif task == 'Regression':\n",
    "        # Get Default Bagging Regressor object\n",
    "        rf_model = RandomForestRegressor(max_depth=3, n_jobs=-1)\n",
    "        rf_model.fit(X, y_output)\n",
    "        y_pred = rf_model.predict(X_new)\n",
    "        score_train = rf_model.score(X, y_output) * 100\n",
    "        print(f'Task: {task}')\n",
    "        print(f'Model: Random Forest Regressor')\n",
    "        print(f'Predictions for {X_new.tolist()}: {np.round(y_pred,2)}')\n",
    "        print(f'R^2 score on training data: {score_train:.2f} %')\n",
    "    print('-'*40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
