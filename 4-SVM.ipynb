{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f303ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# SVM class for Classification and Regression\n",
    "# Kernal functions for SVM \n",
    "# It will be used for classification and regression \n",
    "# Linear Kernel\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)   \n",
    "\n",
    "# Polynomial Kernel\n",
    "def polynomial_kernel(x1, x2 , gamma=1, degree=3, coef0=1):\n",
    "    return ((gamma * np.dot(x1, x2)) + coef0) ** degree\n",
    "\n",
    "# Radial Basis Function (RBF) Kernel \n",
    "def rbf_kernel(x1, x2, gamma=1):\n",
    "    distance = np.linalg.norm(x1 - x2) ** 2\n",
    "    # Know that gamm = 1 / (2 * sigma^2)      \n",
    "    return np.exp(-gamma * distance)\n",
    "\n",
    "# Sigmoid Kernel\n",
    "def sigmoid_kernel(x1, x2, gamma=0.01, coef0=0):\n",
    "    return np.tanh((gamma * np.dot(x1, x2)) + coef0)\n",
    "\n",
    "# Classification and Regression using SVM\n",
    "# Classification class\n",
    "class SVC():\n",
    "\n",
    "    # Initialization\n",
    "    # Tol is threshold for stopping criteria \n",
    "    def __init__(self, c = 1.0, kernel = 'Linear', degree=3, gamma=1, coef0=1, tol=1e-3, max_iter=1000, decision_function_shape='ovr'):\n",
    "        self.c = c\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.decision_function_shape = decision_function_shape\n",
    "        self.models = [] # To store sub-models for multiclass\n",
    "        self.classes = None\n",
    "        \n",
    "        # For each case of kernal is assigned by the retun value of the function to the varaibale kernal and assign the all parameters\n",
    "        # We make varaible as function \n",
    "        if kernel == 'Linear':\n",
    "            self.kernel = lambda x1, x2: linear_kernel(x1, x2)\n",
    "\n",
    "        elif kernel == 'Polynomial':\n",
    "            self.kernel = lambda x1, x2: polynomial_kernel(x1, x2, gamma=self.gamma, degree=self.degree, coef0=self.coef0)    \n",
    "\n",
    "        elif kernel == 'RBF':\n",
    "            self.kernel = lambda x1, x2: rbf_kernel(x1, x2, gamma=self.gamma)\n",
    "\n",
    "        elif kernel == 'Sigmoid':\n",
    "            self.kernel = lambda x1, x2: sigmoid_kernel(x1, x2, gamma=self.gamma, coef0=self.coef0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel.\")    \n",
    "\n",
    "    # Internal helper for binary training (SMO)\n",
    "    def _fit_binary(self, X, y):\n",
    "        m, n = X.shape\n",
    "        theta = np.zeros(m)\n",
    "        b = 0\n",
    "\n",
    "        # Compute the Kernel matrix\n",
    "        K_matrix = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                K_matrix[i, j] = self.kernel(X[i], X[j])   \n",
    "\n",
    "        # SMO Algorithm\n",
    "        for iteration in range(self.max_iter):      \n",
    "            alpha_prev = np.copy(theta)\n",
    "\n",
    "            for i in range(m):\n",
    "                f_xi = np.sum(theta * y * K_matrix[:, i]) + b\n",
    "                E_i = f_xi - y[i]\n",
    "\n",
    "                if (y[i] * E_i < -self.tol and theta[i] < self.c) or (y[i] * E_i > self.tol and theta[i] > 0):\n",
    "                    j = np.random.randint(0, m)\n",
    "                    while j == i:\n",
    "                        j = np.random.randint(0, m)\n",
    "\n",
    "                    f_xj = np.sum(theta * y * K_matrix[:, j]) + b\n",
    "                    E_j = f_xj - y[j]\n",
    "\n",
    "                    alpha_i_old, alpha_j_old = theta[i], theta[j]\n",
    "\n",
    "                    if y[i] != y[j]:\n",
    "                        L, H = max(0, alpha_j_old - alpha_i_old), min(self.c, self.c + alpha_j_old - alpha_i_old)\n",
    "                    else:\n",
    "                        L, H = max(0, alpha_i_old + alpha_j_old - self.c), min(self.c, alpha_i_old + alpha_j_old)\n",
    "\n",
    "                    if L == H: continue\n",
    "\n",
    "                    eta = 2.0 * K_matrix[i, j] - K_matrix[i, i] - K_matrix[j, j]\n",
    "                    if eta >= 0: continue\n",
    "\n",
    "                    theta[j] -= (y[j] * (E_i - E_j)) / eta\n",
    "                    theta[j] = np.clip(theta[j], L, H)\n",
    "\n",
    "                    if abs(theta[j] - alpha_j_old) < 1e-5: continue\n",
    "\n",
    "                    theta[i] += y[i] * y[j] * (alpha_j_old - theta[j])\n",
    "\n",
    "                    b1 = b - E_i - y[i] * (theta[i] - alpha_i_old) * K_matrix[i, i] - y[j] * (theta[j] - alpha_j_old) * K_matrix[i, j]   \n",
    "                    b2 = b - E_j - y[i] * (theta[i] - alpha_i_old) * K_matrix[i, j] - y[j] * (theta[j] - alpha_j_old) * K_matrix[j, j]\n",
    "\n",
    "                    if 0 < theta[i] < self.c: b = b1\n",
    "                    elif 0 < theta[j] < self.c: b = b2\n",
    "                    else: b = (b1 + b2) / 2\n",
    "\n",
    "            if np.linalg.norm(theta - alpha_prev) < self.tol:\n",
    "                break\n",
    "        \n",
    "        return {\"theta\": theta, \"b\": b, \"X\": X, \"y\": y}\n",
    "    \n",
    "    # Fit \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        self.models = []\n",
    "\n",
    "        if n_classes <= 2:\n",
    "            binary_y = np.where(y == self.classes[0], -1, 1)\n",
    "            self.models.append(self._fit_binary(X, binary_y))\n",
    "            \n",
    "        elif self.decision_function_shape == 'ovr':\n",
    "            # One vs Rest\n",
    "            for c in self.classes:\n",
    "                binary_y = np.where(y == c, 1, -1)\n",
    "                self.models.append(self._fit_binary(X, binary_y))\n",
    "\n",
    "        elif self.decision_function_shape == 'ovo':\n",
    "            # One vs One\n",
    "            for i in range(n_classes):\n",
    "                for j in range(i + 1, n_classes):\n",
    "                    idx = np.where((y == self.classes[i]) | (y == self.classes[j]))\n",
    "                    X_sub, y_sub = X[idx], y[idx]\n",
    "                    binary_y = np.where(y_sub == self.classes[i], 1, -1)\n",
    "                    model = self._fit_binary(X_sub, binary_y)\n",
    "                    model['cls_pair'] = (self.classes[i], self.classes[j])\n",
    "                    self.models.append(model)\n",
    "        \n",
    "        # Bias value and other theta values \n",
    "        self.intercept_ = self.models[0]['b']   \n",
    "        self.coef_ = self.models[0]['theta']         \n",
    "\n",
    "    # Decision function for a single model\n",
    "    def _get_score(self, X, model):\n",
    "        scores = []\n",
    "        for x in X:\n",
    "            score = np.sum(model['theta'] * model['y'] * [self.kernel(x_tr, x) for x_tr in model['X']]) + model['b']\n",
    "            scores.append(score)\n",
    "        return np.array(scores)\n",
    "\n",
    "    # Predict             \n",
    "    def predict(self, X):\n",
    "        if len(self.classes) <= 2:\n",
    "            scores = self._get_score(X, self.models[0])\n",
    "            return np.where(scores >= 0, self.classes[1], self.classes[0])\n",
    "\n",
    "        if self.decision_function_shape == 'ovr':\n",
    "            # Highest confidence wins\n",
    "            all_scores = np.array([self._get_score(X, m) for m in self.models])\n",
    "            return self.classes[np.argmax(all_scores, axis=0)]\n",
    "\n",
    "        elif self.decision_function_shape == 'ovo':\n",
    "            # Voting system\n",
    "            votes = np.zeros((len(X), len(self.classes)))\n",
    "            for m in self.models:\n",
    "                scores = self._get_score(X, m)\n",
    "                preds = np.where(scores >= 0, m['cls_pair'][0], m['cls_pair'][1])\n",
    "                for idx, p in enumerate(preds):\n",
    "                    votes[idx, np.where(self.classes == p)[0][0]] += 1\n",
    "            return self.classes[np.argmax(votes, axis=1)]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    # This function is ready to use in soft oting later\n",
    "    # We take in cosideration multiclasification case and binary classofocation case\n",
    "    def predict_proba(self, X):\n",
    "        scores = []\n",
    "        if len(self.classes) <= 2:\n",
    "            scores = self._get_score(X, self.models[0])\n",
    "            probs_class_1 = 1 / (1 + np.exp(-scores))  \n",
    "            return np.vstack([1 - probs_class_1, probs_class_1]).T\n",
    "        else:\n",
    "            all_scores = np.array([self._get_score(X, m) for m in self.models])\n",
    "            exp_scores = np.exp(all_scores)\n",
    "            probas = exp_scores / np.sum(exp_scores, axis=0)\n",
    "            return probas.T  \n",
    "    # Score \n",
    "    def score(self, X_new, y):\n",
    "        # Return accuracy score of the model\n",
    "        y_pred = self.predict(X_new)\n",
    "        return np.mean(y_pred == y) \n",
    "\n",
    "# Regression remains largely the same but usually doesn't use OVO/OVR\n",
    "class SVR():\n",
    "    def __init__(self, c=1.0, epsilon=0.1, kernel='Linear', degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000):\n",
    "        self.c = c\n",
    "        self.epsilon = epsilon\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if kernel == 'Linear':\n",
    "            self.kernel = lambda x1, x2: linear_kernel(x1, x2)\n",
    "        elif kernel == 'Polynomial':\n",
    "            self.kernel = lambda x1, x2: polynomial_kernel(x1, x2, gamma=self.gamma, degree=self.degree, coef0=self.coef0)\n",
    "        elif kernel == 'RBF':\n",
    "            self.kernel = lambda x1, x2: rbf_kernel(x1, x2, gamma=self.gamma)\n",
    "        elif kernel == 'Sigmoid':\n",
    "            self.kernel = lambda x1, x2: sigmoid_kernel(x1, x2, gamma=self.gamma, coef0=self.coef0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        m = len(X)\n",
    "        self.alpha = np.zeros(m)\n",
    "        self.b = 0\n",
    "\n",
    "        self.K_matrix = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                self.K_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(m):\n",
    "                pred = np.sum(self.alpha * self.K_matrix[:, i]) + self.b\n",
    "                error = pred - self.y[i]\n",
    "                if abs(error) > self.epsilon:\n",
    "                    self.alpha[i] -= self.lr * error\n",
    "                    self.alpha[i] = np.clip(self.alpha[i], -self.c, self.c)\n",
    "                    self.b -= self.lr * error\n",
    "\n",
    "        # Bias value and other theta values \n",
    "        self.intercept_ = self.b\n",
    "        self.coef_ = self.alpha          \n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            pred = np.sum(self.alpha * [self.kernel(x_tr, x) for x_tr in self.X]) + self.b\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def score(self, X_new, y):\n",
    "        # Return R^2 score \n",
    "        y_pred = self.predict(X_new)\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_residual = np.sum((y - y_pred) ** 2)\n",
    "        return 1 - ss_residual / ss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51af1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: SVC (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 84.00 %\n",
      "Intercept: -2.22\n",
      "Coefficient: [0.11 1.   0.   0.   1.   1.   0.   1.   0.   0.   0.   1.   1.   0.25\n",
      " 0.   0.   1.   0.   1.   0.   0.   0.   1.   1.   0.64 0.   0.   1.\n",
      " 0.   1.   0.   1.   1.   0.   1.   0.   1.   0.   1.   1.   0.   0.\n",
      " 0.   0.66 1.   0.   0.   1.   0.   1.   1.   1.   0.   0.   1.   0.\n",
      " 0.   0.   0.   0.   1.   1.   1.   0.   0.   1.   0.   1.   0.3  0.\n",
      " 0.   1.   0.   0.   0.   0.   0.   0.   0.   0.35 0.   0.   1.   0.\n",
      " 0.   0.   1.   1.   0.12 0.   1.   0.   0.   1.   0.   0.   0.   1.\n",
      " 0.54 0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 1]\n",
      "Accuracy score on training data: 86.00 %\n",
      "Intercept: -2.18\n",
      "Coefficient: [0.   1.   0.   0.   1.   1.   0.   1.   0.   0.   0.   0.   1.   0.01\n",
      " 0.   0.   1.   0.   1.   0.   0.   0.   1.   1.   0.57 0.   0.   1.\n",
      " 0.   1.   0.   1.   1.   0.   1.   0.   0.   0.1  1.   1.   0.   0.\n",
      " 0.   0.49 1.   0.   0.   1.   0.   1.   1.   0.65 0.   0.   0.32 0.\n",
      " 0.   0.   0.   0.   0.34 1.   1.   0.   0.   1.   0.   1.   0.   0.\n",
      " 0.   1.   0.   0.   0.03 0.   0.   0.   0.93 0.   0.   0.   1.   0.\n",
      " 0.   0.   1.   1.   0.   0.   1.   0.   0.   1.   0.   0.   0.   1.\n",
      " 1.   0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 83.00 %\n",
      "Intercept: -0.28\n",
      "Coefficient: [0.41 1.   0.51 0.   1.   1.   0.   1.   0.   0.   0.   0.89 1.   0.17\n",
      " 0.   0.   1.   0.98 1.   0.   0.   0.   1.   1.   0.05 0.   0.   1.\n",
      " 0.   1.   0.   1.   1.   0.   1.   0.   0.   0.3  1.   1.   0.   0.\n",
      " 0.   1.   1.   0.   0.   1.   0.   1.   1.   1.   0.   0.   1.   0.\n",
      " 0.   0.   0.   0.   1.   1.   1.   0.   0.   1.   0.   1.   0.01 0.\n",
      " 0.   1.   0.   0.   0.94 0.   0.   0.09 1.   0.   0.   0.   1.   0.06\n",
      " 0.   0.   1.   1.   0.   0.   1.   0.   0.17 1.   0.   0.   0.   1.\n",
      " 0.95 0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 52.00 %\n",
      "Intercept: 0.67\n",
      "Coefficient: [0.   1.   1.   0.25 1.   0.   0.   1.   0.   0.   1.   0.   1.   1.\n",
      " 1.   1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.\n",
      " 1.   1.   1.   1.   1.   0.   1.   0.   1.   1.   1.   1.   0.   0.\n",
      " 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   1.   1.\n",
      " 1.   1.   0.   1.   1.   1.   1.   0.   1.   1.   0.   1.   1.   1.\n",
      " 1.   1.   1.   1.   0.   0.   0.   1.   1.   0.   0.   1.   1.   0.\n",
      " 1.   1.   1.   1.   1.   0.   1.   0.75 1.   1.   1.   1.   0.   1.\n",
      " 1.   1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 2 2]\n",
      "Accuracy score on training data: 42.00 %\n",
      "Intercept: 1.87\n",
      "Coefficient: [ 0.    1.    1.    0.    0.    0.14  0.    1.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.31  0.43  1.    0.   -0.    0.    1.    0.56\n",
      "  1.    0.    0.    0.    0.    0.    0.    1.    1.    0.    0.   -0.\n",
      "  0.    0.    1.    1.    0.    0.    0.    1.    0.    0.    0.    0.97\n",
      "  0.    1.    0.    0.    0.    0.    1.    0.    0.64  1.    0.    0.\n",
      "  1.    0.06  1.    0.    1.    1.    0.    1.    1.    0.    1.    0.\n",
      "  0.    0.    0.    0.   -0.    1.    0.    1.    0.    1.    1.    0.\n",
      "  0.    0.    0.    0.45  0.    1.    1.    0.    0.41  0.    0.    0.\n",
      "  0.    0.    1.    1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 2]\n",
      "Accuracy score on training data: 55.00 %\n",
      "Intercept: 1.52\n",
      "Coefficient: [0.   1.   0.3  0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.53 0.   1.   0.   0.   0.   0.02 0.22 1.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.\n",
      " 0.   1.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   1.   0.\n",
      " 1.   1.   0.   0.   1.   0.2  1.   0.   1.   0.01 0.08 1.   0.12 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.   1.   0.\n",
      " 0.   0.   0.27 1.   0.   0.82 0.65 0.   0.   0.   0.61 0.   0.   0.\n",
      " 1.   1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 48.00 %\n",
      "Intercept: -0.04\n",
      "Coefficient: [0.   1.   1.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.17 0.\n",
      " 0.   0.   1.   1.   1.   0.   0.   0.   0.8  0.36 1.   0.59 0.   0.\n",
      " 0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.\n",
      " 0.   1.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   1.   0.\n",
      " 1.   1.   0.   0.   1.   1.   1.   0.   1.   1.   0.01 0.57 1.   0.\n",
      " 0.68 0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   1.   1.   0.\n",
      " 0.   0.   1.   1.   0.   0.08 1.   0.   1.   0.   0.54 0.   0.   0.\n",
      " 1.   1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 40.00 %\n",
      "Intercept: 0.53\n",
      "Coefficient: [0.   1.   1.   1.   0.   1.   1.   1.   0.   1.   0.   1.   1.   1.\n",
      " 1.   0.   0.   1.   1.   0.44 1.   0.   1.   1.   1.   1.   0.   0.\n",
      " 1.   1.   1.   1.   1.   0.44 1.   0.   1.   1.   1.   1.   1.   1.\n",
      " 0.   1.   0.   0.   0.   0.   1.   1.   0.   0.   0.   0.   1.   1.\n",
      " 1.   1.   0.   0.   1.   0.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      " 1.   0.   0.   0.   0.   1.   0.   1.   1.   1.   1.   1.   1.   0.\n",
      " 1.   1.   0.   1.   1.   1.   0.   1.   1.   0.   1.   0.   0.   0.\n",
      " 1.   1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 2 2]\n",
      "Accuracy score on training data: 54.00 %\n",
      "Intercept: 1.57\n",
      "Coefficient: [ 1.    1.    0.    0.    1.    0.    1.    0.    0.31  0.    0.77  0.\n",
      "  1.    1.    0.31  0.    0.59  0.    0.    1.    0.   -0.    0.    1.\n",
      "  1.    1.    0.    1.    0.    1.    0.    0.    1.    1.    1.    1.\n",
      "  1.    1.    0.    1.    1.    0.    1.    0.97  0.    0.    0.    0.\n",
      "  1.    0.    1.    0.    0.66  1.    0.    0.    1.    0.    0.61  0.\n",
      "  0.58  0.    0.    0.    1.    1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 1 2]\n",
      "Accuracy score on training data: 52.00 %\n",
      "Intercept: 1.38\n",
      "Coefficient: [ 1.    0.22  0.    0.    0.03  0.    1.    0.    0.01  0.    0.    0.\n",
      "  0.    1.    0.    0.    0.    0.    0.    1.   -0.    0.    0.    1.\n",
      "  1.    1.    0.    0.   -0.    1.    0.    0.    1.    0.76  1.    1.\n",
      "  1.    1.    0.24  1.    1.    0.    1.    0.    0.    0.    0.    0.\n",
      "  1.    0.    1.   -0.    0.69  1.    0.    0.96  0.43  0.    0.17  0.\n",
      "  0.1   0.    0.25  0.    1.    1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 0 0]\n",
      "Accuracy score on training data: 53.00 %\n",
      "Intercept: 0.07\n",
      "Coefficient: [1.   1.   0.   1.   1.   0.   1.   0.   1.   0.   1.   0.   1.   1.\n",
      " 0.24 0.   0.   0.   0.   1.   0.   0.   0.   1.   1.   1.   0.   0.27\n",
      " 0.   1.   0.   0.   1.   1.   1.   1.   1.   1.   0.28 1.   1.   0.52\n",
      " 1.   0.31 0.   0.   0.   0.15 1.   0.12 1.   0.   0.31 1.   0.   0.\n",
      " 1.   0.   0.87 0.   1.   0.   0.   0.   1.   1.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 44.00 %\n",
      "Intercept: 1.47\n",
      "Coefficient: [1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "----------------------------------------\n",
      "Method: SVR (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0.51061306 1.82983668 2.4862791 ]\n",
      "Accuracy score on training data: 58.93 %\n",
      "Intercept: -0.15\n",
      "Coefficients: [ 0.15 -0.42 -0.16  0.   -0.69 -0.48  0.1  -0.39  0.02  0.12 -0.04  0.28\n",
      " -0.64  0.27 -0.01  0.01  0.56 -0.14  0.62  0.02 -0.01  0.01  0.53  0.5\n",
      " -0.22 -0.01  0.16 -0.51 -0.01  0.35  0.17 -0.4   0.46  0.06 -0.54  0.01\n",
      "  0.24  0.21  0.64 -0.57  0.01  0.01  0.01 -0.29 -0.82  0.16  0.01  0.51\n",
      " -0.01 -0.43  0.45  0.28 -0.12 -0.1  -0.31  0.01 -0.13 -0.18 -0.1   0.12\n",
      " -0.25  0.51 -0.41 -0.02 -0.2  -0.45 -0.01  0.58 -0.14  0.12 -0.02  0.4\n",
      "  0.01  0.16  0.14  0.01 -0.14 -0.26  0.24 -0.23 -0.01 -0.02  0.7  -0.11\n",
      " -0.06  0.11  0.49 -0.45  0.21 -0.04  0.55  0.05 -0.17 -0.71 -0.02  0.01\n",
      "  0.01  0.38 -0.26 -0.18]\n",
      "----------------------------------------\n",
      "Method: SVR (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 0.50897682 -1.93519542 -2.77095822]\n",
      "Accuracy score on training data: 60.52 %\n",
      "Intercept: -0.11\n",
      "Coefficients: [ 0.16 -0.43 -0.17  0.   -0.65 -0.5   0.11 -0.32  0.01  0.12  0.    0.25\n",
      " -0.62  0.25 -0.    0.    0.56 -0.12  0.65  0.05 -0.    0.    0.51  0.52\n",
      " -0.21 -0.01  0.13 -0.52  0.01  0.31  0.09 -0.4   0.44  0.01 -0.55  0.\n",
      "  0.2   0.18  0.65 -0.58 -0.    0.   -0.12 -0.26 -0.79  0.13  0.    0.5\n",
      " -0.   -0.36  0.44  0.31 -0.05  0.06 -0.26  0.   -0.08 -0.16 -0.13  0.02\n",
      " -0.24  0.53 -0.41 -0.01 -0.19 -0.46 -0.01  0.58 -0.14  0.11 -0.02  0.37\n",
      " -0.04  0.07  0.24  0.   -0.18 -0.22  0.3  -0.19 -0.   -0.02  0.73 -0.04\n",
      " -0.13  0.11  0.54 -0.45  0.17 -0.05  0.55  0.05 -0.18 -0.76 -0.02  0.\n",
      "  0.    0.35 -0.26 -0.16]\n",
      "----------------------------------------\n",
      "Method: SVR (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0.45339043 0.32501044 0.32625409]\n",
      "Accuracy score on training data: 61.70 %\n",
      "Intercept: 0.31\n",
      "Coefficients: [ 0.17 -0.44 -0.12 -0.02 -0.68 -0.5   0.05 -0.33  0.03 -0.02  0.09  0.18\n",
      " -0.62  0.17 -0.03  0.02  0.54 -0.13  0.64  0.05 -0.02  0.04  0.48  0.49\n",
      " -0.17 -0.02  0.05 -0.56 -0.01  0.27  0.03 -0.4   0.39 -0.02 -0.58  0.02\n",
      "  0.15  0.2   0.66 -0.55  0.02  0.02  0.01 -0.25 -0.73  0.03  0.02  0.46\n",
      " -0.02 -0.37  0.38  0.28  0.03  0.22 -0.26  0.03 -0.03 -0.06  0.02  0.03\n",
      " -0.21  0.52 -0.4  -0.02 -0.15 -0.47 -0.02  0.56 -0.05  0.04 -0.02  0.31\n",
      "  0.02  0.03  0.31  0.03 -0.05 -0.21  0.34 -0.18 -0.02 -0.02  0.74  0.04\n",
      " -0.04  0.08  0.53 -0.47  0.14 -0.03  0.56  0.06 -0.13 -0.81 -0.02  0.02\n",
      "  0.02  0.29 -0.23 -0.11]\n",
      "----------------------------------------\n",
      "Method: SVR (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [-0.09220088 -0.45280965 -0.44707814]\n",
      "Accuracy score on training data: -204.88 %\n",
      "Intercept: -0.51\n",
      "Coefficients: [ 0.19 -0.6  -0.49 -0.46 -0.95 -0.34 -0.45 -1.    0.43 -0.44  0.15  0.67\n",
      " -0.24  0.69 -0.43  0.67  0.39 -0.41  0.7   0.29 -0.54  0.28  0.51  0.17\n",
      " -0.63 -0.73  0.74 -0.63 -0.52  0.68  0.58 -0.67  0.47 -0.48 -0.63  0.65\n",
      "  0.69  0.75  0.22 -0.26  0.71  0.66  0.7  -0.85 -0.21  0.62  0.7   0.43\n",
      " -0.52 -1.    0.51  0.1   0.79  0.83 -1.    0.43 -0.72 -0.78  0.59  0.67\n",
      " -0.65  0.72 -0.35 -0.65 -0.64 -0.35 -0.7   0.43 -0.45  0.76 -0.54  0.5\n",
      "  0.72  0.63  0.81  0.52  0.69 -0.97  0.79 -0.99 -0.46 -0.88  0.18  0.81\n",
      "  0.63 -0.48 -0.13 -0.71  0.71 -0.59  0.69 -0.54 -0.48 -0.36 -0.68  0.57\n",
      "  0.66  0.51 -0.59 -0.76]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Generate continuous target then convert to binary classes (0 and 1) also have low and high thresholds to generate \n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "low_threshold = np.percentile(y_continuous, 33)\n",
    "high_threshold = np.percentile(y_continuous, 66)\n",
    "y_multi = np.zeros(m, dtype=int)\n",
    "y_multi[y_continuous > low_threshold] = 1\n",
    "y_multi[y_continuous > high_threshold] = 2\n",
    "\n",
    "\n",
    "# Test data\n",
    "X_new = np.array([[1, 2], [3, 4], [4, 3]])\n",
    "\n",
    "\n",
    "# we will just test the SVC \n",
    "models = {\n",
    "    \"SVC (Linear)\": SVC(kernel='Linear'),\n",
    "    \"SVC (Polynomial)\": SVC(kernel='Polynomial', gamma=0.5),\n",
    "    \"SVC (RBF)\": SVC(kernel='RBF', gamma=0.5),\n",
    "    \"SVC (Sigmoid)\": SVC(kernel='Sigmoid'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    model.fit(X, y_labels)\n",
    "    # Note: SVC predicts -1 or 1, while y_labels are 0 or 1.\n",
    "    # The score function in the SVC class handles this internally if y_labels is used.\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, y_labels) * 100\n",
    "    print(f'Method: {name}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficient: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# we will just test SVR with multiclass using OVR\n",
    "models = {\n",
    "    \"SVC (Linear)\": SVC(kernel='Linear', decision_function_shape='ovr'),\n",
    "    \"SVC (Polynomial)\": SVC(kernel='Polynomial', gamma=0.5, decision_function_shape='ovr'),\n",
    "    \"SVC (RBF)\": SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'),\n",
    "    \"SVC (Sigmoid)\": SVC(kernel='Sigmoid', decision_function_shape='ovr'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    model.fit(X, y_multi)\n",
    "    # Note: SVC predicts -1 or 1, while y_labels are 0 or 1.\n",
    "    # The score function in the SVC class handles this internally if y_labels is used.\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, y_labels) * 100  \n",
    "    print(f'Method: {name}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficient: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# we will just test SVR with multiclass using OVO\n",
    "models = {\n",
    "    \"SVC (Linear)\": SVC(kernel='Linear', decision_function_shape='ovo'),\n",
    "    \"SVC (Polynomial)\": SVC(kernel='Polynomial', gamma=0.5, decision_function_shape='ovo'),\n",
    "    \"SVC (RBF)\": SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'),\n",
    "    \"SVC (Sigmoid)\": SVC(kernel='Sigmoid', decision_function_shape='ovo'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    model.fit(X, y_multi)\n",
    "    # Note: SVC predicts -1 or 1, while y_labels are 0 or 1.\n",
    "    # The score function in the SVC class handles this internally if y_labels is used.\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, y_labels) * 100\n",
    "    print(f'Method: {name}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficient: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# we will just test the SVR \n",
    "models = {\n",
    "    \"SVR (Linear)\": SVR(kernel='Linear'),\n",
    "    \"SVR (Polynomial)\": SVR(kernel='Polynomial', gamma=0.5),\n",
    "    \"SVR (RBF)\": SVR(kernel='RBF', gamma=0.5),\n",
    "    \"SVR (Sigmoid)\": SVR(kernel='Sigmoid'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    # Use y_labels for binary, y_multi for multiclass\n",
    "    target = y_labels if \"decision_function_shape\" not in name else y_multi\n",
    "    model.fit(X, target)\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, target) * 100\n",
    "    print(f'Method: {name}') \n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficients: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
