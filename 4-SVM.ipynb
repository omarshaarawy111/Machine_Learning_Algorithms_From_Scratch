{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f303ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# SVM class for Classification and Regression\n",
    "# Kernal functions for SVM \n",
    "# It will be used for classification and regression \n",
    "# Linear Kernel\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)   \n",
    "\n",
    "# Polynomial Kernel\n",
    "def polynomial_kernel(x1, x2 , gamma=1, degree=3, coef0=1):\n",
    "    return ((gamma * np.dot(x1, x2)) + coef0) ** degree\n",
    "\n",
    "# Radial Basis Function (RBF) Kernel \n",
    "def rbf_kernel(x1, x2, gamma=1):\n",
    "    distance = np.linalg.norm(x1 - x2) ** 2\n",
    "    # Know that gamm = 1 / (2 * sigma^2)      \n",
    "    return np.exp(-gamma * distance)\n",
    "\n",
    "# Sigmoid Kernel\n",
    "def sigmoid_kernel(x1, x2, gamma=0.01, coef0=0):\n",
    "    return np.tanh((gamma * np.dot(x1, x2)) + coef0)\n",
    "\n",
    "# Classification and Regression using SVM\n",
    "# Classification class\n",
    "class SVC():\n",
    "\n",
    "    # Initialization\n",
    "    # Tol is threshold for stopping criteria \n",
    "    def __init__(self, c = 1.0, kernel = 'Linear', degree=3, gamma=1, coef0=1, tol=1e-3, max_iter=1000, decision_function_shape='ovr'):\n",
    "        self.c = c\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.decision_function_shape = decision_function_shape\n",
    "        self.models = [] # To store sub-models for multiclass\n",
    "        self.classes = None\n",
    "        \n",
    "        # For each case of kernal is assigned by the retun value of the function to the varaibale kernal and assign the all parameters\n",
    "        # We make varaible as function \n",
    "        if kernel == 'Linear':\n",
    "            self.kernel = lambda x1, x2: linear_kernel(x1, x2)\n",
    "\n",
    "        elif kernel == 'Polynomial':\n",
    "            self.kernel = lambda x1, x2: polynomial_kernel(x1, x2, gamma=self.gamma, degree=self.degree, coef0=self.coef0)    \n",
    "\n",
    "        elif kernel == 'RBF':\n",
    "            self.kernel = lambda x1, x2: rbf_kernel(x1, x2, gamma=self.gamma)\n",
    "\n",
    "        elif kernel == 'Sigmoid':\n",
    "            self.kernel = lambda x1, x2: sigmoid_kernel(x1, x2, gamma=self.gamma, coef0=self.coef0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel.\")    \n",
    "\n",
    "    # Internal helper for binary training (SMO)\n",
    "    def _fit_binary(self, X, y):\n",
    "        m, n = X.shape\n",
    "        theta = np.zeros(m)\n",
    "        b = 0\n",
    "\n",
    "        # Compute the Kernel matrix\n",
    "        K_matrix = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                K_matrix[i, j] = self.kernel(X[i], X[j])   \n",
    "\n",
    "        # SMO Algorithm\n",
    "        for iteration in range(self.max_iter):      \n",
    "            alpha_prev = np.copy(theta)\n",
    "\n",
    "            for i in range(m):\n",
    "                f_xi = np.sum(theta * y * K_matrix[:, i]) + b\n",
    "                E_i = f_xi - y[i]\n",
    "\n",
    "                if (y[i] * E_i < -self.tol and theta[i] < self.c) or (y[i] * E_i > self.tol and theta[i] > 0):\n",
    "                    j = np.random.randint(0, m)\n",
    "                    while j == i:\n",
    "                        j = np.random.randint(0, m)\n",
    "\n",
    "                    f_xj = np.sum(theta * y * K_matrix[:, j]) + b\n",
    "                    E_j = f_xj - y[j]\n",
    "\n",
    "                    alpha_i_old, alpha_j_old = theta[i], theta[j]\n",
    "\n",
    "                    if y[i] != y[j]:\n",
    "                        L, H = max(0, alpha_j_old - alpha_i_old), min(self.c, self.c + alpha_j_old - alpha_i_old)\n",
    "                    else:\n",
    "                        L, H = max(0, alpha_i_old + alpha_j_old - self.c), min(self.c, alpha_i_old + alpha_j_old)\n",
    "\n",
    "                    if L == H: continue\n",
    "\n",
    "                    eta = 2.0 * K_matrix[i, j] - K_matrix[i, i] - K_matrix[j, j]\n",
    "                    if eta >= 0: continue\n",
    "\n",
    "                    theta[j] -= (y[j] * (E_i - E_j)) / eta\n",
    "                    theta[j] = np.clip(theta[j], L, H)\n",
    "\n",
    "                    if abs(theta[j] - alpha_j_old) < 1e-5: continue\n",
    "\n",
    "                    theta[i] += y[i] * y[j] * (alpha_j_old - theta[j])\n",
    "\n",
    "                    b1 = b - E_i - y[i] * (theta[i] - alpha_i_old) * K_matrix[i, i] - y[j] * (theta[j] - alpha_j_old) * K_matrix[i, j]   \n",
    "                    b2 = b - E_j - y[i] * (theta[i] - alpha_i_old) * K_matrix[i, j] - y[j] * (theta[j] - alpha_j_old) * K_matrix[j, j]\n",
    "\n",
    "                    if 0 < theta[i] < self.c: b = b1\n",
    "                    elif 0 < theta[j] < self.c: b = b2\n",
    "                    else: b = (b1 + b2) / 2\n",
    "\n",
    "            if np.linalg.norm(theta - alpha_prev) < self.tol:\n",
    "                break\n",
    "        \n",
    "        return {\"theta\": theta, \"b\": b, \"X\": X, \"y\": y}\n",
    "    \n",
    "    # Fit \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        self.models = []\n",
    "\n",
    "        if n_classes <= 2:\n",
    "            binary_y = np.where(y == self.classes[0], -1, 1)\n",
    "            self.models.append(self._fit_binary(X, binary_y))\n",
    "            \n",
    "        elif self.decision_function_shape == 'ovr':\n",
    "            # One vs Rest\n",
    "            for c in self.classes:\n",
    "                binary_y = np.where(y == c, 1, -1)\n",
    "                self.models.append(self._fit_binary(X, binary_y))\n",
    "\n",
    "        elif self.decision_function_shape == 'ovo':\n",
    "            # One vs One\n",
    "            for i in range(n_classes):\n",
    "                for j in range(i + 1, n_classes):\n",
    "                    idx = np.where((y == self.classes[i]) | (y == self.classes[j]))\n",
    "                    X_sub, y_sub = X[idx], y[idx]\n",
    "                    binary_y = np.where(y_sub == self.classes[i], 1, -1)\n",
    "                    model = self._fit_binary(X_sub, binary_y)\n",
    "                    model['cls_pair'] = (self.classes[i], self.classes[j])\n",
    "                    self.models.append(model)\n",
    "        \n",
    "        # Bias value and other theta values \n",
    "        self.intercept_ = self.models[0]['b']   \n",
    "        self.coef_ = self.models[0]['theta']         \n",
    "\n",
    "    # Decision function for a single model\n",
    "    def _get_score(self, X, model):\n",
    "        scores = []\n",
    "        for x in X:\n",
    "            score = np.sum(model['theta'] * model['y'] * [self.kernel(x_tr, x) for x_tr in model['X']]) + model['b']\n",
    "            scores.append(score)\n",
    "        return np.array(scores)\n",
    "\n",
    "    # Predict             \n",
    "    def predict(self, X):\n",
    "        if len(self.classes) <= 2:\n",
    "            scores = self._get_score(X, self.models[0])\n",
    "            return np.where(scores >= 0, self.classes[1], self.classes[0])\n",
    "\n",
    "        if self.decision_function_shape == 'ovr':\n",
    "            # Highest confidence wins\n",
    "            all_scores = np.array([self._get_score(X, m) for m in self.models])\n",
    "            return self.classes[np.argmax(all_scores, axis=0)]\n",
    "\n",
    "        elif self.decision_function_shape == 'ovo':\n",
    "            # Voting system\n",
    "            votes = np.zeros((len(X), len(self.classes)))\n",
    "            for m in self.models:\n",
    "                scores = self._get_score(X, m)\n",
    "                preds = np.where(scores >= 0, m['cls_pair'][0], m['cls_pair'][1])\n",
    "                for idx, p in enumerate(preds):\n",
    "                    votes[idx, np.where(self.classes == p)[0][0]] += 1\n",
    "            return self.classes[np.argmax(votes, axis=1)]\n",
    "\n",
    "    # Score \n",
    "    def score(self, X_new, y):\n",
    "        # Return accuracy score of the model\n",
    "        y_pred = self.predict(X_new)\n",
    "        return np.mean(y_pred == y) \n",
    "\n",
    "# Regression remains largely the same but usually doesn't use OVO/OVR\n",
    "class SVR():\n",
    "    def __init__(self, c=1.0, epsilon=0.1, kernel='Linear', degree=3, gamma=1, coef0=1, lr=0.001, max_iter=1000):\n",
    "        self.c = c\n",
    "        self.epsilon = epsilon\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if kernel == 'Linear':\n",
    "            self.kernel = lambda x1, x2: linear_kernel(x1, x2)\n",
    "        elif kernel == 'Polynomial':\n",
    "            self.kernel = lambda x1, x2: polynomial_kernel(x1, x2, gamma=self.gamma, degree=self.degree, coef0=self.coef0)\n",
    "        elif kernel == 'RBF':\n",
    "            self.kernel = lambda x1, x2: rbf_kernel(x1, x2, gamma=self.gamma)\n",
    "        elif kernel == 'Sigmoid':\n",
    "            self.kernel = lambda x1, x2: sigmoid_kernel(x1, x2, gamma=self.gamma, coef0=self.coef0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        m = len(X)\n",
    "        self.alpha = np.zeros(m)\n",
    "        self.b = 0\n",
    "\n",
    "        self.K_matrix = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                self.K_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(m):\n",
    "                pred = np.sum(self.alpha * self.K_matrix[:, i]) + self.b\n",
    "                error = pred - self.y[i]\n",
    "                if abs(error) > self.epsilon:\n",
    "                    self.alpha[i] -= self.lr * error\n",
    "                    self.alpha[i] = np.clip(self.alpha[i], -self.c, self.c)\n",
    "                    self.b -= self.lr * error\n",
    "\n",
    "        # Bias value and other theta values \n",
    "        self.intercept_ = self.b\n",
    "        self.coef_ = self.alpha            \n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            pred = np.sum(self.alpha * [self.kernel(x_tr, x) for x_tr in self.X]) + self.b\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def score(self, X_new, y):\n",
    "        # Return R^2 score \n",
    "        y_pred = self.predict(X_new)\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_residual = np.sum((y - y_pred) ** 2)\n",
    "        return 1 - ss_residual / ss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51af1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: SVC (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 89.00 %\n",
      "Intercept: -3.31\n",
      "Coefficient: [ 1.    0.    0.    0.    1.    1.    0.    0.    0.   -0.    0.    0.\n",
      "  0.    0.    1.    0.    1.    0.    0.    1.    1.    1.    1.    0.89\n",
      "  0.    1.    0.   -0.    1.    0.    0.07  1.    0.    1.    0.    0.\n",
      "  0.    0.    1.    0.    1.    0.39  0.    0.    1.    0.    1.    0.\n",
      "  0.99  0.    0.    1.    0.    0.    0.    1.    1.    0.    1.    0.\n",
      "  1.    0.    0.    0.31  1.    1.    1.    1.    1.    0.    0.    0.\n",
      "  0.    0.    0.    1.    1.    0.    1.    1.    0.96  0.57  1.    1.\n",
      "  1.    0.    0.    0.    0.15  0.    0.    0.    0.    1.    0.    1.\n",
      "  1.    0.27  1.    0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 87.00 %\n",
      "Intercept: -2.38\n",
      "Coefficient: [1.   0.   0.   0.   1.   1.   0.   0.   0.   0.   0.41 0.   0.   0.\n",
      " 1.   0.   0.07 0.   0.   1.   1.   0.31 1.   0.02 0.   1.   0.   0.\n",
      " 0.87 0.   0.   1.   0.   1.   0.   0.   0.   0.   1.   0.   1.   0.\n",
      " 0.   0.   1.   0.   0.6  0.   1.   0.   0.   1.   0.   0.   0.   0.2\n",
      " 0.09 0.   1.   0.   0.86 0.   0.   0.26 0.15 1.   1.   1.   1.   0.\n",
      " 0.   0.   0.   0.   0.   1.   1.   0.   1.   0.   0.67 0.56 1.   1.\n",
      " 1.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.54 1.   0.\n",
      " 0.12 0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 1 1]\n",
      "Accuracy score on training data: 85.00 %\n",
      "Intercept: 0.22\n",
      "Coefficient: [ 1.    0.    0.    0.    1.    1.    0.    0.44  0.    0.    0.    0.\n",
      "  0.    0.    1.    0.    1.    0.    0.    0.92  1.    1.    1.    1.\n",
      "  0.    1.    0.    0.66  1.    0.    1.    1.    0.    1.    0.    0.\n",
      "  0.    0.    1.    0.    1.    0.55  0.    0.    1.    0.    1.    0.\n",
      "  1.    0.    0.    1.    0.    0.    0.    1.    1.    0.    1.    0.\n",
      "  1.   -0.    0.    1.    1.    1.    1.    1.    1.    0.    0.    0.\n",
      "  0.28  0.    0.    1.    0.98  0.    1.    0.    1.    1.    1.    1.\n",
      "  1.    0.    0.01  0.    0.    0.    0.78  0.    0.    1.    0.    0.94\n",
      "  1.   -0.    1.    0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 41.00 %\n",
      "Intercept: -0.58\n",
      "Coefficient: [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0.]\n",
      "----------------------------------------\n",
      "Method: SVC (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [2 2 2]\n",
      "Accuracy score on training data: 41.00 %\n",
      "Intercept: 2.08\n",
      "Coefficient: [1.   0.   0.   0.35 0.   1.   1.   1.   0.   0.   0.   1.   0.   0.72\n",
      " 0.99 0.33 1.   0.   0.   0.   0.27 1.   0.   0.   0.   1.   0.34 0.\n",
      " 1.   0.   1.   0.   0.   0.8  0.   1.   0.   0.   1.   0.   1.   1.\n",
      " 1.   1.   1.   0.59 0.   0.   1.   0.   0.   1.   0.   0.16 0.   1.\n",
      " 0.   0.   1.   0.   1.   0.   1.   0.   0.2  1.   0.48 1.   1.   0.\n",
      " 0.73 0.   0.   0.   1.   0.23 0.   0.   1.   1.   0.   1.   1.   1.\n",
      " 1.   0.   0.93 0.   0.   0.   1.   0.   0.   1.   0.99 0.   0.   1.\n",
      " 0.17 0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [2 2 2]\n",
      "Accuracy score on training data: 44.00 %\n",
      "Intercept: 1.44\n",
      "Coefficient: [ 1.    0.    0.33  0.    0.    1.    1.    1.    0.    0.    0.    1.\n",
      "  0.    0.    0.53  1.    1.    0.    0.    0.    0.    1.    0.    0.\n",
      "  0.    1.    0.29  0.    1.    0.    0.41  0.    0.    0.    0.    1.\n",
      "  0.    0.    1.    0.    1.    1.    1.    1.    1.    0.6   0.04  0.\n",
      "  0.05  0.    0.    1.    0.    0.    0.    1.    0.    0.    1.    0.\n",
      "  1.    0.    1.    0.    0.    1.    1.    1.    1.    0.    0.    0.\n",
      "  0.    0.    1.    0.    0.    0.    1.    1.    0.    1.    1.    1.\n",
      "  1.    0.    0.    0.    0.    0.    1.    0.    0.11  0.44  0.32  0.\n",
      " -0.    1.    0.    0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [2 0 0]\n",
      "Accuracy score on training data: 42.00 %\n",
      "Intercept: 0.02\n",
      "Coefficient: [ 1.    0.    0.    0.65  0.09  1.    1.    1.    0.    0.    0.    1.\n",
      "  0.    1.    1.    1.    1.    0.    0.    0.07  1.    1.    0.    0.\n",
      "  0.    1.    1.    0.    1.    0.    1.    0.    0.    0.08  0.    1.\n",
      "  0.    0.    1.    0.    1.    1.    1.    1.    1.    0.63  0.95  0.\n",
      "  0.94 -0.    0.    1.   -0.    0.39  0.    1.    0.    0.    1.    0.\n",
      "  1.    0.    1.    0.   -0.    1.    1.    1.    1.    0.    0.    0.\n",
      "  0.01  0.    1.    0.    0.    0.    1.    1.    0.    1.    1.    1.\n",
      "  1.    0.    1.    0.    0.    0.    1.    0.    0.16  1.    0.7   0.\n",
      "  0.    1.    0.39  0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [2 0 0]\n",
      "Accuracy score on training data: 0.00 %\n",
      "Intercept: -0.21\n",
      "Coefficient: [1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "----------------------------------------\n",
      "Method: SVC (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 2 2]\n",
      "Accuracy score on training data: 51.00 %\n",
      "Intercept: 1.76\n",
      "Coefficient: [1.   0.   0.   0.   1.   1.   0.62 0.   0.   1.   0.   1.   0.89 0.\n",
      " 1.   0.   0.   1.   0.   1.   0.71 1.   0.99 0.   1.   1.   1.   1.\n",
      " 0.99 1.   1.   0.71 0.   1.   1.   0.   1.   0.47 0.   1.   1.   0.\n",
      " 1.   1.   0.88 1.   1.   0.61 1.   1.   1.   1.   0.07 1.   1.   1.\n",
      " 1.   0.97 0.   0.   1.   0.   0.98 0.   0.72 0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [2 2 2]\n",
      "Accuracy score on training data: 56.00 %\n",
      "Intercept: 1.70\n",
      "Coefficient: [1.   0.   0.   0.   1.   1.   1.   0.   0.   1.   0.   0.15 0.82 0.67\n",
      " 1.   0.   0.   1.   0.   1.   0.   1.   0.48 0.   1.   1.   1.   1.\n",
      " 1.   1.   1.   0.99 0.   0.19 1.   0.   0.85 0.   0.   1.   1.   0.\n",
      " 1.   1.   1.   1.   1.   0.   1.   0.   1.   1.   0.   1.   1.   1.\n",
      " 1.   0.   0.   0.   1.   0.87 0.   0.   1.   0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [1 2 2]\n",
      "Accuracy score on training data: 50.00 %\n",
      "Intercept: 0.07\n",
      "Coefficient: [1.   0.62 0.   0.12 1.   1.   1.   0.   0.   1.   0.   1.   1.   1.\n",
      " 1.   0.   0.06 1.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      " 1.   1.   1.   0.   0.9  1.   1.   0.54 1.   0.   0.   1.   1.   0.\n",
      " 1.   1.   1.   1.   1.   0.   1.   0.   1.   1.   0.52 1.   1.   1.\n",
      " 1.   0.95 0.   0.21 1.   0.   0.76 0.07 1.   0.  ]\n",
      "----------------------------------------\n",
      "Method: SVC (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0 0 0]\n",
      "Accuracy score on training data: 46.00 %\n",
      "Intercept: 1.12\n",
      "Coefficient: [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "----------------------------------------\n",
      "Method: SVR (Linear)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0.62517968 2.19660455 2.7798759 ]\n",
      "Accuracy score on training data: 56.03 %\n",
      "Intercept: -0.26\n",
      "Coefficients: [-0.53  0.06 -0.02 -0.01 -0.65 -0.5  -0.02 -0.2   0.08 -0.02  0.23 -0.06\n",
      "  0.17 -0.03  0.43 -0.05 -0.35  0.1  -0.1   0.33  0.42 -0.37 -0.87  0.28\n",
      "  0.02  0.53 -0.02  0.2  -0.34 -0.09 -0.22 -0.7   0.02  0.42 -0.12 -0.02\n",
      "  0.02 -0.16 -0.42  0.15 -0.43 -0.27 -0.05 -0.22  0.48 -0.03  0.34  0.25\n",
      "  0.45  0.02 -0.08 -0.41  0.02 -0.02  0.04 -0.33  0.38  0.01 -0.34  0.14\n",
      " -0.26  0.04 -0.17  0.29  0.36  0.58 -0.56  0.85 -0.72  0.2  -0.02  0.01\n",
      "  0.26  0.15 -0.02  0.38  0.33  0.02  0.56 -0.26  0.35 -0.34 -0.51 -0.44\n",
      "  0.55  0.05 -0.13  0.27  0.27  0.22 -0.25 -0.12 -0.02  0.51 -0.03  0.35\n",
      " -0.73 -0.24  0.38  0.07]\n",
      "----------------------------------------\n",
      "Method: SVR (Polynomial)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 0.71082795  0.25288925 -2.04985498]\n",
      "Accuracy score on training data: 57.42 %\n",
      "Intercept: -0.10\n",
      "Coefficients: [-0.51  0.   -0.01  0.06 -0.66 -0.52 -0.01 -0.18  0.06 -0.07  0.23 -0.04\n",
      "  0.14 -0.02  0.43 -0.03 -0.32  0.09 -0.08  0.34  0.38 -0.36 -0.9   0.29\n",
      "  0.    0.52 -0.01  0.2  -0.34 -0.13 -0.18 -0.74  0.    0.41 -0.06 -0.01\n",
      "  0.   -0.13 -0.42  0.15 -0.42 -0.24 -0.03 -0.18  0.49 -0.01  0.35  0.25\n",
      "  0.44  0.04 -0.07 -0.4   0.   -0.    0.06 -0.3   0.38  0.   -0.33  0.13\n",
      " -0.23  0.03 -0.14  0.29  0.35  0.59 -0.55  0.88 -0.76  0.2  -0.   -0.03\n",
      "  0.24  0.14 -0.    0.37  0.33  0.01  0.56 -0.24  0.32 -0.33 -0.51 -0.45\n",
      "  0.55  0.   -0.02  0.25  0.22  0.16 -0.23 -0.14 -0.02  0.49 -0.02  0.33\n",
      " -0.73 -0.22  0.33  0.11]\n",
      "----------------------------------------\n",
      "Method: SVR (RBF)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [0.66031316 0.35172016 0.34877515]\n",
      "Accuracy score on training data: 58.96 %\n",
      "Intercept: 0.34\n",
      "Coefficients: [-0.5   0.04 -0.02 -0.03 -0.68 -0.52 -0.02 -0.06 -0.01  0.05  0.24 -0.03\n",
      "  0.07 -0.03  0.42 -0.03 -0.31  0.07  0.03  0.35  0.39 -0.32 -0.95  0.29\n",
      "  0.04  0.51 -0.02  0.21 -0.3   0.04 -0.2  -0.75  0.03  0.36  0.03 -0.02\n",
      "  0.03  0.04 -0.43  0.1  -0.44 -0.2  -0.03 -0.16  0.45 -0.02  0.35  0.26\n",
      "  0.4   0.04  0.04 -0.41  0.03 -0.04  0.03 -0.34  0.32  0.03 -0.3   0.04\n",
      " -0.22 -0.02 -0.04  0.26  0.3   0.61 -0.57  0.94 -0.77  0.16 -0.03  0.05\n",
      "  0.22  0.04 -0.02  0.32  0.31  0.08  0.55 -0.2   0.28 -0.28 -0.52 -0.45\n",
      "  0.54  0.04 -0.09  0.19  0.22  0.2  -0.19  0.02 -0.02  0.48 -0.02  0.29\n",
      " -0.68 -0.18  0.35  0.09]\n",
      "----------------------------------------\n",
      "Method: SVR (Sigmoid)\n",
      "Predictions for [[1, 2], [3, 4], [4, 3]]: [ 0.04293138 -0.34025055 -0.33868526]\n",
      "Accuracy score on training data: -142.89 %\n",
      "Intercept: -0.27\n",
      "Coefficients: [-1.    0.71 -0.81 -0.47 -0.63 -0.44 -0.72 -0.66 -0.7   0.77  0.27 -0.56\n",
      " -0.48 -1.    0.37 -0.55 -1.   -0.64  0.74  0.12  0.68 -0.57 -0.37  0.22\n",
      "  0.74  0.64 -0.8   0.33 -0.47  0.76 -0.41 -0.3   0.7   0.62  0.61 -0.53\n",
      "  0.63  0.77 -0.36  0.43 -0.4  -0.56 -0.59 -0.47  0.63 -0.65  0.17  0.25\n",
      "  0.62  0.52  0.46 -0.39  0.64 -0.45  0.62 -0.36  0.63  0.63 -0.52  0.68\n",
      " -0.46 -0.8  -0.6   0.37  0.66  0.32 -0.62  0.41 -0.29  0.44 -0.48  0.4\n",
      "  0.71  0.52 -0.58  0.57  0.31  0.36  0.65 -0.48  0.62 -0.61 -0.49 -0.39\n",
      "  0.6   0.7  -0.44  0.61  0.69  0.71 -0.87  0.65 -0.72  0.61 -0.62  0.58\n",
      " -0.96 -0.48  0.68 -0.54]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# Create number of rows and random x and y matrices\n",
    "m = 100\n",
    "X1 = 2 * np.random.rand(m, 1)\n",
    "X2 = 2 * np.random.rand(m, 1)\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Generate continuous target then convert to binary classes (0 and 1) also have low and high thresholds to generate \n",
    "y_continuous = 4 + 3 * X[:, 0] + np.random.randn(m)\n",
    "threshold = np.mean(y_continuous)\n",
    "y_labels = (y_continuous > threshold).astype(int)\n",
    "low_threshold = np.percentile(y_continuous, 33)\n",
    "high_threshold = np.percentile(y_continuous, 66)\n",
    "y_multi = np.zeros(m, dtype=int)\n",
    "y_multi[y_continuous > low_threshold] = 1\n",
    "y_multi[y_continuous > high_threshold] = 2\n",
    "\n",
    "\n",
    "# Test data\n",
    "X_new = np.array([[1, 2], [3, 4], [4, 3]])\n",
    "\n",
    "\n",
    "# we will just test the SVC \n",
    "models = {\n",
    "    \"SVC (Linear)\": SVC(kernel='Linear'),\n",
    "    \"SVC (Polynomial)\": SVC(kernel='Polynomial', gamma=0.5),\n",
    "    \"SVC (RBF)\": SVC(kernel='RBF', gamma=0.5),\n",
    "    \"SVC (Sigmoid)\": SVC(kernel='Sigmoid'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    model.fit(X, y_labels)\n",
    "    # Note: SVC predicts -1 or 1, while y_labels are 0 or 1.\n",
    "    # The score function in the SVC class handles this internally if y_labels is used.\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, y_labels) * 100\n",
    "    print(f'Method: {name}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficient: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# we will just test SVR with multiclass using OVR\n",
    "models = {\n",
    "    \"SVC (Linear)\": SVC(kernel='Linear', decision_function_shape='ovr'),\n",
    "    \"SVC (Polynomial)\": SVC(kernel='Polynomial', gamma=0.5, decision_function_shape='ovr'),\n",
    "    \"SVC (RBF)\": SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovr'),\n",
    "    \"SVC (Sigmoid)\": SVC(kernel='Sigmoid', decision_function_shape='ovr'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    model.fit(X, y_multi)\n",
    "    # Note: SVC predicts -1 or 1, while y_labels are 0 or 1.\n",
    "    # The score function in the SVC class handles this internally if y_labels is used.\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, y_labels) * 100  \n",
    "    print(f'Method: {name}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficient: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# we will just test SVR with multiclass using OVO\n",
    "models = {\n",
    "    \"SVC (Linear)\": SVC(kernel='Linear', decision_function_shape='ovo'),\n",
    "    \"SVC (Polynomial)\": SVC(kernel='Polynomial', gamma=0.5, decision_function_shape='ovo'),\n",
    "    \"SVC (RBF)\": SVC(kernel='RBF', gamma=0.5, decision_function_shape='ovo'),\n",
    "    \"SVC (Sigmoid)\": SVC(kernel='Sigmoid', decision_function_shape='ovo'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    model.fit(X, y_multi)\n",
    "    # Note: SVC predicts -1 or 1, while y_labels are 0 or 1.\n",
    "    # The score function in the SVC class handles this internally if y_labels is used.\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, y_labels) * 100\n",
    "    print(f'Method: {name}')\n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficient: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# we will just test the SVR \n",
    "models = {\n",
    "    \"SVR (Linear)\": SVR(kernel='Linear'),\n",
    "    \"SVR (Polynomial)\": SVR(kernel='Polynomial', gamma=0.5),\n",
    "    \"SVR (RBF)\": SVR(kernel='RBF', gamma=0.5),\n",
    "    \"SVR (Sigmoid)\": SVR(kernel='Sigmoid'),\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    # Get SVM object\n",
    "    model = models[name]\n",
    "    # Use y_labels for binary, y_multi for multiclass\n",
    "    target = y_labels if \"decision_function_shape\" not in name else y_multi\n",
    "    model.fit(X, target)\n",
    "    y_pred = model.predict(X_new)\n",
    "    score_train = model.score(X, target) * 100\n",
    "    print(f'Method: {name}') \n",
    "    print(f'Predictions for {X_new.tolist()}: {y_pred}')\n",
    "    print(f'Accuracy score on training data: {score_train:.2f} %')\n",
    "    print(f'Intercept: {model.intercept_:.2f}')\n",
    "    print(f'Coefficients: {np.round(model.coef_, 2)}')\n",
    "    print('-' * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
